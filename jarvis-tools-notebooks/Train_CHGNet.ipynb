{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCww2GPypSsU/pCzJJpVlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knc6/jarvis-tools-notebooks/blob/master/jarvis-tools-notebooks/Train_CHGNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q98RWu5O4wIm",
        "outputId": "aa52d722-ac82-4f82-f473-a61c75deb885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.7/807.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pymatgen (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -q chgnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('jarvis_leaderboard'):\n",
        "  !git clone https://github.com/usnistgov/jarvis_leaderboard.git\n",
        "os.chdir('jarvis_leaderboard')\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsDnBjVsAK-E",
        "outputId": "efcbf7e4-82b0-4b95-94de-359e9a88bee4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jarvis_leaderboard'...\n",
            "remote: Enumerating objects: 60667, done.\u001b[K\n",
            "remote: Counting objects: 100% (679/679), done.\u001b[K\n",
            "remote: Compressing objects: 100% (268/268), done.\u001b[K\n",
            "remote: Total 60667 (delta 313), reused 472 (delta 115), pack-reused 59988\u001b[K\n",
            "Receiving objects: 100% (60667/60667), 390.06 MiB | 22.70 MiB/s, done.\n",
            "Resolving deltas: 100% (31735/31735), done.\n",
            "Updating files: 100% (3643/3643), done.\n",
            "Obtaining file:///content/jarvis_leaderboard\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (1.11.4)\n",
            "Collecting jarvis-tools>=2021.07.19 (from jarvis-leaderboard==2024.1.16)\n",
            "  Downloading jarvis_tools-2023.12.12-py2.py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.7/975.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (1.5.3)\n",
            "Collecting rouge>=1.0.1 (from jarvis-leaderboard==2024.1.16)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting mkdocs>=1.5.2 (from jarvis-leaderboard==2024.1.16)\n",
            "  Downloading mkdocs-1.5.3-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs-material>=9.0.5 (from jarvis-leaderboard==2024.1.16)\n",
            "  Downloading mkdocs_material-9.5.6-py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic>=2.3.0 (from jarvis-leaderboard==2024.1.16)\n",
            "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (3.5.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (5.15.0)\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (1.4.0)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2024.1.16) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2024.1.16) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2024.1.16) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2024.1.16) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2024.1.16) (4.66.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (3.7.1)\n",
            "Requirement already satisfied: spglib>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (2.31.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (0.12.0)\n",
            "Collecting xmltodict>=0.11.0 (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting ghp-import>=1.0 (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16) (3.1.3)\n",
            "Requirement already satisfied: markupsafe>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16) (2.1.4)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.5 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16) (23.2)\n",
            "Collecting pathspec>=0.11.1 (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16) (4.1.0)\n",
            "Collecting pyyaml-env-tag>=0.1 (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading pyyaml_env_tag-0.1-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16) (6.0.1)\n",
            "Collecting watchdog>=2.0 (from mkdocs>=1.5.2->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel~=2.10 in /usr/local/lib/python3.10/dist-packages (from mkdocs-material>=9.0.5->jarvis-leaderboard==2024.1.16) (2.14.0)\n",
            "Collecting colorama~=0.4 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting mkdocs-material-extensions~=1.3 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading mkdocs_material_extensions-1.3.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting paginate~=0.5 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading paginate-0.5.6.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygments~=2.16 in /usr/local/lib/python3.10/dist-packages (from mkdocs-material>=9.0.5->jarvis-leaderboard==2024.1.16) (2.16.1)\n",
            "Collecting pymdown-extensions~=10.2 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading pymdown_extensions-10.7-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->jarvis-leaderboard==2024.1.16) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->jarvis-leaderboard==2024.1.16) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.3.0->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic>=2.3.0->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.6.1 (from pydantic>=2.3.0->jarvis-leaderboard==2024.1.16)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge>=1.0.1->jarvis-leaderboard==2024.1.16) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->jarvis-leaderboard==2024.1.16) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->jarvis-leaderboard==2024.1.16) (8.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2024.1.16) (2023.11.17)\n",
            "Building wheels for collected packages: paginate\n",
            "  Building wheel for paginate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paginate: filename=paginate-0.5.6-py3-none-any.whl size=12666 sha256=e38a8b07267c81a84b706daafef61ada5c5ee5afa0462ebcbb372d5b07ad91a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/d3/18/0b5bebc873f29bea61fedece1e92cbcbef416839dfe5bd0eef\n",
            "Successfully built paginate\n",
            "Installing collected packages: paginate, xmltodict, watchdog, typing-extensions, rouge, pyyaml-env-tag, pymdown-extensions, pathspec, mkdocs-material-extensions, mergedeep, colorama, annotated-types, pydantic-core, ghp-import, pydantic, mkdocs, mkdocs-material, jarvis-tools, jarvis-leaderboard\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.14\n",
            "    Uninstalling pydantic-1.10.14:\n",
            "      Successfully uninstalled pydantic-1.10.14\n",
            "  Running setup.py develop for jarvis-leaderboard\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 colorama-0.4.6 ghp-import-2.1.0 jarvis-leaderboard-2024.1.16 jarvis-tools-2023.12.12 mergedeep-1.3.4 mkdocs-1.5.3 mkdocs-material-9.5.6 mkdocs-material-extensions-1.3.1 paginate-0.5.6 pathspec-0.12.1 pydantic-2.5.3 pydantic-core-2.14.6 pymdown-extensions-10.7 pyyaml-env-tag-0.1 rouge-1.0.1 typing-extensions-4.9.0 watchdog-3.0.0 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://gitlab.com/ase/ase.git@optimizer-tests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0cmiCNb870a",
        "outputId": "765a496c-8cb9-4f7c-bf69-d3260a5f5f82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://gitlab.com/ase/ase.git@optimizer-tests\n",
            "  Cloning https://gitlab.com/ase/ase.git (to revision optimizer-tests) to /tmp/pip-req-build-yg_4iobh\n",
            "  Running command git clone --filter=blob:none --quiet https://gitlab.com/ase/ase.git /tmp/pip-req-build-yg_4iobh\n",
            "  Running command git checkout -b optimizer-tests --track origin/optimizer-tests\n",
            "  Switched to a new branch 'optimizer-tests'\n",
            "  Branch 'optimizer-tests' set up to track remote branch 'optimizer-tests' from 'origin'.\n",
            "  Resolved https://gitlab.com/ase/ase.git to commit a9012957be4fe649aedef9e8e626d332dccdb83d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from ase==3.23.0b1) (1.26.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ase==3.23.0b1) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase==3.23.0b1) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase==3.23.0b1) (1.16.0)\n",
            "Building wheels for collected packages: ase\n",
            "  Building wheel for ase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ase: filename=ase-3.23.0b1-py3-none-any.whl size=2422389 sha256=8c0148a4c102e50cded3a80e96887b4e4bb5b41f461e0a5814097cb533f3b80b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-twppsa0j/wheels/02/41/aa/7f35c006fc1c7a98036f78766e96ed43a92fae3dfffec81fcd\n",
            "Successfully built ase\n",
            "Installing collected packages: ase\n",
            "  Attempting uninstall: ase\n",
            "    Found existing installation: ase 3.22.1\n",
            "    Uninstalling ase-3.22.1:\n",
            "      Successfully uninstalled ase-3.22.1\n",
            "Successfully installed ase-3.23.0b1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n",
        "# then restart session"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "m_m1uNro40XM",
        "outputId": "676e5c8b-a036-41d2-d2f4-f113da5925d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.3\n",
            "    Uninstalling numpy-1.26.3:\n",
            "      Successfully uninstalled numpy-1.26.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "pymatgen 2024.1.27 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://figshare.com/ndownloader/files/40357663 -O mlearn.json.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTOtokdn424A",
        "outputId": "186c5b78-7a57-436b-d2c8-93906b5b3e3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-28 12:31:06--  https://figshare.com/ndownloader/files/40357663\n",
            "Resolving figshare.com (figshare.com)... 34.241.111.122, 54.171.25.118, 2a05:d018:1f4:d000:110e:8d24:f208:180e, ...\n",
            "Connecting to figshare.com (figshare.com)|34.241.111.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240128/eu-west-1/s3/aws4_request&X-Amz-Date=20240128T123106Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=23e3d01fb30cae4f48cb865da89202201fd36348fe696869be3c4a6cdc81ad33 [following]\n",
            "--2024-01-28 12:31:06--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240128/eu-west-1/s3/aws4_request&X-Amz-Date=20240128T123106Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=23e3d01fb30cae4f48cb865da89202201fd36348fe696869be3c4a6cdc81ad33\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.33.192, 52.92.19.16, 52.218.88.91, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.33.192|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542319 (2.4M) [application/zip]\n",
            "Saving to: ‘mlearn.json.zip’\n",
            "\n",
            "mlearn.json.zip     100%[===================>]   2.42M  3.58MB/s    in 0.7s    \n",
            "\n",
            "2024-01-28 12:31:07 (3.58 MB/s) - ‘mlearn.json.zip’ saved [2542319/2542319]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json,zipfile\n",
        "mlearn = json.loads(\n",
        "        zipfile.ZipFile(\"mlearn.json.zip\").read(\n",
        "            \"mlearn.json\"\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "cLYOlQDZ44yr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from jarvis.core.atoms import Atoms\n",
        "import json\n",
        "import shutil\n",
        "import warnings\n",
        "os.chdir('/content')\n",
        "elements = ['Si']\n",
        "max_epochs=100\n",
        "models={}\n",
        "for element in elements:\n",
        "\n",
        "    benchmark_energies = (\n",
        "        \"jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_\"\n",
        "        + element\n",
        "        + \"_energy.json.zip\"\n",
        "    )\n",
        "    temp_energies = benchmark_energies.split(\"/\")[-1].split(\".zip\")[0]\n",
        "    energies = json.loads(\n",
        "        zipfile.ZipFile(benchmark_energies).read(temp_energies)\n",
        "    )\n",
        "    train_ids = list(energies[\"train\"].keys())\n",
        "    test_ids = list(energies[\"test\"].keys())\n",
        "\n",
        "    train_energies=[]\n",
        "    train_forces=[]\n",
        "    train_stresses=[]\n",
        "    train_structures=[]\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in train_ids:\n",
        "            # print(i)\n",
        "            train_energies.append(i[\"energy\"])\n",
        "            train_forces.append(i[\"forces\"])\n",
        "            train_stresses.append(i['stresses'])\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            train_structures.append(atoms.pymatgen_converter())\n",
        "\n",
        "    test_energies=[]\n",
        "    test_forces=[]\n",
        "    test_stresses=[]\n",
        "    test_structures=[]\n",
        "    tids = []\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in test_ids:\n",
        "            # print(i)\n",
        "            test_energies.append(i[\"energy\"])\n",
        "            test_forces.append(i[\"forces\"])\n",
        "            test_stresses.append(i['stresses'])\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            test_structures.append(atoms.pymatgen_converter())\n",
        "            tids.append(i['jid'])\n"
      ],
      "metadata": {
        "id": "5K3KZB1U5DAx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_structures)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHzj_PBt6GMO",
        "outputId": "19a641da-c6c8-4f5c-edad-23ddba720b3c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chgnet.data.dataset import StructureData, get_train_val_test_loader\n",
        "from chgnet.trainer import Trainer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from chgnet.model.model import CHGNet\n",
        "from chgnet.data.dataset import collate_graphs\n",
        "chgnet = CHGNet()\n",
        "\n",
        "batch_size = 2\n",
        "train_dataset = StructureData(\n",
        "    structures=train_structures,\n",
        "    energies=train_energies,\n",
        "    forces=train_forces,\n",
        "    #stresses=train_stresses,\n",
        "    # magmoms=None,\n",
        ")\n",
        "test_dataset = StructureData(\n",
        "    structures=test_structures,\n",
        "    energies=test_energies,\n",
        "    forces=test_forces,\n",
        "    #stresses=test_stresses,\n",
        "    # magmoms=None,\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_graphs,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_graphs,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=collate_graphs,\n",
        "    num_workers=0,\n",
        "    pin_memory=False,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=chgnet,\n",
        "    targets=\"ef\",\n",
        "    optimizer=\"Adam\",\n",
        "    criterion=\"MSE\",\n",
        "    learning_rate=1e-2,\n",
        "    epochs=100,\n",
        "    use_device=\"cpu\",\n",
        ")\n",
        "\n",
        "trainer.train(train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LY5p0ku6Nry",
        "outputId": "49a9b578-64c4-4cdb-a6d3-8ef5a70bbd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHGNet initialized with 412,525 parameters\n",
            "214 structures imported\n",
            "25 structures imported\n",
            "Begin Training: using cpu device\n",
            "training targets: ef\n",
            "Epoch: [0][1/107] | Time (1.431)(0.053) | Loss 95330.4453(95330.4453) | MAE e 308.234(308.234)  f 0.886(0.886)  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://gitlab.com/ase/ase.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "ds7QerFt8WG4",
        "outputId": "bece8bf3-0f98-4444-b478-0e6a5c3a417a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://gitlab.com/ase/ase.git@master\n",
            "  Cloning https://gitlab.com/ase/ase.git (to revision master) to /tmp/pip-req-build-vrldud_1\n",
            "  Running command git clone --filter=blob:none --quiet https://gitlab.com/ase/ase.git /tmp/pip-req-build-vrldud_1\n",
            "  Resolved https://gitlab.com/ase/ase.git to commit 826acc31bf0fb794af268a176c04392dd02610ef\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from ase==3.23.0b1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ase==3.23.0b1) (1.11.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.10/dist-packages (from ase==3.23.0b1) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.4->ase==3.23.0b1) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase==3.23.0b1) (1.16.0)\n",
            "Building wheels for collected packages: ase\n",
            "  Building wheel for ase (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ase: filename=ase-3.23.0b1-py3-none-any.whl size=2690144 sha256=0604746a8e69fe23e70989034d7e32b9e58bb89da1ed00a3b50b68e56d84a1a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z7fqpr5n/wheels/01/fc/60/e6134958a4ba6921161034c849be512a068712df0013719724\n",
            "Successfully built ase\n",
            "Installing collected packages: ase\n",
            "  Attempting uninstall: ase\n",
            "    Found existing installation: ase 3.22.1\n",
            "    Uninstalling ase-3.22.1:\n",
            "      Successfully uninstalled ase-3.22.1\n",
            "Successfully installed ase-3.23.0b1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ase"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ase import filters"
      ],
      "metadata": {
        "id": "Jqwhi7RP6NpU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-uwIrYD76Nm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkRCehT26Nj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEipOJWA6NhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "from jarvis.core.atoms import Atoms\n",
        "import os\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from dgl.data.utils import split_dataset\n",
        "#from mp_api.client import MPRester\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "import numpy as np\n",
        "import torch\n",
        "from matgl.apps.pes import Potential\n",
        "import zipfile,json, glob\n",
        "import pandas as pd\n",
        "import matgl\n",
        "from matgl.ext.pymatgen import Structure2Graph, get_element_list\n",
        "from matgl.graph.data import M3GNetDataset, MGLDataLoader, collate_fn_efs\n",
        "from matgl.models import M3GNet\n",
        "from matgl.utils.training import PotentialLightningModule\n",
        "\n",
        "# To suppress warnings for clearer output\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "os.chdir('/content')\n",
        "elements = ['Cu']\n",
        "max_epochs=100\n",
        "models={}\n",
        "for element in elements:\n",
        "\n",
        "    benchmark_energies = (\n",
        "        \"jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_\"\n",
        "        + element\n",
        "        + \"_energy.json.zip\"\n",
        "    )\n",
        "    temp_energies = benchmark_energies.split(\"/\")[-1].split(\".zip\")[0]\n",
        "    energies = json.loads(\n",
        "        zipfile.ZipFile(benchmark_energies).read(temp_energies)\n",
        "    )\n",
        "    train_ids = list(energies[\"train\"].keys())\n",
        "    test_ids = list(energies[\"test\"].keys())\n",
        "\n",
        "    train_energies=[]\n",
        "    train_forces=[]\n",
        "    train_stresses=[]\n",
        "    train_structures=[]\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in train_ids:\n",
        "            # print(i)\n",
        "            train_energies.append(i[\"energy\"])\n",
        "            train_forces.append(i[\"forces\"])\n",
        "            train_stresses.append(i['stresses'])\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            train_structures.append(atoms.pymatgen_converter())\n",
        "\n",
        "\n",
        "    labels = {\n",
        "        \"energies\": train_energies,\n",
        "        \"forces\": train_forces,\n",
        "        \"stresses\": train_stresses,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    element_types = get_element_list(train_structures)\n",
        "    converter = Structure2Graph(element_types=element_types, cutoff=5.0)\n",
        "    train_data = M3GNetDataset(\n",
        "        threebody_cutoff=4.0,\n",
        "        structures=train_structures,\n",
        "        converter=converter,\n",
        "        labels=labels,\n",
        "    )\n",
        "\n",
        "\n",
        "    test_energies=[]\n",
        "    test_forces=[]\n",
        "    test_stresses=[]\n",
        "    test_structures=[]\n",
        "    tids = []\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in test_ids:\n",
        "            # print(i)\n",
        "            test_energies.append(i[\"energy\"])\n",
        "            test_forces.append(i[\"forces\"])\n",
        "            test_stresses.append(i['stresses'])\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            test_structures.append(atoms.pymatgen_converter())\n",
        "            tids.append(i['jid'])\n",
        "\n",
        "    labels = {\n",
        "        \"energies\": test_energies,\n",
        "        \"forces\": test_forces,\n",
        "        \"stresses\": test_stresses,\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    element_types = get_element_list(test_structures)\n",
        "    converter = Structure2Graph(element_types=element_types, cutoff=5.0)\n",
        "    test_data = M3GNetDataset(\n",
        "        threebody_cutoff=4.0,\n",
        "        structures=test_structures,\n",
        "        converter=converter,\n",
        "        labels=labels,\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader, val_loader, test_loader = MGLDataLoader(\n",
        "        train_data=train_data,\n",
        "        val_data=test_data,\n",
        "        test_data=test_data,\n",
        "        collate_fn=collate_fn_efs,\n",
        "        batch_size=2,\n",
        "        num_workers=1,\n",
        "    )\n",
        "    model = M3GNet(\n",
        "        element_types=element_types,\n",
        "        is_intensive=False,\n",
        "    )\n",
        "    lit_module = PotentialLightningModule(model=model)\n",
        "\n",
        "\n",
        "\n",
        "    logger = CSVLogger(\"logs\", name=\"M3GNet_training\")\n",
        "    # Inference mode = False is required for calculating forces, stress in test mode and prediction mode\n",
        "    trainer = pl.Trainer(max_epochs=max_epochs, accelerator=\"cpu\", logger=logger, inference_mode=False)\n",
        "    trainer.fit(model=lit_module, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
        "    models[element]=trainer\n",
        "    trainer.test(dataloaders=test_loader)\n",
        "\n",
        "    ######\n",
        "\n",
        "    def get_matgl_pred(atoms=None,model=None):\n",
        "        element_types = get_element_list([atoms.pymatgen_converter()])\n",
        "        p2g = Structure2Graph(element_types=element_types, cutoff=5.0)\n",
        "        graph, lat, state = p2g.get_graph(atoms.pymatgen_converter())\n",
        "        ff = Potential(model=model, calc_hessian=False)\n",
        "        e, f, s, h = ff(graph, lat, torch.tensor(state))\n",
        "        return e.detach().numpy(),f.detach().numpy(),s.detach().numpy()\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        json.loads(\n",
        "            zipfile.ZipFile(\"mlearn.json.zip\").read(\n",
        "                \"mlearn.json\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    print(df)\n",
        "    i_model = trainer.model.model.model\n",
        "    for i in glob.glob(\"/content/jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/*energy*.zip\"):\n",
        "        if \"mlearn\" in i and element in i:\n",
        "            fname_e = (\n",
        "                \"AI-MLFF-energy-\"\n",
        "                + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "                + \"-test-mae.csv\"\n",
        "            )\n",
        "            fname_f = (\n",
        "                \"AI-MLFF-forces-\"\n",
        "                + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "                + \"-test-multimae.csv\"\n",
        "            )\n",
        "            fname_s = (\n",
        "                \"AI-MLFF-stresses-\"\n",
        "                + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "                + \"-test-multimae.csv\"\n",
        "            )\n",
        "            f_e = open(fname_e, \"w\")\n",
        "            f_f = open(fname_f, \"w\")\n",
        "            f_s = open(fname_s, \"w\")\n",
        "\n",
        "            f_e.write(\"id,prediction\\n\")\n",
        "            f_f.write(\"id,prediction\\n\")\n",
        "            f_s.write(\"id,prediction\\n\")\n",
        "\n",
        "            print(i)\n",
        "            dat = json.loads(\n",
        "                zipfile.ZipFile(i).read(i.split(\"/\")[-1].split(\".zip\")[0])\n",
        "            )\n",
        "            print(dat[\"test\"])\n",
        "            for key, val in dat[\"test\"].items():\n",
        "                entry = df[df[\"jid\"] == key]\n",
        "                atoms = Atoms.from_dict(entry.atoms.values[0])\n",
        "                # print(key,val,df[df['jid']==key],atoms)\n",
        "                # energy,forces=get_alignn_forces(atoms)\n",
        "                energy, forces, stress = get_matgl_pred(atoms=atoms,model=i_model)\n",
        "                print(key, val, energy, atoms.num_atoms)\n",
        "                line = key + \",\" + str(energy) + \"\\n\"\n",
        "                f_e.write(line)\n",
        "                line = (\n",
        "                    key\n",
        "                    + \",\"\n",
        "                    + str(\";\".join(map(str, np.array(forces).flatten())))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f_f.write(line)\n",
        "                line = (\n",
        "                    key\n",
        "                    + \",\"\n",
        "                    + str(\";\".join(map(str, np.array(stress).flatten())))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f_s.write(line)\n",
        "            f_e.close()\n",
        "            f_f.close()\n",
        "            f_s.close()\n",
        "            zname = fname_e + \".zip\"\n",
        "            with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "                myzip.write(fname_e)\n",
        "\n",
        "            zname = fname_f + \".zip\"\n",
        "            with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "                myzip.write(fname_f)\n",
        "\n",
        "            zname = fname_s + \".zip\"\n",
        "            with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "                myzip.write(fname_s)\n",
        "\n",
        "\n",
        "# x=[]\n",
        "# y=[]\n",
        "# i_model = trainer.model.model.model\n",
        "# for i,j,k in zip(test_structures,test_energies,tids):\n",
        "\n",
        "#     e, f, s, h = get_matgl_pred(structure=i,model=i_model)\n",
        "#     #pen = trainer.model.model.model.predict_structure(i)\n",
        "#     nat = len(i)\n",
        "#     print(j,e)\n",
        "#     x.append(j/nat)\n",
        "#     y.append(e/nat)\n",
        "# x = np.array(x)\n",
        "# y = np.array(y)\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# mean_absolute_error(x,y)\n",
        "\n"
      ],
      "metadata": {
        "id": "7Bpk4aWc46ft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}