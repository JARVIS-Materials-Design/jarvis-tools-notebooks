{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvSeVNPqXiJvMt455q7GQR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knc6/jarvis-tools-notebooks/blob/master/jarvis-tools-notebooks/Train_MLFF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install wandb\n",
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!pip install nequip==0.5.5 torch==1.11\n",
        "\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJl9OzZZKWh_",
        "outputId": "4e38cf36-6105-4aa5-af16-d20bdddbaff7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=3a8f1724fd44eb0be2d07a9e7c7530ee5800a852a64a75b07e2256a47c35d601\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n",
            "Collecting nequip==0.5.5\n",
            "  Downloading nequip-0.5.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.11\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (1.22.4)\n",
            "Collecting ase (from nequip==0.5.5)\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (4.65.0)\n",
            "Collecting e3nn<0.6.0,>=0.3.5 (from nequip==0.5.5)\n",
            "  Downloading e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip==0.5.5)\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip==0.5.5)\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting scikit-learn<=1.0.1 (from nequip==0.5.5)\n",
            "  Downloading scikit-learn-1.0.1.tar.gz (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 28 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.97 KiB | 1.89 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nequip>=0.5.3 (from mir-allegro==0.2.0)\n",
            "  Downloading nequip-0.5.6-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.22.4)\n",
            "Collecting ase (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.65.0)\n",
            "Collecting torch!=1.9.0,<1.13,>=1.10.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting e3nn<0.6.0,>=0.4.4 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27439 sha256=2df5622d5361cb19a3c9e3f3e56fe5467c32948a2f78b7ae519f9bd9c82015c2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qa7ura5s/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: torch-runstats, torch, torch-ema, opt-einsum-fx, e3nn, ase, nequip, mir-allegro\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ase-3.22.1 e3nn-0.5.1 mir-allegro-0.2.0 nequip-0.5.6 opt-einsum-fx-0.1.4 torch-1.12.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruL2JdaB5hbS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r1_3LmhI8y3",
        "outputId": "78bbe58a-68e5-4558-df40-e923f268c5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 15:55:00--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ni/test.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/test.json [following]\n",
            "--2023-07-22 15:55:00--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611126 (597K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 596.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-22 15:55:00 (13.1 MB/s) - ‘test.json’ saved [611126/611126]\n",
            "\n",
            "--2023-07-22 15:55:00--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Cu/test.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/test.json [following]\n",
            "--2023-07-22 15:55:00--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 608400 (594K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 594.14K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-22 15:55:00 (13.3 MB/s) - ‘test.json’ saved [608400/608400]\n",
            "\n",
            "--2023-07-22 15:55:00--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Mo/test.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/test.json [following]\n",
            "--2023-07-22 15:55:01--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250629 (245K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 244.75K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-22 15:55:01 (8.54 MB/s) - ‘test.json’ saved [250629/250629]\n",
            "\n",
            "--2023-07-22 15:55:01--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ge/test.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/test.json [following]\n",
            "--2023-07-22 15:55:01--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 311848 (305K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 304.54K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-22 15:55:01 (9.03 MB/s) - ‘test.json’ saved [311848/311848]\n",
            "\n",
            "--2023-07-22 15:55:01--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Si/test.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/test.json [following]\n",
            "--2023-07-22 15:55:02--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 309803 (303K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 302.54K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-22 15:55:02 (9.60 MB/s) - ‘test.json’ saved [309803/309803]\n",
            "\n",
            "--2023-07-22 15:55:02--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Li/test.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/test.json [following]\n",
            "--2023-07-22 15:55:02--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279684 (273K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 273.13K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-22 15:55:02 (7.92 MB/s) - ‘test.json’ saved [279684/279684]\n",
            "\n",
            "allegro       Ge_test.json  Mo_test.json  sample_data\n",
            "Cu_test.json  Li_test.json  Ni_test.json  Si_test.json\n"
          ]
        }
      ],
      "source": [
        "# get data from mlearn\n",
        "#!rm *.json\n",
        "!for m in Ni Cu Mo Ge Si Li; do wget https://github.com/materialsvirtuallab/mlearn/raw/master/data/${m}/test.json; mv test.json ${m}_test.json; done;\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data from mlearn\n",
        "#!rm *.json\n",
        "!for m in Ni Cu Mo Ge Si Li; do wget https://github.com/materialsvirtuallab/mlearn/raw/master/data/${m}/training.json; mv training.json ${m}_training.json; done;\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soGwt8gCJUx4",
        "outputId": "69069007-65af-4c12-fff4-bbae0fa538f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 15:55:02--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ni/training.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/training.json [following]\n",
            "--2023-07-22 15:55:03--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5289402 (5.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   5.04M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-07-22 15:55:03 (66.0 MB/s) - ‘training.json’ saved [5289402/5289402]\n",
            "\n",
            "--2023-07-22 15:55:03--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Cu/training.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/training.json [following]\n",
            "--2023-07-22 15:55:03--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5246466 (5.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   5.00M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-07-22 15:55:04 (63.9 MB/s) - ‘training.json’ saved [5246466/5246466]\n",
            "\n",
            "--2023-07-22 15:55:04--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Mo/training.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/training.json [following]\n",
            "--2023-07-22 15:55:04--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2127325 (2.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.03M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-07-22 15:55:04 (31.7 MB/s) - ‘training.json’ saved [2127325/2127325]\n",
            "\n",
            "--2023-07-22 15:55:04--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ge/training.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/training.json [following]\n",
            "--2023-07-22 15:55:05--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2796663 (2.7M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.67M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-07-22 15:55:05 (40.7 MB/s) - ‘training.json’ saved [2796663/2796663]\n",
            "\n",
            "--2023-07-22 15:55:05--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Si/training.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/training.json [following]\n",
            "--2023-07-22 15:55:05--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2669946 (2.5M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.55M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-07-22 15:55:06 (37.6 MB/s) - ‘training.json’ saved [2669946/2669946]\n",
            "\n",
            "--2023-07-22 15:55:06--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Li/training.json\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/training.json [following]\n",
            "--2023-07-22 15:55:06--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446522 (2.3M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.33M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-07-22 15:55:07 (34.8 MB/s) - ‘training.json’ saved [2446522/2446522]\n",
            "\n",
            "allegro\t\t  Ge_training.json  Mo_training.json  Si_test.json\n",
            "Cu_test.json\t  Li_test.json\t    Ni_test.json      Si_training.json\n",
            "Cu_training.json  Li_training.json  Ni_training.json\n",
            "Ge_test.json\t  Mo_test.json\t    sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymatgen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O3hJ4WVJDyf",
        "outputId": "01526d7b-b6de-4925-d768-b4b47c7c46b7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymatgen\n",
            "  Downloading pymatgen-2023.7.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.7.1)\n",
            "Collecting monty>=3.0.2 (from pymatgen)\n",
            "  Downloading monty-2023.5.8-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mp-api>=0.27.3 (from pymatgen)\n",
            "  Downloading mp_api-0.33.3-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.1)\n",
            "Requirement already satisfied: numpy>=1.20.1 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.22.4)\n",
            "Requirement already satisfied: palettable>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.5.3)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (5.13.1)\n",
            "Collecting pybtex (from pymatgen)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.10.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pymatgen) (2.27.1)\n",
            "Collecting ruamel.yaml>=0.17.0 (from pymatgen)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.10.1)\n",
            "Collecting spglib>=2.0.2 (from pymatgen)\n",
            "  Downloading spglib-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (515 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.11.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from pymatgen) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pymatgen) (4.65.0)\n",
            "Collecting uncertainties>=3.1.4 (from pymatgen)\n",
            "  Downloading uncertainties-3.1.7-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (67.7.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (4.7.1)\n",
            "Collecting emmet-core>=0.54.0 (from mp-api>=0.27.3->pymatgen)\n",
            "  Downloading emmet_core-0.63.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.5.0->pymatgen) (8.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (3.4)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.0->pymatgen)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from uncertainties>=3.1.4->pymatgen) (0.18.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pymatgen) (2022.7.1)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.10/dist-packages (from pybtex->pymatgen) (6.0.1)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex->pymatgen)\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pybtex->pymatgen) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->pymatgen) (1.3.0)\n",
            "Installing collected packages: uncertainties, spglib, ruamel.yaml.clib, monty, latexcodec, ruamel.yaml, pybtex, emmet-core, mp-api, pymatgen\n",
            "Successfully installed emmet-core-0.63.0 latexcodec-2.0.1 monty-2023.5.8 mp-api-0.33.3 pybtex-0.24.0 pymatgen-2023.7.20 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 spglib-2.0.2 uncertainties-3.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FqDN7vTJ8ds",
        "outputId": "5aa47aa0-a999-4a1f-e46b-bdd748456e2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'allegro' already exists and is not an empty directory.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.5.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.22.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.22.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.9.0,<1.13,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27439 sha256=f3a0f9b010cb6b8748167a591fd2e1ce344998cd141b75fe338c6ad7eb3a4c4b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-47z3nrp4/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "  Attempting uninstall: mir-allegro\n",
            "    Found existing installation: mir-allegro 0.2.0\n",
            "    Uninstalling mir-allegro-0.2.0:\n",
            "      Successfully uninstalled mir-allegro-0.2.0\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jarvis-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_vtPWpNY7Tl",
        "outputId": "980baeb3-d122-44a2-9648-2f81d314bd09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jarvis-tools\n",
            "  Downloading jarvis_tools-2023.5.26-py2.py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (3.7.1)\n",
            "Requirement already satisfied: spglib>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (2.0.2)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (2.27.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (0.12.0)\n",
            "Collecting xmltodict>=0.11.0 (from jarvis-tools)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (4.65.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->jarvis-tools) (1.16.0)\n",
            "Installing collected packages: xmltodict, jarvis-tools\n",
            "Successfully installed jarvis-tools-2023.5.26 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from jarvis.core.atoms import pmg_to_atoms\n",
        "from jarvis.db.jsonutils import dumpjson\n",
        "from jarvis.db.jsonutils import loadjson\n",
        "from monty.serialization import loadfn, MontyEncoder, MontyDecoder\n",
        "from ase.stress import voigt_6_to_full_3x3_stress\n",
        "# Ref: https://github.com/materialsvirtuallab/mlearn\n",
        "data = loadfn(\n",
        "    \"Si_training.json\",\n",
        "    cls=MontyDecoder,\n",
        ")\n",
        "train_structures = [d[\"structure\"] for d in data]\n",
        "train_energies = [d[\"outputs\"][\"energy\"] for d in data]\n",
        "train_forces = [d[\"outputs\"][\"forces\"] for d in data]\n",
        "train_stresses = [d[\"outputs\"][\"virial_stress\"] for d in data]\n",
        "\n",
        "\n",
        "data = loadfn(\n",
        "    \"Si_test.json\",\n",
        "    cls=MontyDecoder,\n",
        ")\n",
        "test_structures = [d[\"structure\"] for d in data]\n",
        "test_energies = [d[\"outputs\"][\"energy\"] for d in data]\n",
        "test_forces = [d[\"outputs\"][\"forces\"] for d in data]\n",
        "test_stresses = [d[\"outputs\"][\"virial_stress\"] for d in data]\n",
        "\n"
      ],
      "metadata": {
        "id": "uQcYsDbgJIiC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "os.makedirs('Si_data')"
      ],
      "metadata": {
        "id": "huyOqxYmY5G7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"Si_data/sitraj.xyz\", \"w\")\n",
        "mem = []\n",
        "count = 0\n",
        "line = \"\"\n",
        "for i, j, k, l in zip(\n",
        "    train_structures, train_energies, train_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    #print(atoms)\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    #info[\"stresses\"] = l\n",
        "    #info[\"stresses\"] = voigt_6_to_full_3x3_stress(l)\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(l).flatten()))\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    #info[\"stresses\"] = l\n",
        "    #info[\"stresses\"] = voigt_6_to_full_3x3_stress(l)\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(l).flatten()))\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "f.close()\n",
        "dumpjson(data=mem, filename=\"Si_data/id_prop.json\")\n"
      ],
      "metadata": {
        "id": "e3Vk8wEOZOdg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!rm -rf ./results\n",
        "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiIbyRqIZtIW",
        "outputId": "e473d447-dc45-49bc-8562-2f66e2f277e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230722_155605-8gf60lsk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-121328701729598718/allegro-tutorial?apiKey=a3119b2aa4383bcda016425f70e052266828d485\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-121328701729598718/allegro-tutorial/runs/8gf60lsk?apiKey=a3119b2aa4383bcda016425f70e052266828d485\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[2139211, 1], batch=[2139211], cell=[34980, 3, 3], edge_cell_shift=[57841530, 3], edge_index=[2, 57841530], forces=[2139211, 3], free_energy=[34980], pbc=[34980, 3], pos=[2139211, 3], ptr=[34981], stress=[34980, 3, 3], total_energy=[34980, 1])\n",
            "    processed data size: ~1629.18 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(34980)...\n",
            "Replace string dataset_forces_rms to 0.9137988090515137\n",
            "Replace string dataset_per_atom_total_energy_mean to -5.076940059661865\n",
            "Atomic outputs are scaled by: [Si: 0.913799], shifted by [Si: -5.076940].\n",
            "Replace string dataset_forces_rms to 0.9137988090515137\n",
            "Initially outputs are globally scaled by: 0.9137988090515137, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:276: UserWarning: operator() profile_node %884 : int[] = prim::profile_ivalue(%882)\n",
            " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:108.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Equivariance test passed; equivariance errors:\n",
            "   Errors are in real units, where relevant.\n",
            "   Please note that the large scale of the typical\n",
            "   shifts to the (atomic) energy can cause\n",
            "   catastrophic cancellation and give incorrectly\n",
            "   the equivariance error as zero for those fields.\n",
            "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field free_energy          -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field stress               -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field total_energy         -> max error=6.104e-05\n",
            "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field pbc                  -> max error=0.000e+00\n",
            "   node permutation equivariance of field forces                     -> max error=1.974e-07\n",
            "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_embedding             -> max error=2.831e-07\n",
            "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_features              -> max error=1.192e-07\n",
            "   edge permutation equivariance of field edge_energy                -> max error=1.043e-07\n",
            "   node permutation equivariance of field atomic_energy              -> max error=4.768e-07\n",
            "   node permutation equivariance of field batch                      -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=4.678e-07\n",
            "   (parity_k=0, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=7.477e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=2.147e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=4.686e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=6.557e-07\n",
            "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=forces              )     -> max error=1.570e-06\n",
            "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=9.413e-07\n",
            "   (parity_k=0, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=7.423e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=3.855e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=6.437e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=7.451e-07\n",
            "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=forces              )     -> max error=2.282e-06\n",
            "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=4.721e-07\n",
            "   (parity_k=1, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=7.423e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.839e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=4.590e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=5.066e-07\n",
            "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=forces              )     -> max error=1.730e-06\n",
            "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=9.385e-07\n",
            "   (parity_k=1, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=8.649e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=2.132e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=4.590e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=forces              )     -> max error=1.300e-06\n",
            "Number of weights: 37352\n",
            "Number of trainable weights: 37352\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2         1.09        0.836        0.251         0.54        0.835         26.2        0.414\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    6.205    0.002         0.82        0.212         1.03        0.539        0.828         22.9         0.36\n",
            "Wall time: 6.206524298000204\n",
            "! Best model        0    1.032\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.383        0.265        0.118         0.37        0.471         20.1        0.314\n",
            "      1    20        0.275        0.202       0.0727        0.329        0.411         15.5        0.246\n",
            "      1    30        0.337        0.274       0.0631        0.381        0.478         14.7        0.229\n",
            "      1    40        0.189         0.11       0.0798        0.239        0.302         16.3        0.258\n",
            "      1    50        0.496        0.419       0.0774        0.481        0.591           16        0.254\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2        0.366        0.274        0.092        0.327        0.478         17.4        0.274\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   16.098    0.002        0.429        0.133        0.561        0.399        0.605         18.7        0.297\n",
            "! Validation          1   16.098    0.002        0.263       0.0874        0.351        0.314        0.469         16.7        0.262\n",
            "Wall time: 16.099436577999995\n",
            "! Best model        1    0.351\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10       0.0777       0.0703      0.00735        0.194        0.242         4.94       0.0784\n",
            "      2    20        0.759        0.522        0.237         0.47         0.66         28.5        0.445\n",
            "      2    30        0.365        0.322       0.0425         0.41        0.519         11.9        0.188\n",
            "      2    40        0.818        0.621        0.197        0.544         0.72         25.9        0.405\n",
            "      2    50        0.331        0.145        0.186        0.274        0.348         25.2        0.394\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2        0.276        0.213       0.0625        0.289        0.422           14         0.22\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   17.593    0.002        0.223       0.0815        0.305        0.292        0.436         14.8        0.234\n",
            "! Validation          2   17.593    0.002          0.2       0.0632        0.264        0.276        0.409         14.2        0.223\n",
            "Wall time: 17.59445267199999\n",
            "! Best model        2    0.264\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10       0.0842       0.0627       0.0215        0.183        0.229         8.43        0.134\n",
            "      3    20        0.159       0.0583          0.1        0.174        0.221         18.2        0.289\n",
            "      3    30         0.19        0.187      0.00277        0.312        0.396         3.08       0.0481\n",
            "      3    40       0.0748       0.0525       0.0224        0.167        0.209         8.61        0.137\n",
            "      3    50        0.319        0.221       0.0988        0.248        0.429         18.4        0.287\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2        0.234        0.168        0.065        0.261        0.375         14.4        0.226\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   19.391    0.002        0.179       0.0707         0.25        0.268        0.391         13.3        0.212\n",
            "! Validation          3   19.391    0.002        0.158       0.0613        0.219        0.249        0.363         13.7        0.216\n",
            "Wall time: 19.39341814600016\n",
            "! Best model        3    0.219\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0661       0.0512       0.0149        0.164        0.207         7.03        0.112\n",
            "      4    20         0.37        0.212        0.158        0.328        0.421         22.9        0.364\n",
            "      4    30        0.253         0.25      0.00283        0.353        0.457         3.06       0.0486\n",
            "      4    40        0.227        0.168       0.0593        0.298        0.375         14.2        0.223\n",
            "      4    50        0.125        0.119      0.00565        0.245        0.315          4.4       0.0687\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2         0.19        0.133       0.0565        0.235        0.334         13.4        0.211\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   21.368    0.002        0.145       0.0494        0.194        0.245        0.351         10.8        0.173\n",
            "! Validation          4   21.368    0.002        0.131       0.0537        0.184        0.227         0.33         12.9        0.202\n",
            "Wall time: 21.370358600000145\n",
            "! Best model        4    0.184\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0644       0.0466       0.0178        0.158        0.197         7.69        0.122\n",
            "      5    20       0.0228       0.0115       0.0114       0.0373       0.0979         1.17       0.0975\n",
            "      5    30        0.258        0.129         0.13        0.261        0.328         20.7        0.329\n",
            "      5    40        0.233        0.168       0.0653        0.298        0.374         14.7        0.234\n",
            "      5    50        0.171     6.44e-11        0.171     4.51e-06     7.33e-06         24.2        0.377\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2        0.159        0.123       0.0361        0.225        0.321         10.7        0.169\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   23.340    0.002        0.126       0.0426        0.169        0.232        0.328         9.94        0.158\n",
            "! Validation          5   23.340    0.002        0.119       0.0359        0.155        0.218        0.315         10.7        0.167\n",
            "Wall time: 23.34174522000012\n",
            "! Best model        5    0.155\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0373       0.0312      0.00611        0.133        0.161         4.57       0.0714\n",
            "      6    20        0.151         0.11       0.0404        0.245        0.304         11.8        0.184\n",
            "      6    30        0.297        0.197          0.1        0.333        0.405         18.5        0.289\n",
            "      6    40       0.0658       0.0532       0.0126        0.172        0.211         6.47        0.103\n",
            "      6    50       0.0921       0.0771        0.015        0.147        0.254         7.15        0.112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2         0.15        0.119       0.0314        0.221        0.315         9.89        0.156\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   24.829    0.002        0.114       0.0305        0.145        0.222        0.311         8.88         0.14\n",
            "! Validation          6   24.829    0.002        0.116       0.0308        0.147        0.215        0.312         9.59         0.15\n",
            "Wall time: 24.82983593600011\n",
            "! Best model        6    0.147\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0119      0.00907      0.00287       0.0368        0.087        0.587       0.0489\n",
            "      7    20         0.23        0.188       0.0414        0.325        0.396         11.9        0.186\n",
            "      7    30       0.0414         0.04      0.00138        0.144        0.183         2.14       0.0339\n",
            "      7    40        0.263        0.244       0.0186        0.361        0.451         7.98        0.125\n",
            "      7    50        0.113        0.082       0.0309        0.211        0.262         10.3        0.161\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2        0.151         0.13       0.0213        0.229        0.329         7.99        0.126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   26.154    0.002        0.119       0.0197        0.139        0.225        0.318         6.53        0.105\n",
            "! Validation          7   26.154    0.002        0.123       0.0213        0.144        0.219         0.32          7.8        0.122\n",
            "Wall time: 26.15526405500009\n",
            "! Best model        7    0.144\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10        0.169        0.144       0.0241        0.275        0.347         8.94        0.142\n",
            "      8    20       0.0409       0.0374      0.00345        0.142        0.177         3.38       0.0537\n",
            "      8    30        0.259        0.187       0.0717        0.311        0.396         15.7        0.245\n",
            "      8    40       0.0587       0.0516      0.00712        0.162        0.207         4.86       0.0771\n",
            "      8    50       0.0501       0.0433      0.00673         0.15         0.19         4.72        0.075\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2        0.142         0.12       0.0222        0.221        0.317         8.14        0.128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   27.469    0.002        0.109       0.0229        0.132        0.218        0.304          7.3        0.116\n",
            "! Validation          8   27.469    0.002        0.116       0.0222        0.138        0.214        0.311         7.93        0.124\n",
            "Wall time: 27.47042564200001\n",
            "! Best model        8    0.138\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10        0.152          0.1       0.0517        0.167        0.289         13.3        0.208\n",
            "      9    20        0.138        0.113       0.0259        0.244        0.307         9.26        0.147\n",
            "      9    30        0.121        0.115       0.0062         0.25        0.309         4.61        0.072\n",
            "      9    40      0.00977      0.00757      0.00221       0.0328       0.0795        0.515       0.0429\n",
            "      9    50       0.0504       0.0304         0.02        0.124        0.159         8.26        0.129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2        0.136        0.117       0.0192        0.218        0.312         7.56        0.119\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   28.784    0.002        0.104        0.022        0.126        0.214        0.297         7.31        0.116\n",
            "! Validation          9   28.784    0.002        0.113       0.0197        0.133        0.211        0.307          7.5        0.118\n",
            "Wall time: 28.78449959300019\n",
            "! Best model        9    0.133\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0417       0.0408     0.000837        0.146        0.185         1.67       0.0264\n",
            "     10    20       0.0658       0.0338       0.0321        0.134        0.168         10.3        0.164\n",
            "     10    30        0.159        0.159     0.000403        0.296        0.364         1.17       0.0183\n",
            "     10    40        0.211        0.168       0.0431          0.3        0.374         12.1         0.19\n",
            "     10    50        0.138        0.137     0.000344        0.266        0.338         1.07       0.0169\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2        0.133        0.115       0.0181        0.217         0.31         7.22        0.114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   30.092    0.002        0.104       0.0218        0.126        0.214        0.298         6.58        0.107\n",
            "! Validation         10   30.092    0.002        0.111       0.0186         0.13        0.209        0.305         7.13        0.112\n",
            "Wall time: 30.092846285000178\n",
            "! Best model       10    0.130\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10        0.113        0.101       0.0113        0.232        0.291          6.2       0.0969\n",
            "     11    20        0.274        0.163         0.11         0.29        0.369         19.4        0.303\n",
            "     11    30        0.132        0.132     3.01e-05        0.258        0.332        0.316      0.00501\n",
            "     11    40        0.278        0.197       0.0813        0.318        0.406         16.7         0.26\n",
            "     11    50        0.313        0.218       0.0944        0.336        0.427           18        0.281\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2        0.128        0.111       0.0177        0.214        0.304         7.13        0.112\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   31.398    0.002       0.0986       0.0216         0.12        0.209         0.29         6.79        0.108\n",
            "! Validation         11   31.398    0.002        0.108       0.0184        0.126        0.207          0.3         7.09        0.111\n",
            "Wall time: 31.399047548999988\n",
            "! Best model       11    0.126\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10       0.0613       0.0605     0.000861        0.179        0.225         1.69       0.0268\n",
            "     12    20       0.0675       0.0266       0.0409         0.12        0.149         11.8        0.185\n",
            "     12    30         0.23        0.178       0.0526        0.312        0.385         13.4         0.21\n",
            "     12    40       0.0315        0.031     0.000483        0.125        0.161         1.26       0.0201\n",
            "     12    50        0.063       0.0334       0.0296        0.133        0.167         10.1        0.157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2        0.126        0.111       0.0146        0.214        0.305         6.36          0.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   32.720    0.002       0.0971       0.0162        0.113        0.206        0.287         6.14       0.0987\n",
            "! Validation         12   32.720    0.002        0.107       0.0156        0.123        0.206        0.299         6.43        0.101\n",
            "Wall time: 32.720739902000105\n",
            "! Best model       12    0.123\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10        0.176        0.123       0.0528        0.255         0.32         13.2         0.21\n",
            "     13    20       0.0518       0.0418      0.00999        0.145        0.187         5.75       0.0913\n",
            "     13    30        0.161         0.15       0.0109        0.281        0.354         6.01       0.0953\n",
            "     13    40         0.15        0.148      0.00159        0.274        0.352         2.29       0.0364\n",
            "     13    50        0.234        0.207       0.0277        0.325        0.415         9.73        0.152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2        0.125        0.112       0.0133        0.215        0.305         5.94       0.0934\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   34.096    0.002        0.107       0.0153        0.122        0.215        0.301         5.83       0.0943\n",
            "! Validation         13   34.096    0.002        0.107       0.0144        0.121        0.205        0.299         6.02       0.0943\n",
            "Wall time: 34.09757367700013\n",
            "! Best model       13    0.121\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10          0.2        0.185       0.0154        0.316        0.393         7.13        0.113\n",
            "     14    20        0.129        0.112       0.0173        0.245        0.306         7.68         0.12\n",
            "     14    30       0.0857       0.0817        0.004          0.2        0.261          3.7       0.0578\n",
            "     14    40        0.202        0.178       0.0239        0.304        0.385         8.89        0.141\n",
            "     14    50        0.101       0.0952      0.00591        0.225        0.282          4.5       0.0703\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2         0.12        0.107        0.013        0.211        0.299         5.83       0.0917\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   35.877    0.002       0.0974       0.0203        0.118        0.207        0.287          6.8        0.108\n",
            "! Validation         14   35.877    0.002        0.104       0.0142        0.118        0.202        0.295         5.97       0.0935\n",
            "Wall time: 35.87804695\n",
            "! Best model       14    0.118\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.104       0.0825       0.0211        0.202        0.262          8.5        0.133\n",
            "     15    20       0.0774       0.0457       0.0317        0.154        0.195         10.3        0.163\n",
            "     15    30        0.163        0.155      0.00743        0.284         0.36         4.96       0.0788\n",
            "     15    40        0.121        0.117      0.00412        0.246        0.312         3.75       0.0586\n",
            "     15    50        0.143        0.138      0.00515        0.282        0.339         4.13       0.0656\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2        0.119        0.105       0.0134        0.209        0.296         5.84       0.0918\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   37.863    0.002       0.0905       0.0191         0.11        0.202        0.277         6.32        0.102\n",
            "! Validation         15   37.863    0.002        0.103       0.0147        0.117        0.201        0.293         5.92       0.0928\n",
            "Wall time: 37.86461178200011\n",
            "! Best model       15    0.117\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10        0.116       0.0722       0.0436        0.142        0.246         12.2        0.191\n",
            "     16    20       0.0469       0.0467     0.000247        0.114        0.197        0.918       0.0143\n",
            "     16    30       0.0429       0.0328       0.0101        0.129        0.165         5.79       0.0919\n",
            "     16    40        0.151        0.147      0.00412        0.276         0.35          3.7       0.0587\n",
            "     16    50        0.216        0.183       0.0326        0.318        0.391         10.6        0.165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2        0.117        0.105       0.0119        0.209        0.296          5.4       0.0848\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   39.816    0.002       0.0927       0.0124        0.105        0.202         0.28         5.23       0.0847\n",
            "! Validation         16   39.816    0.002        0.102       0.0134        0.115        0.201        0.292         5.63       0.0882\n",
            "Wall time: 39.817879889000096\n",
            "! Best model       16    0.115\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10        0.129        0.128      0.00129        0.264        0.327         2.06       0.0328\n",
            "     17    20       0.0435       0.0412      0.00228        0.146        0.185         2.75       0.0437\n",
            "     17    30       0.0346       0.0328      0.00183        0.132        0.165         2.46        0.039\n",
            "     17    40        0.239         0.19       0.0489        0.317        0.398         12.9        0.202\n",
            "     17    50        0.152        0.124       0.0278        0.256        0.322         9.76        0.152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2        0.113        0.101        0.012        0.205        0.291         5.43       0.0854\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   41.437    0.002       0.0934       0.0177        0.111        0.202        0.282         6.49        0.104\n",
            "! Validation         17   41.437    0.002       0.0992       0.0136        0.113        0.198        0.288          5.7       0.0894\n",
            "Wall time: 41.437974334000046\n",
            "! Best model       17    0.113\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10        0.121         0.12     8.69e-05        0.254        0.317        0.545      0.00852\n",
            "     18    20        0.015        0.014      0.00105       0.0428        0.108        0.355       0.0296\n",
            "     18    30       0.0541       0.0367       0.0174        0.143        0.175         7.59        0.121\n",
            "     18    40        0.193        0.137       0.0563        0.269        0.338         13.9        0.217\n",
            "     18    50       0.0968       0.0731       0.0238        0.197        0.247         9.02        0.141\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2        0.112        0.101       0.0117        0.205         0.29         5.28        0.083\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   42.755    0.002       0.0893       0.0163        0.106        0.198        0.275         6.11       0.0973\n",
            "! Validation         18   42.755    0.002       0.0985       0.0134        0.112        0.197        0.287         5.63       0.0882\n",
            "Wall time: 42.75604814400003\n",
            "! Best model       18    0.112\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10        0.122       0.0938       0.0281        0.222         0.28          9.8        0.153\n",
            "     19    20        0.157        0.115       0.0414         0.25         0.31         11.9        0.186\n",
            "     19    30        0.103        0.103     0.000308        0.169        0.293         1.03        0.016\n",
            "     19    40        0.157         0.14       0.0163        0.275        0.342         7.35        0.117\n",
            "     19    50       0.0303       0.0301     0.000151        0.129        0.159        0.719       0.0112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2         0.11       0.0986       0.0117        0.202        0.287         5.22       0.0819\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   44.052    0.002       0.0869       0.0134          0.1        0.196        0.271         5.55       0.0899\n",
            "! Validation         19   44.052    0.002       0.0966       0.0136         0.11        0.195        0.284         5.64       0.0884\n",
            "Wall time: 44.052739125000016\n",
            "! Best model       19    0.110\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.142        0.132         0.01        0.266        0.332         5.86       0.0916\n",
            "     20    20         0.14        0.138      0.00125        0.278         0.34         2.03       0.0323\n",
            "     20    30       0.0462       0.0353       0.0109        0.134        0.172         6.01       0.0954\n",
            "     20    40       0.0867       0.0866     0.000157        0.209        0.269        0.732       0.0114\n",
            "     20    50        0.197        0.152       0.0441        0.284        0.357         12.3        0.192\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2        0.109       0.0985       0.0106        0.202        0.287         4.84        0.076\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   45.382    0.002        0.089       0.0159        0.105        0.198        0.274         5.62       0.0908\n",
            "! Validation         20   45.382    0.002       0.0963       0.0125        0.109        0.194        0.284         5.39       0.0845\n",
            "Wall time: 45.383323692000204\n",
            "! Best model       20    0.109\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10      0.00958      0.00958     5.25e-06       0.0516       0.0894        0.134      0.00209\n",
            "     21    20        0.171        0.162      0.00896        0.293        0.368         5.53       0.0865\n",
            "     21    30        0.202        0.131       0.0711        0.269        0.331         15.6        0.244\n",
            "     21    40      0.00939     5.43e-11      0.00939     4.17e-06     6.74e-06         5.67       0.0886\n",
            "     21    50        0.133        0.125       0.0083        0.257        0.323         5.24       0.0832\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2        0.108       0.0962       0.0114          0.2        0.283         5.03       0.0791\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   46.678    0.002       0.0819       0.0165       0.0984        0.191        0.263         5.65       0.0891\n",
            "! Validation         21   46.678    0.002       0.0947       0.0134        0.108        0.193        0.281          5.6       0.0877\n",
            "Wall time: 46.679247928999985\n",
            "! Best model       21    0.108\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10        0.189        0.166       0.0231        0.215        0.373         8.88        0.139\n",
            "     22    20        0.163         0.15       0.0132        0.286        0.354          6.6        0.105\n",
            "     22    30       0.0829       0.0775       0.0054        0.203        0.254          4.3       0.0672\n",
            "     22    40       0.0587       0.0559      0.00279        0.172        0.216         3.04       0.0483\n",
            "     22    50        0.241        0.168       0.0727        0.296        0.374         15.8        0.246\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2        0.106       0.0952       0.0109        0.199        0.282         4.82       0.0756\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   47.990    0.002       0.0876       0.0149        0.102        0.195        0.272         5.73       0.0924\n",
            "! Validation         22   47.990    0.002       0.0936        0.013        0.107        0.192         0.28         5.48       0.0858\n",
            "Wall time: 47.99094174900006\n",
            "! Best model       22    0.107\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10       0.0955       0.0943      0.00129        0.225        0.281          2.1       0.0328\n",
            "     23    20        0.161         0.15       0.0117        0.278        0.354         6.24        0.099\n",
            "     23    30        0.155        0.136       0.0192        0.278        0.336         8.09        0.126\n",
            "     23    40        0.164         0.12       0.0437        0.253        0.317           12        0.191\n",
            "     23    50       0.0764       0.0434        0.033        0.149         0.19         10.5        0.166\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2        0.104       0.0933       0.0104        0.197        0.279         4.66       0.0732\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   49.304    0.002       0.0839       0.0163          0.1        0.192        0.266         5.87        0.095\n",
            "! Validation         23   49.304    0.002       0.0922       0.0126        0.105         0.19        0.277          5.4       0.0846\n",
            "Wall time: 49.30485614600002\n",
            "! Best model       23    0.105\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10         0.13        0.121      0.00897        0.253        0.318         5.45       0.0866\n",
            "     24    20       0.0491       0.0207       0.0284        0.106        0.131         9.86        0.154\n",
            "     24    30       0.0763       0.0742      0.00213        0.201        0.249          2.7       0.0422\n",
            "     24    40        0.198        0.129        0.069        0.262        0.328         15.1         0.24\n",
            "     24    50       0.0745       0.0357       0.0388       0.0997        0.173         11.5         0.18\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2        0.103       0.0928      0.00994        0.196        0.278         4.42       0.0694\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   50.634    0.002       0.0816       0.0148       0.0964        0.188        0.262         5.45       0.0877\n",
            "! Validation         24   50.634    0.002       0.0913       0.0122        0.103        0.189        0.276         5.26       0.0823\n",
            "Wall time: 50.63548598400007\n",
            "! Best model       24    0.103\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10        0.203        0.143       0.0597        0.278        0.345         14.3        0.223\n",
            "     25    20         0.11         0.11     5.47e-05        0.251        0.303        0.426      0.00676\n",
            "     25    30       0.0319         0.03      0.00187        0.123        0.158         2.49       0.0395\n",
            "     25    40        0.192        0.166       0.0258        0.298        0.373          9.4        0.147\n",
            "     25    50      0.00611     6.03e-11      0.00611     4.77e-06     7.09e-06         4.57       0.0714\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2        0.102       0.0921      0.00958        0.195        0.277          4.2       0.0659\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   52.368    0.002        0.082       0.0134       0.0954         0.19        0.263         5.19       0.0842\n",
            "! Validation         25   52.368    0.002       0.0904        0.012        0.102        0.188        0.275         5.14       0.0805\n",
            "Wall time: 52.36945648000005\n",
            "! Best model       25    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10        0.118        0.106       0.0122        0.239        0.297         6.35        0.101\n",
            "     26    20       0.0522        0.022       0.0303        0.109        0.135         10.2        0.159\n",
            "     26    30        0.102       0.0896       0.0124        0.158        0.273         6.51        0.102\n",
            "     26    40        0.125        0.115      0.00975        0.251         0.31         5.78       0.0902\n",
            "     26    50        0.134        0.127      0.00698        0.256        0.326         4.81       0.0763\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2          0.1       0.0899       0.0102        0.193        0.274         4.41       0.0691\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   54.292    0.002       0.0802       0.0237        0.104        0.189         0.26         6.82        0.108\n",
            "! Validation         26   54.292    0.002       0.0889       0.0126        0.102        0.186        0.272         5.32       0.0833\n",
            "Wall time: 54.29318391400011\n",
            "! Best model       26    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10        0.229        0.227      0.00192        0.342        0.435         2.56       0.0401\n",
            "     27    20         0.14        0.108       0.0323        0.241          0.3         10.3        0.164\n",
            "     27    30        0.127       0.0906       0.0362         0.22        0.275         11.1        0.174\n",
            "     27    40       0.0362        0.036     0.000178        0.134        0.173        0.768       0.0122\n",
            "     27    50       0.0747       0.0747     3.01e-09        0.202         0.25       0.0032     5.01e-05\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2       0.0995         0.09      0.00952        0.192        0.274         4.18       0.0655\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   56.301    0.002       0.0837       0.0207        0.104        0.189        0.266         6.39        0.105\n",
            "! Validation         27   56.301    0.002       0.0883       0.0119          0.1        0.185        0.271         5.15       0.0807\n",
            "Wall time: 56.303499583000075\n",
            "! Best model       27    0.100\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10        0.067       0.0226       0.0445        0.108        0.137         12.3        0.193\n",
            "     28    20       0.0737       0.0712      0.00243        0.195        0.244         2.88       0.0451\n",
            "     28    30        0.212        0.164       0.0479        0.292         0.37         12.8          0.2\n",
            "     28    40        0.239        0.161       0.0777        0.297        0.367         16.3        0.255\n",
            "     28    50        0.119        0.117      0.00225        0.252        0.313         2.73       0.0434\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2       0.0983       0.0893      0.00901        0.192        0.273         4.01       0.0629\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   58.114    0.002       0.0793       0.0209          0.1        0.186        0.259         6.48        0.106\n",
            "! Validation         28   58.114    0.002       0.0875       0.0113       0.0988        0.184         0.27         5.03       0.0787\n",
            "Wall time: 58.11439637400008\n",
            "! Best model       28    0.099\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10       0.0135     5.91e-11       0.0135     4.66e-06     7.03e-06         6.79        0.106\n",
            "     29    20        0.161        0.127       0.0342         0.26        0.325         10.8        0.169\n",
            "     29    30        0.134        0.134     0.000124        0.264        0.334        0.641       0.0102\n",
            "     29    40        0.103       0.0694       0.0337        0.186        0.241         10.7        0.168\n",
            "     29    50       0.0551        0.051      0.00409        0.119        0.206         3.74       0.0584\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2       0.0981       0.0893      0.00875        0.192        0.273         3.85       0.0604\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   59.429    0.002       0.0805       0.0163       0.0968        0.186        0.261         5.84       0.0948\n",
            "! Validation         29   59.429    0.002       0.0871       0.0111       0.0982        0.184         0.27         4.91       0.0768\n",
            "Wall time: 59.43028433000018\n",
            "! Best model       29    0.098\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10        0.108       0.0893       0.0186         0.22        0.273         7.97        0.125\n",
            "     30    20        0.024       0.0194      0.00461        0.102        0.127         3.97        0.062\n",
            "     30    30       0.0447       0.0139       0.0308       0.0867        0.108         10.3         0.16\n",
            "     30    40        0.102        0.101     0.000203        0.235        0.291        0.833        0.013\n",
            "     30    50       0.0363        0.033      0.00327        0.131        0.166         3.29       0.0523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2       0.0965       0.0875      0.00906         0.19         0.27         4.06       0.0636\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   60.725    0.002       0.0738       0.0174       0.0913         0.18        0.249         6.18       0.0992\n",
            "! Validation         30   60.725    0.002       0.0856       0.0114        0.097        0.182        0.267         5.06       0.0792\n",
            "Wall time: 60.72593052800016\n",
            "! Best model       30    0.097\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10       0.0199       0.0162      0.00367       0.0944        0.116         3.54       0.0554\n",
            "     31    20       0.0548       0.0183       0.0364       0.0472        0.124         2.09        0.174\n",
            "     31    30         0.16        0.119       0.0407        0.254        0.316         11.8        0.184\n",
            "     31    40        0.108        0.099       0.0088        0.221        0.288          5.4       0.0857\n",
            "     31    50      0.00543      5.8e-11      0.00543     4.62e-06     6.96e-06         4.31       0.0674\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2       0.0945       0.0857      0.00881        0.188        0.268         4.01        0.063\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   62.034    0.002       0.0745       0.0129       0.0874         0.18        0.251         4.89       0.0811\n",
            "! Validation         31   62.034    0.002       0.0841       0.0111       0.0952        0.181        0.265         5.02       0.0786\n",
            "Wall time: 62.0345880320001\n",
            "! Best model       31    0.095\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10        0.124        0.106       0.0171        0.172        0.298         7.64        0.119\n",
            "     32    20       0.0097     5.55e-11       0.0097     4.53e-06     6.81e-06         5.76         0.09\n",
            "     32    30        0.125        0.109       0.0153        0.246        0.302         7.12        0.113\n",
            "     32    40         0.07       0.0674      0.00262        0.193        0.237         2.99       0.0468\n",
            "     32    50       0.0196       0.0173       0.0023       0.0946         0.12          2.8       0.0438\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2       0.0928       0.0837      0.00903        0.186        0.264         4.09       0.0641\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   63.335    0.002       0.0725       0.0121       0.0847        0.178        0.247         5.16       0.0834\n",
            "! Validation         32   63.335    0.002       0.0825       0.0114       0.0939        0.179        0.262          5.1       0.0798\n",
            "Wall time: 63.33574177100013\n",
            "! Best model       32    0.094\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10        0.172        0.154       0.0184        0.207        0.358         7.94        0.124\n",
            "     33    20       0.0416       0.0408     0.000748        0.144        0.185         1.57        0.025\n",
            "     33    30        0.124        0.114      0.00973        0.242        0.309         5.68       0.0901\n",
            "     33    40        0.209        0.149       0.0604        0.275        0.353         14.4        0.225\n",
            "     33    50       0.0334        0.033     0.000404        0.131        0.166         1.16       0.0184\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2       0.0919        0.083      0.00894        0.185        0.263            4       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   64.640    0.002       0.0781       0.0165       0.0947        0.182        0.256         5.63       0.0927\n",
            "! Validation         33   64.640    0.002       0.0818       0.0113       0.0931        0.179        0.261         5.04       0.0789\n",
            "Wall time: 64.64077707500019\n",
            "! Best model       33    0.093\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10        0.129        0.117       0.0124        0.252        0.312         6.51        0.102\n",
            "     34    20       0.0526       0.0171       0.0355       0.0964         0.12           11        0.172\n",
            "     34    30        0.127        0.114       0.0131        0.249        0.308         6.58        0.105\n",
            "     34    40         0.17        0.116       0.0538         0.18        0.312         13.6        0.212\n",
            "     34    50       0.0785       0.0785     6.71e-06        0.148        0.256        0.151      0.00237\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2       0.0897        0.081      0.00867        0.183         0.26            4       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   65.999    0.002       0.0728       0.0175       0.0903        0.177        0.247         6.01        0.095\n",
            "! Validation         34   65.999    0.002         0.08        0.011        0.091        0.177        0.258         5.01       0.0784\n",
            "Wall time: 66.00008639900011\n",
            "! Best model       34    0.091\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10         0.12        0.101       0.0184        0.232        0.291          7.8        0.124\n",
            "     35    20      0.00786     5.94e-11      0.00786     4.79e-06     7.05e-06         5.19        0.081\n",
            "     35    30       0.0928       0.0602       0.0326        0.179        0.224         10.6        0.165\n",
            "     35    40        0.117        0.108      0.00984        0.243          0.3          5.8       0.0906\n",
            "     35    50     0.000189     7.73e-11     0.000189     5.09e-06     8.03e-06        0.804       0.0126\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2       0.0882       0.0797      0.00852        0.181        0.258         3.97       0.0623\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   67.329    0.002       0.0706       0.0179       0.0884        0.174        0.244         5.94       0.0966\n",
            "! Validation         35   67.329    0.002       0.0787       0.0108       0.0895        0.175        0.256         4.98       0.0779\n",
            "Wall time: 67.32975756100018\n",
            "! Best model       35    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10        0.225        0.195       0.0304        0.308        0.403         10.2        0.159\n",
            "     36    20       0.0361       0.0357     0.000431        0.131        0.173          1.2        0.019\n",
            "     36    30        0.122        0.105       0.0161        0.238        0.297         7.31        0.116\n",
            "     36    40       0.0257       0.0253     0.000448        0.112        0.145         1.22       0.0193\n",
            "     36    50        0.122        0.114       0.0078        0.243        0.309         5.08       0.0807\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2       0.0874       0.0795      0.00796        0.181        0.258         3.72       0.0583\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   68.995    0.002        0.073        0.012       0.0849        0.177        0.248            5       0.0816\n",
            "! Validation         36   68.995    0.002       0.0781       0.0102       0.0883        0.175        0.255         4.79        0.075\n",
            "Wall time: 68.9977623650002\n",
            "! Best model       36    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10       0.0442       0.0437     0.000519        0.151        0.191         1.33       0.0208\n",
            "     37    20       0.0498       0.0381       0.0117        0.138        0.178         6.32       0.0988\n",
            "     37    30       0.0281       0.0273     0.000792        0.117        0.151         1.62       0.0257\n",
            "     37    40       0.0277       0.0249      0.00278        0.111        0.144         3.04       0.0482\n",
            "     37    50       0.0806       0.0599       0.0207         0.18        0.224         8.42        0.132\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2        0.086       0.0786       0.0074         0.18        0.256         3.53       0.0553\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   70.866    0.002       0.0718       0.0108       0.0826        0.174        0.245         4.61        0.074\n",
            "! Validation         37   70.866    0.002       0.0771      0.00955       0.0867        0.174        0.254         4.64       0.0727\n",
            "Wall time: 70.86770910500013\n",
            "! Best model       37    0.087\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10       0.0481       0.0481     3.06e-05        0.156          0.2        0.324      0.00506\n",
            "     38    20        0.119        0.113      0.00519         0.25        0.308         4.21       0.0658\n",
            "     38    30       0.0567        0.045       0.0117        0.112        0.194         6.33       0.0989\n",
            "     38    40        0.115        0.109       0.0057        0.241        0.302         4.35        0.069\n",
            "     38    50       0.0256       0.0254     0.000155         0.11        0.146        0.717       0.0114\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2       0.0837       0.0758      0.00784        0.177        0.252         3.72       0.0584\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   72.863    0.002       0.0661       0.0139         0.08        0.169        0.236         5.31       0.0858\n",
            "! Validation         38   72.863    0.002        0.075       0.0101        0.085        0.172         0.25          4.8       0.0751\n",
            "Wall time: 72.86528628100018\n",
            "! Best model       38    0.085\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10       0.0708       0.0706     0.000185        0.196        0.243        0.795       0.0124\n",
            "     39    20       0.0442       0.0295       0.0147        0.127        0.157         6.97        0.111\n",
            "     39    30        0.193        0.153       0.0392        0.274        0.358         11.6        0.181\n",
            "     39    40       0.0391       0.0304      0.00868        0.122        0.159         5.36       0.0851\n",
            "     39    50         0.12        0.108        0.012        0.241          0.3          6.3          0.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2       0.0819       0.0746      0.00731        0.175         0.25         3.54       0.0554\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   74.862    0.002       0.0677       0.0129       0.0806         0.17        0.238         5.42        0.087\n",
            "! Validation         39   74.862    0.002       0.0738      0.00949       0.0833        0.171        0.248         4.67        0.073\n",
            "Wall time: 74.86277496700018\n",
            "! Best model       39    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10       0.0194       0.0151      0.00429       0.0896        0.112         3.83       0.0598\n",
            "     40    20       0.0306       0.0255      0.00502        0.111        0.146         4.08       0.0648\n",
            "     40    30       0.0813       0.0813        4e-05        0.203        0.261        0.364      0.00578\n",
            "     40    40        0.103       0.0996       0.0039        0.166        0.288         3.65       0.0571\n",
            "     40    50         0.14        0.123       0.0173        0.263         0.32          7.7         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2       0.0805       0.0731      0.00735        0.174        0.247         3.53       0.0553\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   76.173    0.002       0.0667       0.0131       0.0798        0.169        0.237         4.88       0.0805\n",
            "! Validation         40   76.173    0.002       0.0724      0.00957        0.082        0.169        0.246         4.66       0.0728\n",
            "Wall time: 76.1743991960002\n",
            "! Best model       40    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10       0.0295       0.0291     0.000388        0.124        0.156         1.13        0.018\n",
            "     41    20       0.0824       0.0677       0.0146        0.192        0.238         7.07        0.111\n",
            "     41    30       0.0955       0.0938      0.00163         0.22         0.28         2.33       0.0369\n",
            "     41    40       0.0584       0.0163       0.0421        0.092        0.117           12        0.187\n",
            "     41    50        0.181        0.117       0.0645        0.254        0.312         14.8        0.232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2       0.0799       0.0728      0.00708        0.173        0.247         3.42       0.0536\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   77.481    0.002       0.0672       0.0128         0.08         0.17        0.237          5.1       0.0819\n",
            "! Validation         41   77.481    0.002        0.072      0.00926       0.0812        0.169        0.245         4.56       0.0714\n",
            "Wall time: 77.48225752799999\n",
            "! Best model       41    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10        0.123        0.118      0.00508         0.25        0.314          4.1       0.0651\n",
            "     42    20       0.0931       0.0781        0.015        0.199        0.255         7.04        0.112\n",
            "     42    30        0.028       0.0142       0.0139       0.0859        0.109         6.89        0.108\n",
            "     42    40     3.37e-05     7.69e-11     3.37e-05     5.44e-06     8.01e-06        0.339       0.0053\n",
            "     42    50        0.128        0.121      0.00654        0.184        0.318         4.73       0.0739\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2       0.0779       0.0713      0.00655        0.172        0.244         3.25        0.051\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   78.796    0.002       0.0681      0.00945       0.0776        0.169        0.239         4.69       0.0771\n",
            "! Validation         42   78.796    0.002       0.0704      0.00867       0.0791        0.167        0.242         4.43       0.0693\n",
            "Wall time: 78.7971429690001\n",
            "! Best model       42    0.079\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10        0.114        0.111      0.00356        0.236        0.304         3.44       0.0545\n",
            "     43    20        0.059       0.0565      0.00249        0.176        0.217         2.92       0.0456\n",
            "     43    30       0.0872       0.0871     0.000131        0.212         0.27        0.659       0.0105\n",
            "     43    40        0.193        0.116       0.0773        0.241        0.311         16.3        0.254\n",
            "     43    50        0.142        0.114       0.0282        0.178        0.309         9.82        0.153\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2       0.0768       0.0704      0.00637        0.171        0.243         3.24       0.0507\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   80.132    0.002       0.0683       0.0136       0.0819        0.169         0.24         5.12       0.0832\n",
            "! Validation         43   80.132    0.002       0.0698      0.00841       0.0782        0.166        0.241         4.39       0.0687\n",
            "Wall time: 80.1334141100001\n",
            "! Best model       43    0.078\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10       0.0162       0.0141      0.00211       0.0876        0.109         2.68       0.0419\n",
            "     44    20        0.029       0.0234      0.00553        0.108         0.14         4.28        0.068\n",
            "     44    30       0.0396       0.0389     0.000698        0.139         0.18         1.54       0.0241\n",
            "     44    40        0.127        0.122      0.00487        0.252         0.32         4.08       0.0638\n",
            "     44    50       0.0359       0.0348      0.00112        0.132         0.17         1.93       0.0306\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2        0.077       0.0709      0.00612        0.171        0.243         3.08       0.0482\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   81.476    0.002       0.0658      0.00955       0.0753        0.167        0.235         4.39       0.0723\n",
            "! Validation         44   81.476    0.002       0.0699      0.00817       0.0781        0.167        0.242         4.27       0.0668\n",
            "Wall time: 81.47654692600008\n",
            "! Best model       44    0.078\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10        0.018       0.0109      0.00712       0.0754       0.0954         4.94       0.0771\n",
            "     45    20        0.118         0.11      0.00819        0.175        0.303         5.29       0.0827\n",
            "     45    30       0.0667       0.0648       0.0019        0.187        0.233         2.55       0.0399\n",
            "     45    40       0.0435       0.0325        0.011        0.134        0.165         6.03       0.0957\n",
            "     45    50        0.129        0.123       0.0055        0.249        0.321         4.27       0.0678\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2       0.0763       0.0702      0.00608        0.171        0.242         3.07       0.0481\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   82.810    0.002       0.0657       0.0127       0.0784        0.166        0.234         4.89       0.0797\n",
            "! Validation         45   82.810    0.002       0.0693      0.00815       0.0774        0.166         0.24         4.27       0.0667\n",
            "Wall time: 82.81088046700006\n",
            "! Best model       45    0.077\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10          0.1       0.0995      0.00057        0.232        0.288         1.37       0.0218\n",
            "     46    20       0.0139       0.0138     0.000104       0.0864        0.107        0.596      0.00931\n",
            "     46    30       0.0463       0.0343        0.012         0.13        0.169         6.29       0.0999\n",
            "     46    40       0.0834       0.0694        0.014        0.139        0.241         6.91        0.108\n",
            "     46    50       0.0868       0.0775      0.00932        0.205        0.254         5.65       0.0882\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2       0.0751       0.0694      0.00571         0.17        0.241         2.93       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   84.116    0.002       0.0666       0.0111       0.0777        0.167        0.236         4.45       0.0738\n",
            "! Validation         46   84.116    0.002       0.0686      0.00767       0.0763        0.166        0.239         4.14       0.0648\n",
            "Wall time: 84.11712977399998\n",
            "! Best model       46    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10        0.113        0.095       0.0181        0.224        0.282         7.74        0.123\n",
            "     47    20        0.274        0.182        0.092        0.302         0.39         17.7        0.277\n",
            "     47    30       0.0363       0.0347      0.00159        0.141         0.17         2.29       0.0364\n",
            "     47    40        0.112       0.0954       0.0164        0.226        0.282         7.37        0.117\n",
            "     47    50        0.153        0.128       0.0248        0.189        0.328         9.22        0.144\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2       0.0736       0.0674       0.0062        0.167        0.237         3.29       0.0516\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   85.674    0.002       0.0667       0.0149       0.0816        0.167        0.237         5.86       0.0949\n",
            "! Validation         47   85.674    0.002       0.0675      0.00812       0.0756        0.164        0.237         4.38       0.0685\n",
            "Wall time: 85.67590357000017\n",
            "! Best model       47    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10       0.0901       0.0898      0.00027        0.214        0.274        0.945        0.015\n",
            "     48    20         0.13        0.108       0.0216        0.163        0.301         4.84        0.134\n",
            "     48    30       0.0306       0.0227      0.00786        0.106        0.138          5.1        0.081\n",
            "     48    40       0.0518       0.0517     3.56e-05        0.163        0.208        0.343      0.00545\n",
            "     48    50       0.0824       0.0807      0.00169        0.206         0.26          2.4       0.0376\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2       0.0738       0.0681      0.00572        0.168        0.238         3.11       0.0487\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   87.526    0.002       0.0646       0.0114        0.076        0.165        0.233         5.06       0.0826\n",
            "! Validation         48   87.526    0.002        0.068      0.00752       0.0755        0.165        0.238         4.19       0.0656\n",
            "Wall time: 87.52810144099999\n",
            "! Best model       48    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10       0.0305        0.011       0.0195       0.0756       0.0957         8.16        0.128\n",
            "     49    20        0.123        0.119      0.00417        0.249        0.315         3.72        0.059\n",
            "     49    30        0.157        0.107       0.0499        0.238        0.299         13.1        0.204\n",
            "     49    40        0.135        0.124       0.0109        0.186        0.321         6.11       0.0955\n",
            "     49    50       0.0662       0.0564      0.00974        0.125        0.217         5.77       0.0902\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2       0.0732       0.0677      0.00548        0.167        0.238         3.01       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   89.527    0.002       0.0656       0.0137       0.0793        0.165        0.234         5.57       0.0906\n",
            "! Validation         49   89.527    0.002       0.0673      0.00727       0.0746        0.164        0.237          4.1       0.0641\n",
            "Wall time: 89.52865907800015\n",
            "! Best model       49    0.075\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10        0.117        0.117     6.55e-05        0.249        0.313        0.466       0.0074\n",
            "     50    20       0.0366       0.0329      0.00365        0.134        0.166         3.48       0.0552\n",
            "     50    30        0.103       0.0984      0.00427        0.231        0.287         3.76       0.0597\n",
            "     50    40       0.0159       0.0115      0.00442       0.0779        0.098         3.89       0.0608\n",
            "     50    50       0.0329       0.0225       0.0103        0.106        0.137         5.85       0.0929\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2       0.0737       0.0684      0.00526        0.168        0.239         2.82       0.0442\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   91.474    0.002        0.066       0.0095       0.0755        0.166        0.235         4.23       0.0698\n",
            "! Validation         50   91.474    0.002       0.0677      0.00707       0.0748        0.165        0.238         3.98       0.0622\n",
            "Wall time: 91.47561553300011\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10        0.078       0.0747      0.00336        0.192         0.25         3.34        0.053\n",
            "     51    20        0.138          0.1       0.0373        0.229        0.289         11.3        0.177\n",
            "     51    30       0.0934       0.0885      0.00492        0.217        0.272         4.04       0.0641\n",
            "     51    40       0.0314       0.0309     0.000458         0.12        0.161         1.23       0.0196\n",
            "     51    50       0.0121       0.0112      0.00089       0.0763       0.0968         1.74       0.0273\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2       0.0725       0.0673      0.00513        0.167        0.237         2.82       0.0443\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   92.880    0.002       0.0648       0.0139       0.0787        0.165        0.233         5.47       0.0887\n",
            "! Validation         51   92.880    0.002        0.067       0.0069       0.0739        0.164        0.237         3.97       0.0622\n",
            "Wall time: 92.88123096900017\n",
            "! Best model       51    0.074\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10        0.145        0.125       0.0206        0.247        0.323         8.26        0.131\n",
            "     52    20        0.103       0.0733       0.0296        0.197        0.247          9.9        0.157\n",
            "     52    30         0.13       0.0958       0.0345        0.225        0.283         10.9         0.17\n",
            "     52    40         0.22        0.151       0.0687        0.287        0.355         15.3         0.24\n",
            "     52    50        0.093        0.077        0.016        0.199        0.254         7.29        0.116\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2       0.0715       0.0655        0.006        0.165        0.234         3.36       0.0526\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   94.171    0.002       0.0644       0.0234       0.0878        0.165        0.233          7.2        0.119\n",
            "! Validation         52   94.171    0.002       0.0658      0.00782       0.0736        0.162        0.234         4.36       0.0682\n",
            "Wall time: 94.17162295600019\n",
            "! Best model       52    0.074\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10       0.0486        0.017       0.0316        0.047        0.119         1.95        0.162\n",
            "     53    20       0.0568       0.0542      0.00262        0.171        0.213         2.95       0.0468\n",
            "     53    30       0.0892       0.0856      0.00359        0.211        0.267         3.45       0.0547\n",
            "     53    40        0.218        0.161       0.0574        0.296        0.366           14        0.219\n",
            "     53    50        0.119        0.102       0.0172         0.23        0.292         7.68         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2       0.0715       0.0657      0.00585        0.164        0.234         3.31       0.0519\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   95.476    0.002       0.0652       0.0142       0.0794        0.165        0.234         5.69       0.0924\n",
            "! Validation         53   95.476    0.002       0.0661      0.00757       0.0737        0.162        0.235         4.29       0.0671\n",
            "Wall time: 95.47657163000008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10       0.0907       0.0907     1.29e-05         0.22        0.275         0.21      0.00328\n",
            "     54    20       0.0939       0.0829        0.011        0.152        0.263         6.14       0.0959\n",
            "     54    30       0.0283       0.0276     0.000695        0.108        0.152         1.52       0.0241\n",
            "     54    40        0.141        0.109       0.0321        0.239        0.301         10.5        0.164\n",
            "     54    50       0.0512       0.0317       0.0195        0.122        0.163         8.04        0.128\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2        0.072       0.0665       0.0055        0.165        0.236         3.12       0.0489\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   96.819    0.002       0.0641       0.0105       0.0747        0.163        0.232         4.68       0.0768\n",
            "! Validation         54   96.819    0.002       0.0665       0.0072       0.0737        0.163        0.236         4.14       0.0648\n",
            "Wall time: 96.81964709099998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10        0.132       0.0855        0.047        0.215        0.267         12.5        0.198\n",
            "     55    20        0.118       0.0931       0.0249        0.226        0.279         9.09        0.144\n",
            "     55    30        0.129         0.12      0.00872        0.174        0.317         3.07       0.0854\n",
            "     55    40        0.167        0.152       0.0154        0.271        0.356         7.25        0.113\n",
            "     55    50       0.0136       0.0113      0.00232        0.077       0.0971         2.81        0.044\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2       0.0711       0.0654      0.00571        0.164        0.234         3.23       0.0506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   98.136    0.002       0.0622       0.0121       0.0743        0.162        0.228          5.2       0.0845\n",
            "! Validation         55   98.136    0.002       0.0656       0.0075       0.0731        0.162        0.234         4.25       0.0665\n",
            "Wall time: 98.1371862750002\n",
            "! Best model       55    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10         0.11        0.103      0.00684        0.233        0.293         4.76       0.0756\n",
            "     56    20       0.0327       0.0306      0.00205         0.12         0.16         2.61       0.0414\n",
            "     56    30        0.144        0.136      0.00803        0.186        0.337         2.95       0.0819\n",
            "     56    40       0.0279       0.0273     0.000596        0.115        0.151         1.41       0.0223\n",
            "     56    50       0.0294       0.0294     4.34e-05        0.116        0.157        0.379      0.00602\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2       0.0707       0.0651      0.00562        0.164        0.233         3.15       0.0493\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   99.448    0.002       0.0638       0.0112        0.075        0.164        0.231         4.78       0.0788\n",
            "! Validation         56   99.448    0.002       0.0652      0.00745       0.0727        0.162        0.233         4.21       0.0658\n",
            "Wall time: 99.45066124100003\n",
            "! Best model       56    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10        0.131        0.103       0.0282         0.24        0.293         9.83        0.154\n",
            "     57    20       0.0348       0.0261      0.00877         0.11        0.148         5.39       0.0856\n",
            "     57    30       0.0315       0.0312     0.000291        0.131        0.161        0.982       0.0156\n",
            "     57    40       0.0335       0.0326     0.000985        0.122        0.165         1.81       0.0287\n",
            "     57    50        0.198        0.147       0.0514         0.28         0.35         13.3        0.207\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2       0.0705       0.0649      0.00566        0.164        0.233         3.15       0.0495\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57  100.809    0.002       0.0647      0.00963       0.0744        0.165        0.233         4.53        0.074\n",
            "! Validation         57  100.809    0.002       0.0652      0.00749       0.0727        0.161        0.233         4.21       0.0659\n",
            "Wall time: 100.80991286300014\n",
            "! Best model       57    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10       0.0459       0.0341       0.0119         0.13        0.169         6.28       0.0996\n",
            "     58    20        0.119        0.115      0.00384        0.247         0.31         3.57       0.0566\n",
            "     58    30       0.0944       0.0894        0.005        0.222        0.273         4.07       0.0646\n",
            "     58    40       0.0955       0.0876      0.00787         0.21        0.271         5.11       0.0811\n",
            "     58    50        0.138       0.0892       0.0491        0.219        0.273         12.8        0.202\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2       0.0703       0.0647       0.0056        0.163        0.233         3.08       0.0483\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58  102.264    0.002       0.0638       0.0114       0.0752        0.163        0.231         4.74       0.0801\n",
            "! Validation         58  102.264    0.002       0.0647      0.00749       0.0722        0.161        0.232         4.17       0.0653\n",
            "Wall time: 102.26593335000007\n",
            "! Best model       58    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10        0.115         0.11      0.00504        0.249        0.303         4.15       0.0649\n",
            "     59    20       0.0715       0.0695      0.00199         0.19        0.241         2.61       0.0408\n",
            "     59    30        0.113        0.109       0.0039        0.251        0.302         3.65       0.0571\n",
            "     59    40       0.0274       0.0264        0.001        0.108        0.148         1.82       0.0289\n",
            "     59    50       0.0806       0.0792      0.00144        0.209        0.257         2.19       0.0347\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2       0.0698        0.064      0.00586        0.162        0.231         3.27       0.0513\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59  104.043    0.002       0.0627       0.0114       0.0741        0.163        0.229         5.11       0.0829\n",
            "! Validation         59  104.043    0.002       0.0644      0.00774       0.0722         0.16        0.232         4.31       0.0675\n",
            "Wall time: 104.0445039350002\n",
            "! Best model       59    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10       0.0772       0.0745      0.00265          0.2        0.249         3.01        0.047\n",
            "     60    20        0.131        0.107        0.024        0.236        0.299         9.05        0.141\n",
            "     60    30        0.144        0.119       0.0257         0.26        0.315         9.38        0.147\n",
            "     60    40       0.0401        0.038      0.00208        0.129        0.178         2.62       0.0416\n",
            "     60    50       0.0536       0.0241       0.0295        0.109        0.142         9.89        0.157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2       0.0695       0.0639      0.00561        0.162        0.231          3.2       0.0501\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60  106.037    0.002       0.0617       0.0112       0.0729        0.162        0.228         4.68       0.0777\n",
            "! Validation         60  106.037    0.002       0.0645      0.00741       0.0719         0.16        0.232         4.23       0.0662\n",
            "Wall time: 106.03901392000012\n",
            "! Best model       60    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10        0.182        0.118       0.0637        0.244        0.314         14.8        0.231\n",
            "     61    20       0.0211       0.0139      0.00719       0.0849        0.108         4.96       0.0775\n",
            "     61    30        0.115        0.109       0.0055        0.242        0.302         4.27       0.0678\n",
            "     61    40        0.177        0.157       0.0197        0.288        0.363          8.2        0.128\n",
            "     61    50       0.0658       0.0148        0.051       0.0874        0.111         13.2        0.206\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2       0.0694       0.0642      0.00517        0.162        0.232            3       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61  108.016    0.002       0.0641       0.0161       0.0802        0.166        0.232         6.14        0.101\n",
            "! Validation         61  108.016    0.002       0.0646      0.00695       0.0716        0.161        0.232         4.08       0.0639\n",
            "Wall time: 108.01846257500006\n",
            "! Best model       61    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10     5.34e-07     8.68e-11     5.34e-07     5.35e-06     8.51e-06       0.0427     0.000668\n",
            "     62    20        0.152        0.134       0.0183        0.273        0.334         7.91        0.124\n",
            "     62    30       0.0562        0.052      0.00418        0.166        0.208         3.72       0.0591\n",
            "     62    40       0.0276       0.0275     8.21e-05         0.11        0.151        0.522      0.00828\n",
            "     62    50       0.0571        0.055       0.0021        0.169        0.214         2.68       0.0419\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2       0.0696       0.0647      0.00493        0.163        0.232         2.83       0.0444\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62  109.609    0.002       0.0655       0.0135        0.079        0.166        0.234         4.97        0.082\n",
            "! Validation         62  109.609    0.002       0.0646      0.00679       0.0713        0.161        0.232         3.99       0.0624\n",
            "Wall time: 109.61030814800006\n",
            "! Best model       62    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10       0.0831       0.0814      0.00169        0.208        0.261         2.36       0.0375\n",
            "     63    20        0.145        0.104       0.0411        0.236        0.294         11.9        0.185\n",
            "     63    30       0.0159       0.0112      0.00465       0.0764       0.0968         3.99       0.0623\n",
            "     63    40        0.084       0.0685       0.0155        0.194        0.239         7.27        0.114\n",
            "     63    50       0.0544       0.0515      0.00295         0.12        0.207         3.17       0.0496\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2       0.0691       0.0635      0.00557        0.162         0.23         3.11       0.0488\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63  110.931    0.002        0.062       0.0114       0.0734        0.161        0.228         4.65       0.0762\n",
            "! Validation         63  110.931    0.002       0.0638      0.00753       0.0713         0.16        0.231         4.21       0.0659\n",
            "Wall time: 110.93184176099999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10       0.0222       0.0219     0.000301        0.103        0.135        0.998       0.0158\n",
            "     64    20       0.0424       0.0264        0.016        0.109        0.148         7.27        0.115\n",
            "     64    30       0.0415       0.0197       0.0218       0.0502        0.128         1.62        0.135\n",
            "     64    40       0.0198       0.0137       0.0061       0.0841        0.107         4.57       0.0714\n",
            "     64    50         0.15        0.142      0.00846        0.277        0.344         5.38       0.0841\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2       0.0689       0.0636      0.00533        0.162         0.23         2.99       0.0469\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64  112.234    0.002       0.0622       0.0101       0.0723        0.162        0.228         4.59       0.0755\n",
            "! Validation         64  112.234    0.002       0.0639      0.00723       0.0711         0.16        0.231         4.11       0.0643\n",
            "Wall time: 112.23479097900008\n",
            "! Best model       64    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10       0.0973        0.096      0.00129        0.219        0.283         2.07       0.0328\n",
            "     65    20       0.0171       0.0151        0.002         0.09        0.112         2.62       0.0409\n",
            "     65    30       0.0349       0.0254      0.00956       0.0538        0.146         1.07       0.0894\n",
            "     65    40        0.184        0.183     0.000229        0.226        0.391        0.885       0.0138\n",
            "     65    50        0.127        0.123      0.00385        0.265        0.321         3.63       0.0567\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2       0.0679       0.0629      0.00504         0.16        0.229         2.89       0.0453\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65  113.554    0.002       0.0627      0.00977       0.0725        0.161        0.229          4.7       0.0759\n",
            "! Validation         65  113.554    0.002       0.0635      0.00684       0.0703        0.159         0.23         4.01       0.0627\n",
            "Wall time: 113.55491477100009\n",
            "! Best model       65    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10        0.202        0.135       0.0671        0.257        0.336         15.1        0.237\n",
            "     66    20       0.0291       0.0291     6.66e-06        0.109        0.156        0.149      0.00236\n",
            "     66    30        0.133        0.115       0.0173        0.247         0.31         7.69         0.12\n",
            "     66    40        0.102       0.0914       0.0104        0.221        0.276         5.87       0.0931\n",
            "     66    50       0.0701        0.053       0.0171        0.165         0.21         7.53        0.119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2       0.0684       0.0634      0.00499        0.161         0.23          2.9       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  114.892    0.002       0.0647       0.0113        0.076        0.166        0.233         4.96       0.0793\n",
            "! Validation         66  114.892    0.002       0.0641      0.00676       0.0708         0.16        0.231         3.99       0.0625\n",
            "Wall time: 114.8931862190002\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10       0.0902       0.0777       0.0125          0.2        0.255         6.43        0.102\n",
            "     67    20       0.0649       0.0398       0.0251        0.141        0.182         9.27        0.145\n",
            "     67    30       0.0186      0.00985      0.00875       0.0524       0.0907         5.47       0.0855\n",
            "     67    40       0.0479       0.0449      0.00307        0.112        0.194         3.24       0.0507\n",
            "     67    50       0.0301       0.0298     0.000364        0.115        0.158          1.1       0.0174\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2       0.0683       0.0634       0.0049        0.161         0.23         2.77       0.0434\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  116.229    0.002       0.0611      0.00938       0.0705        0.161        0.226         4.24       0.0708\n",
            "! Validation         67  116.229    0.002        0.064      0.00667       0.0707         0.16        0.231         3.91       0.0611\n",
            "Wall time: 116.23016239200001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10       0.0568       0.0417       0.0151        0.108        0.187         7.17        0.112\n",
            "     68    20        0.152        0.117       0.0351        0.255        0.312           11        0.171\n",
            "     68    30        0.096       0.0878      0.00821        0.217        0.271         5.22       0.0828\n",
            "     68    40       0.0321       0.0321     1.09e-05        0.134        0.164         0.19      0.00302\n",
            "     68    50        0.134        0.132      0.00237        0.259        0.332         2.85       0.0445\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2       0.0681       0.0632       0.0049         0.16         0.23         2.76       0.0432\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  117.552    0.002       0.0628        0.008       0.0708        0.161        0.229         3.95       0.0651\n",
            "! Validation         68  117.552    0.002       0.0637      0.00671       0.0704        0.159        0.231         3.92       0.0613\n",
            "Wall time: 117.55306987500012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10       0.0325       0.0306      0.00184         0.12         0.16         2.47       0.0392\n",
            "     69    20       0.0567       0.0425       0.0142         0.15        0.188         6.96        0.109\n",
            "     69    30        0.179        0.142       0.0373        0.275        0.344         11.3        0.176\n",
            "     69    40       0.0297       0.0225      0.00717        0.106        0.137         4.87       0.0774\n",
            "     69    50         0.03       0.0258      0.00423        0.109        0.147         3.74       0.0594\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2       0.0674       0.0626      0.00482         0.16        0.229         2.75       0.0432\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  118.924    0.002       0.0603       0.0146        0.075         0.16        0.225         5.83       0.0949\n",
            "! Validation         69  118.924    0.002       0.0632      0.00663       0.0699        0.159         0.23         3.92       0.0613\n",
            "Wall time: 118.92567081400011\n",
            "! Best model       69    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10        0.038       0.0243       0.0137       0.0822        0.142         6.86        0.107\n",
            "     70    20       0.0999       0.0866       0.0133        0.211        0.269         6.65        0.106\n",
            "     70    30       0.0403       0.0381       0.0022        0.139        0.178         2.74       0.0429\n",
            "     70    40        0.162         0.14       0.0224        0.279        0.341         8.75        0.137\n",
            "     70    50       0.0557       0.0422       0.0135         0.15        0.188          6.8        0.106\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2       0.0673       0.0626       0.0047         0.16        0.229         2.73       0.0428\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  120.728    0.002       0.0598       0.0101       0.0699        0.159        0.224         4.72       0.0774\n",
            "! Validation         70  120.728    0.002       0.0632      0.00648       0.0697        0.158         0.23         3.89       0.0608\n",
            "Wall time: 120.7300484660002\n",
            "! Best model       70    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10       0.0546       0.0518      0.00276        0.165        0.208         3.02        0.048\n",
            "     71    20       0.0627       0.0184       0.0444       0.0475        0.124         2.31        0.192\n",
            "     71    30       0.0298       0.0289     0.000911        0.109        0.155         1.74       0.0276\n",
            "     71    40        0.142        0.127       0.0155        0.254        0.325         7.29        0.114\n",
            "     71    50       0.0758       0.0711      0.00472        0.196        0.244         3.96       0.0628\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2       0.0673       0.0626      0.00472         0.16        0.229         2.76       0.0433\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  122.706    0.002       0.0599      0.00967       0.0696        0.159        0.224         4.35       0.0724\n",
            "! Validation         71  122.706    0.002       0.0631      0.00648       0.0696        0.158        0.229          3.9       0.0609\n",
            "Wall time: 122.70826236000016\n",
            "! Best model       71    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10        0.038       0.0372     0.000816        0.128        0.176         1.64       0.0261\n",
            "     72    20       0.0165       0.0148      0.00168       0.0898        0.111          2.4       0.0375\n",
            "     72    30       0.0612       0.0593       0.0019        0.175        0.223         2.55       0.0398\n",
            "     72    40        0.106        0.102      0.00407        0.226        0.292         3.67       0.0583\n",
            "     72    50       0.0916       0.0757       0.0158          0.2        0.251         7.25        0.115\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2       0.0668       0.0619       0.0049        0.159        0.227         2.85       0.0447\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  124.695    0.002       0.0621      0.00817       0.0703        0.161        0.228         4.07       0.0664\n",
            "! Validation         72  124.695    0.002       0.0626      0.00668       0.0693        0.158        0.229         3.96       0.0619\n",
            "Wall time: 124.696597828\n",
            "! Best model       72    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10        0.103       0.0984      0.00434        0.165        0.287         3.85       0.0602\n",
            "     73    20        0.124        0.111       0.0136        0.252        0.304         6.82        0.107\n",
            "     73    30        0.134        0.109       0.0258        0.236        0.301         9.39        0.147\n",
            "     73    40        0.022       0.0115       0.0105       0.0775       0.0979            6       0.0937\n",
            "     73    50       0.0932       0.0891       0.0041         0.22        0.273         3.69       0.0585\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2       0.0669       0.0625      0.00447         0.16        0.228         2.62       0.0411\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  126.399    0.002       0.0626      0.00772       0.0704        0.163        0.229         4.08       0.0673\n",
            "! Validation         73  126.399    0.002       0.0629      0.00622       0.0691        0.158        0.229         3.79       0.0593\n",
            "Wall time: 126.39960853800017\n",
            "! Best model       73    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10        0.159        0.121       0.0381         0.26        0.318         11.4        0.178\n",
            "     74    20        0.102        0.102     0.000163        0.227        0.291        0.735       0.0117\n",
            "     74    30       0.0351       0.0277      0.00743        0.116        0.152         4.96       0.0787\n",
            "     74    40       0.0135        0.013     0.000529       0.0833        0.104         1.35        0.021\n",
            "     74    50       0.0915       0.0864      0.00511        0.212        0.269         4.18       0.0653\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2       0.0668       0.0623      0.00446        0.159        0.228         2.58       0.0404\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  127.750    0.002       0.0615       0.0157       0.0772        0.161        0.227         5.68       0.0932\n",
            "! Validation         74  127.750    0.002       0.0626      0.00626       0.0689        0.158        0.229         3.77        0.059\n",
            "Wall time: 127.75133718699999\n",
            "! Best model       74    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     0.000262     7.31e-11     0.000262     5.38e-06     7.81e-06        0.946       0.0148\n",
            "     75    20       0.0641       0.0534       0.0107        0.166        0.211         5.95       0.0944\n",
            "     75    30        0.109        0.107      0.00207        0.236        0.298         2.66       0.0416\n",
            "     75    40        0.115       0.0886       0.0262        0.215        0.272         9.31        0.148\n",
            "     75    50       0.0278       0.0275     0.000255        0.114        0.152        0.919       0.0146\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2        0.067       0.0625      0.00452         0.16        0.228         2.65       0.0416\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  129.051    0.002       0.0628       0.0112       0.0739        0.162        0.229         4.95       0.0799\n",
            "! Validation         75  129.051    0.002       0.0628      0.00631       0.0691        0.158        0.229         3.81       0.0596\n",
            "Wall time: 129.05153958200003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10       0.0351       0.0328      0.00234         0.12        0.165         2.79       0.0442\n",
            "     76    20       0.0617       0.0476       0.0141        0.115        0.199         6.95        0.109\n",
            "     76    30        0.119        0.109         0.01         0.24        0.302         5.86       0.0915\n",
            "     76    40        0.113        0.102       0.0111        0.241        0.292         6.16       0.0963\n",
            "     76    50       0.0738       0.0732     0.000631        0.195        0.247         1.47        0.023\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2       0.0661       0.0612      0.00486        0.158        0.226         2.89       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  130.382    0.002       0.0608      0.00787       0.0686        0.159        0.225         3.87       0.0617\n",
            "! Validation         76  130.382    0.002       0.0623       0.0066       0.0689        0.157        0.228         3.98       0.0622\n",
            "Wall time: 130.38238039700013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10       0.0266       0.0221       0.0045       0.0511        0.136        0.735       0.0613\n",
            "     77    20        0.182        0.126       0.0552        0.249        0.325         13.7        0.215\n",
            "     77    30       0.0798        0.079     0.000853          0.2        0.257         1.68       0.0267\n",
            "     77    40       0.0952       0.0952      5.1e-05        0.224        0.282        0.418      0.00652\n",
            "     77    50       0.0751       0.0751     8.46e-07        0.202         0.25       0.0529      0.00084\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2       0.0658       0.0606      0.00516        0.157        0.225         3.03       0.0475\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  131.690    0.002       0.0588      0.00914        0.068        0.158        0.222         4.29       0.0691\n",
            "! Validation         77  131.690    0.002       0.0621      0.00689        0.069        0.157        0.228         4.06       0.0636\n",
            "Wall time: 131.69131595600015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10       0.0826       0.0821     0.000475        0.211        0.262         1.25       0.0199\n",
            "     78    20        0.179        0.151       0.0279        0.271        0.355         9.77        0.153\n",
            "     78    30       0.0302       0.0275      0.00276        0.113        0.151         3.02        0.048\n",
            "     78    40        0.149        0.129       0.0199        0.256        0.329         8.25        0.129\n",
            "     78    50       0.0717         0.02       0.0517        0.047        0.129         2.49        0.208\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2       0.0658       0.0608      0.00496        0.157        0.225         2.91       0.0457\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  133.006    0.002       0.0597      0.00927        0.069        0.159        0.224         4.25       0.0713\n",
            "! Validation         78  133.006    0.002       0.0621      0.00668       0.0688        0.157        0.228         3.98       0.0622\n",
            "Wall time: 133.006712849\n",
            "! Best model       78    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10       0.0195       0.0073       0.0122       0.0451       0.0781         6.46        0.101\n",
            "     79    20       0.0289       0.0288     0.000114        0.117        0.155        0.615      0.00976\n",
            "     79    30       0.0811       0.0773      0.00378        0.205        0.254         3.54       0.0562\n",
            "     79    40       0.0186       0.0126        0.006       0.0814        0.103         4.53       0.0708\n",
            "     79    50       0.0228       0.0226     0.000199        0.105        0.137        0.811       0.0129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2       0.0657       0.0609      0.00481        0.157        0.226         2.83       0.0444\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  134.356    0.002       0.0607       0.0102       0.0709         0.16        0.226         4.57       0.0755\n",
            "! Validation         79  134.356    0.002       0.0619      0.00655       0.0684        0.157        0.227         3.91       0.0612\n",
            "Wall time: 134.3573408750001\n",
            "! Best model       79    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10       0.0514       0.0151       0.0363       0.0887        0.112         11.1        0.174\n",
            "     80    20        0.017       0.0143      0.00274       0.0862        0.109         3.06       0.0479\n",
            "     80    30      0.00142     8.47e-11      0.00142     5.24e-06     8.41e-06          2.2       0.0344\n",
            "     80    40       0.0307       0.0137        0.017       0.0844        0.107         7.63        0.119\n",
            "     80    50       0.0372       0.0291       0.0081        0.118        0.156         5.18       0.0822\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2       0.0653       0.0603      0.00508        0.156        0.224            3        0.047\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  135.716    0.002       0.0603       0.0107        0.071         0.16        0.225         4.66       0.0772\n",
            "! Validation         80  135.716    0.002       0.0615      0.00681       0.0683        0.156        0.227         4.03       0.0631\n",
            "Wall time: 135.7170751770002\n",
            "! Best model       80    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10        0.135        0.123       0.0126        0.259         0.32         6.58        0.103\n",
            "     81    20       0.0923       0.0887       0.0036         0.22        0.272         3.46       0.0549\n",
            "     81    30        0.115        0.113      0.00172        0.239        0.307         2.39       0.0379\n",
            "     81    40        0.102        0.102     0.000147        0.228        0.291        0.698       0.0111\n",
            "     81    50        0.136        0.101       0.0348         0.23        0.291         10.9         0.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2       0.0654       0.0606      0.00484        0.157        0.225         2.87       0.0451\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  137.494    0.002       0.0611      0.00928       0.0704         0.16        0.226         4.48       0.0727\n",
            "! Validation         81  137.494    0.002       0.0615      0.00658        0.068        0.156        0.227         3.94       0.0617\n",
            "Wall time: 137.495926005\n",
            "! Best model       81    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10        0.132        0.117       0.0142        0.253        0.313         6.97        0.109\n",
            "     82    20       0.0902       0.0899     0.000266        0.214        0.274        0.939       0.0149\n",
            "     82    30       0.0272       0.0266     0.000679        0.109        0.149          1.5       0.0238\n",
            "     82    40       0.0975       0.0974     0.000122        0.224        0.285        0.635       0.0101\n",
            "     82    50       0.0292       0.0271      0.00213       0.0556         0.15        0.506       0.0422\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2       0.0656       0.0611      0.00456        0.157        0.226         2.65       0.0415\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  139.395    0.002       0.0603      0.00598       0.0663         0.16        0.224         3.31       0.0539\n",
            "! Validation         82  139.395    0.002       0.0619      0.00629       0.0682        0.156        0.227         3.78       0.0592\n",
            "Wall time: 139.39706425500003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10        0.143        0.094        0.049        0.222         0.28         12.9        0.202\n",
            "     83    20       0.0275       0.0129       0.0146       0.0825        0.104         7.08        0.111\n",
            "     83    30       0.0707       0.0663      0.00431        0.187        0.235         3.78         0.06\n",
            "     83    40        0.147        0.123       0.0235        0.262        0.321         8.97         0.14\n",
            "     83    50        0.123        0.108        0.015        0.238        0.301         7.17        0.112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2       0.0651       0.0604      0.00468        0.157        0.225          2.8       0.0439\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  141.348    0.002       0.0598       0.0131        0.073        0.158        0.224         5.14       0.0849\n",
            "! Validation         83  141.348    0.002       0.0612       0.0064       0.0676        0.156        0.226         3.88       0.0608\n",
            "Wall time: 141.34985558300014\n",
            "! Best model       83    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10        0.052       0.0479      0.00413        0.163          0.2         3.76       0.0587\n",
            "     84    20       0.0353       0.0258      0.00951       0.0531        0.147         1.07       0.0891\n",
            "     84    30        0.143       0.0974       0.0454        0.231        0.285         12.5        0.195\n",
            "     84    40       0.0936       0.0922      0.00147        0.215        0.277         2.21       0.0351\n",
            "     84    50       0.0569       0.0568     0.000118        0.171        0.218        0.635      0.00992\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2       0.0651       0.0605      0.00461        0.157        0.225         2.73       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  143.152    0.002       0.0599      0.00805        0.068        0.161        0.224         4.14       0.0677\n",
            "! Validation         84  143.152    0.002       0.0614      0.00633       0.0677        0.156        0.226         3.84         0.06\n",
            "Wall time: 143.15298648700013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10       0.0906       0.0905       0.0001        0.218        0.275        0.576      0.00915\n",
            "     85    20        0.129       0.0946       0.0347        0.155        0.281         6.13         0.17\n",
            "     85    30          0.1        0.099      0.00115        0.234        0.288         1.98       0.0309\n",
            "     85    40         0.16          0.1       0.0592        0.232        0.289         14.2        0.222\n",
            "     85    50       0.0237       0.0222      0.00141        0.104        0.136         2.16       0.0343\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2       0.0652       0.0601      0.00518        0.156        0.224         3.05       0.0479\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  144.480    0.002       0.0585       0.0112       0.0697        0.159        0.222         4.29       0.0715\n",
            "! Validation         85  144.480    0.002       0.0613      0.00691       0.0682        0.156        0.226         4.07       0.0636\n",
            "Wall time: 144.481072563\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10      0.00027     7.91e-11      0.00027     5.56e-06     8.13e-06        0.961        0.015\n",
            "     86    20        0.119        0.108       0.0106        0.241        0.301         6.03       0.0942\n",
            "     86    30       0.0317       0.0317     1.92e-05        0.121        0.163        0.252      0.00401\n",
            "     86    40        0.094        0.094     1.05e-05        0.223         0.28         0.19      0.00296\n",
            "     86    50        0.101       0.0996      0.00162        0.224        0.288         2.32       0.0368\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2       0.0651       0.0601      0.00495        0.156        0.224         2.93       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  145.817    0.002       0.0595      0.00806       0.0676        0.158        0.223         4.21       0.0686\n",
            "! Validation         86  145.817    0.002       0.0612      0.00669       0.0679        0.156        0.226         3.98       0.0623\n",
            "Wall time: 145.81810197000004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10       0.0578       0.0577      0.00012        0.173        0.219        0.641         0.01\n",
            "     87    20       0.0429       0.0259        0.017       0.0523        0.147         1.43        0.119\n",
            "     87    30        0.124        0.117      0.00712        0.246        0.313         4.86       0.0771\n",
            "     87    40        0.132        0.132     8.49e-05        0.268        0.332        0.539      0.00842\n",
            "     87    50        0.106       0.0769       0.0288        0.204        0.253         9.77        0.155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2       0.0651       0.0604      0.00468        0.157        0.225         2.81       0.0441\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  147.122    0.002       0.0667        0.011       0.0777        0.166        0.236         4.99        0.082\n",
            "! Validation         87  147.122    0.002       0.0612      0.00641       0.0676        0.156        0.226         3.89       0.0609\n",
            "Wall time: 147.12288241700003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10        0.119        0.103       0.0161        0.237        0.294         7.42        0.116\n",
            "     88    20      0.00218     7.35e-11      0.00218     5.14e-06     7.83e-06         2.73       0.0427\n",
            "     88    30       0.0911       0.0911     5.73e-08        0.226        0.276       0.0138     0.000218\n",
            "     88    40       0.0289       0.0285     0.000439        0.116        0.154         1.21       0.0192\n",
            "     88    50       0.0782       0.0767      0.00151        0.197        0.253         2.23       0.0355\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2       0.0649       0.0604      0.00457        0.156        0.225         2.64       0.0413\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  148.419    0.002       0.0615      0.00877       0.0703        0.161        0.227         4.14        0.068\n",
            "! Validation         88  148.419    0.002       0.0612      0.00633       0.0676        0.155        0.226         3.79       0.0593\n",
            "Wall time: 148.41997922300015\n",
            "! Best model       88    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10       0.0823       0.0822     0.000116        0.151        0.262        0.631      0.00985\n",
            "     89    20       0.0499       0.0366       0.0134        0.134        0.175         6.66        0.106\n",
            "     89    30        0.152         0.12       0.0324         0.25        0.316         10.4        0.165\n",
            "     89    40       0.0923       0.0911      0.00119         0.22        0.276         1.99       0.0315\n",
            "     89    50       0.0268       0.0264      0.00036       0.0544        0.149        0.208       0.0173\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2       0.0645       0.0597      0.00479        0.155        0.223         2.86       0.0449\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  149.712    0.002       0.0612      0.00959       0.0708         0.16        0.226         4.24       0.0686\n",
            "! Validation         89  149.712    0.002       0.0609      0.00648       0.0674        0.155        0.226         3.93       0.0614\n",
            "Wall time: 149.7135981580002\n",
            "! Best model       89    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10       0.0558       0.0497      0.00604        0.118        0.204         4.55        0.071\n",
            "     90    20       0.0231       0.0048       0.0183       0.0366       0.0633         7.92        0.124\n",
            "     90    30       0.0809       0.0788       0.0021        0.201        0.256         2.64       0.0419\n",
            "     90    40        0.112        0.102      0.00999         0.16        0.292         3.29       0.0913\n",
            "     90    50       0.0348       0.0333      0.00153         0.12        0.167         2.25       0.0357\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2       0.0648       0.0597      0.00503        0.156        0.223         2.96       0.0465\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  151.036    0.002       0.0608       0.0111       0.0719        0.161        0.226         4.86       0.0793\n",
            "! Validation         90  151.036    0.002       0.0611      0.00672       0.0678        0.155        0.226         3.99       0.0624\n",
            "Wall time: 151.037316337\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10       0.0325       0.0165        0.016       0.0921        0.118          7.4        0.116\n",
            "     91    20        0.156        0.128       0.0278        0.256        0.327         9.75        0.152\n",
            "     91    30        0.114        0.105      0.00935        0.243        0.296         5.66       0.0884\n",
            "     91    40         0.12        0.103       0.0176        0.234        0.293         7.77        0.121\n",
            "     91    50       0.0758       0.0743      0.00141        0.196        0.249          2.2       0.0344\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2       0.0646       0.0597      0.00487        0.155        0.223          2.9       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  152.336    0.002       0.0606      0.00785       0.0685         0.16        0.225         3.98       0.0659\n",
            "! Validation         91  152.336    0.002       0.0611      0.00654       0.0676        0.155        0.226         3.93       0.0615\n",
            "Wall time: 152.3370314880001\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10       0.0914       0.0913     0.000171        0.217        0.276        0.754        0.012\n",
            "     92    20       0.0951       0.0912      0.00387        0.216        0.276         3.58       0.0568\n",
            "     92    30       0.0251       0.0232      0.00192        0.107        0.139         2.53       0.0401\n",
            "     92    40       0.0152        0.014      0.00117       0.0868        0.108            2       0.0312\n",
            "     92    50        0.093       0.0916       0.0014        0.221        0.277         2.19       0.0342\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2       0.0647       0.0601      0.00457        0.156        0.224         2.74       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  153.962    0.002        0.059      0.00934       0.0684        0.158        0.222         4.28       0.0682\n",
            "! Validation         92  153.962    0.002       0.0613      0.00617       0.0675        0.156        0.226          3.8       0.0594\n",
            "Wall time: 153.9634865720002\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10        0.109        0.102      0.00681        0.227        0.292         4.75       0.0754\n",
            "     93    20       0.0807       0.0792      0.00147        0.206        0.257         2.24       0.0351\n",
            "     93    30        0.138        0.123       0.0147        0.185        0.321         7.09        0.111\n",
            "     93    40       0.0112     8.15e-11       0.0112     5.23e-06     8.25e-06         6.19       0.0967\n",
            "     93    50        0.119        0.101       0.0181        0.239        0.291         7.86        0.123\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2       0.0644       0.0589      0.00547        0.154        0.222         3.22       0.0506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  155.789    0.002       0.0607      0.00919       0.0699        0.159        0.225         4.32       0.0717\n",
            "! Validation         93  155.789    0.002       0.0606      0.00711       0.0677        0.155        0.225         4.16       0.0651\n",
            "Wall time: 155.79113780700004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10        0.137        0.104       0.0323        0.242        0.295         10.5        0.164\n",
            "     94    20          0.1       0.0971      0.00292        0.226        0.285         3.11       0.0494\n",
            "     94    30       0.0268       0.0268     4.22e-05        0.114        0.149        0.374      0.00593\n",
            "     94    40       0.0123       0.0046      0.00769       0.0358        0.062         5.13       0.0801\n",
            "     94    50       0.0289       0.0288     8.11e-05        0.117        0.155        0.519      0.00823\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2       0.0643       0.0594      0.00492        0.155        0.223         2.95       0.0463\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  157.793    0.002        0.059      0.00879       0.0677        0.158        0.223         4.22       0.0702\n",
            "! Validation         94  157.793    0.002       0.0608      0.00655       0.0673        0.155        0.225         3.96       0.0619\n",
            "Wall time: 157.7935859470001\n",
            "! Best model       94    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10       0.0271       0.0269     0.000211        0.109         0.15        0.837       0.0133\n",
            "     95    20       0.0394       0.0327      0.00665         0.13        0.165         4.77       0.0745\n",
            "     95    30       0.0285       0.0281     0.000404        0.113        0.153         1.16       0.0184\n",
            "     95    40        0.119       0.0921        0.027         0.22        0.277         9.46         0.15\n",
            "     95    50       0.0863       0.0849      0.00135        0.209        0.266         2.15       0.0336\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2       0.0641       0.0593      0.00487        0.155        0.222         2.92       0.0458\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  159.741    0.002       0.0593      0.00989       0.0692         0.16        0.223         4.64       0.0758\n",
            "! Validation         95  159.741    0.002       0.0607      0.00653       0.0672        0.155        0.225         3.94       0.0617\n",
            "Wall time: 159.74316792500008\n",
            "! Best model       95    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10        0.164        0.155      0.00863        0.276         0.36         5.43       0.0849\n",
            "     96    20       0.0587       0.0583     0.000361        0.179        0.221         1.11       0.0174\n",
            "     96    30        0.132        0.127      0.00553        0.178        0.325         2.45        0.068\n",
            "     96    40       0.0423       0.0344      0.00786        0.133         0.17         5.18        0.081\n",
            "     96    50       0.0263       0.0263     2.98e-05        0.107        0.148        0.315      0.00499\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2       0.0643       0.0592      0.00512        0.155        0.222         2.94       0.0462\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  161.126    0.002       0.0632       0.0104       0.0736        0.162         0.23         4.24       0.0686\n",
            "! Validation         96  161.126    0.002       0.0602      0.00689       0.0671        0.154        0.224         3.98       0.0624\n",
            "Wall time: 161.12722070500013\n",
            "! Best model       96    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10      0.00231     8.23e-11      0.00231     5.26e-06     8.29e-06         2.81        0.044\n",
            "     97    20        0.174         0.15       0.0237        0.273        0.354         9.01        0.141\n",
            "     97    30       0.0426        0.016       0.0266       0.0925        0.116         9.53        0.149\n",
            "     97    40       0.0441       0.0439     0.000179        0.157        0.192        0.783       0.0122\n",
            "     97    50       0.0388       0.0388     2.12e-06         0.13         0.18       0.0837      0.00133\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2       0.0637       0.0589      0.00484        0.154        0.222         2.88       0.0452\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  162.430    0.002       0.0601      0.00862       0.0687        0.158        0.224         4.14       0.0654\n",
            "! Validation         97  162.430    0.002       0.0601      0.00652       0.0667        0.154        0.224         3.92       0.0613\n",
            "Wall time: 162.43092208899998\n",
            "! Best model       97    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10       0.0294        0.028       0.0014        0.113        0.153         2.15       0.0341\n",
            "     98    20        0.144        0.131       0.0125        0.261        0.331         6.54        0.102\n",
            "     98    30       0.0127       0.0117      0.00107       0.0783       0.0987         1.91       0.0299\n",
            "     98    40       0.0366        0.034      0.00263        0.122        0.169         2.95       0.0468\n",
            "     98    50       0.0425       0.0414      0.00108        0.148        0.186         1.92         0.03\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2       0.0643       0.0596      0.00472        0.155        0.223         2.82       0.0441\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  163.711    0.002       0.0595      0.00683       0.0664        0.159        0.224         3.62       0.0602\n",
            "! Validation         98  163.711    0.002       0.0606      0.00639        0.067        0.155        0.225         3.86       0.0604\n",
            "Wall time: 163.71217972099998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10       0.0322       0.0309      0.00125        0.127        0.161         2.03       0.0323\n",
            "     99    20     0.000172     7.42e-11     0.000172     5.44e-06     7.87e-06        0.767        0.012\n",
            "     99    30       0.0779       0.0578       0.0202        0.173         0.22         8.31         0.13\n",
            "     99    40       0.0695       0.0694     7.68e-05        0.189        0.241        0.512      0.00801\n",
            "     99    50       0.0833       0.0796       0.0037        0.203        0.258          3.5       0.0556\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2       0.0638       0.0593      0.00455        0.155        0.222         2.64       0.0413\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  165.038    0.002       0.0608      0.00859       0.0694         0.16        0.226         4.27       0.0703\n",
            "! Validation         99  165.038    0.002       0.0605       0.0062       0.0667        0.154        0.225         3.74       0.0585\n",
            "Wall time: 165.039170432\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10       0.0311       0.0228      0.00829       0.0796        0.138         5.32       0.0832\n",
            "    100    20       0.0501       0.0487      0.00141        0.161        0.202          2.2       0.0343\n",
            "    100    30       0.0893       0.0846      0.00474        0.213        0.266         3.97       0.0629\n",
            "    100    40        0.104       0.0825       0.0213        0.206        0.263         8.53        0.133\n",
            "    100    50       0.0124       0.0123     8.89e-05       0.0798        0.101        0.551      0.00861\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2       0.0635       0.0591      0.00441        0.154        0.222         2.59       0.0406\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  166.365    0.002       0.0619       0.0124       0.0743        0.162        0.228         4.89       0.0806\n",
            "! Validation        100  166.365    0.002       0.0606      0.00597       0.0665        0.154        0.225         3.69       0.0578\n",
            "Wall time: 166.36592601100006\n",
            "! Best model      100    0.067\n",
            "! Stop training: max epochs\n",
            "Wall time: 166.3796381510001\n",
            "Cumulative wall time: 166.3796381510001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae █▆▃▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae █▆▄▃▃▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae █▄▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse █▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss █▄▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e █▅▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae █▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae █▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR 0.002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall 166.36504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae 0.08057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae 4.89086\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae 0.16246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse 0.22751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss 0.07428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e 0.01237\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f 0.0619\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae 0.05778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae 3.6947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae 0.15438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse 0.22491\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss 0.06654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e 0.00597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f 0.06057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall 166.36504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msi\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-121328701729598718/allegro-tutorial/runs/8gf60lsk?apiKey=a3119b2aa4383bcda016425f70e052266828d485\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230722_155605-8gf60lsk/logs\u001b[0m\n",
            "CPU times: user 8.19 s, sys: 1 s, total: 9.19 s\n",
            "Wall time: 22min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls results/silicon-tutorial/si #/best_model.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KRsXlG65jGH",
        "outputId": "759186b5-9590-451a-8965-2e35f424fc49"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model.pth\tlog\t\t\t metrics_epoch.csv\n",
            "config.yaml\tmetrics_batch_train.csv  metrics_initialization.csv\n",
            "last_model.pth\tmetrics_batch_val.csv\t trainer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nequip.utils import Config\n",
        "from nequip.model import model_from_config\n",
        "from nequip.data import AtomicData, ASEDataset\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "config = Config.from_file(\"results/silicon-tutorial/si/config.yaml\")\n",
        "\n",
        "#config[\"train_on_keys\"]=[\"forces\", \"total_energy\"]\n",
        "#config[\"model_builders\"] = [\"EnergyModel\", \"PerSpeciesRescale\", \"ForceOutput\", \"RescaleEnergyEtc\"]\n",
        "model = model_from_config(config, initialize=False)\n",
        "d = torch.load('results/silicon-tutorial/si/best_model.pth',map_location=device)\n",
        "model.load_state_dict(d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg9svjLM5jI2",
        "outputId": "a8555e69-1583-4c89-81d9-0757966ff0da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0fb_mry5jLl",
        "outputId": "22794f24-de5d-417a-e933-a1d52a1dc7eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RescaleOutput(\n",
              "  (model): GradientOutput(\n",
              "    (func): SequentialGraphNetwork(\n",
              "      (one_hot): OneHotAtomEncoding()\n",
              "      (radial_basis): RadialBasisEdgeEncoding(\n",
              "        (basis): NormalizedBasis(\n",
              "          (basis): BesselBasis()\n",
              "        )\n",
              "        (cutoff): PolynomialCutoff()\n",
              "      )\n",
              "      (spharm): SphericalHarmonicEdgeAttrs(\n",
              "        (sh): SphericalHarmonics()\n",
              "      )\n",
              "      (allegro): Allegro_Module(\n",
              "        (latents): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (env_embed_mlps): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (tps): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (linears): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (env_linears): ModuleList(\n",
              "          (0): Identity()\n",
              "        )\n",
              "        (_env_weighter): MakeWeightedChannels()\n",
              "        (final_latent): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng): ScalarMLP(\n",
              "        (_module): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng_sum): EdgewiseEnergySum()\n",
              "      (per_species_rescale): PerSpeciesScaleShift()\n",
              "      (total_energy_sum): AtomwiseReduce()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jarvis.core.atoms import Atoms\n",
        "from jarvis.db.figshare import get_jid_data\n",
        "atoms = Atoms.from_dict(get_jid_data(jid='JVASP-1002',dataset='dft_3d')['atoms'])\n",
        "ase_atoms = atoms.ase_converter()\n",
        "a = ASEDataset.from_atoms_list([ase_atoms,ase_atoms],extra_fixed_fields={\"r_max\": 5.0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxCCG4xx5jOK",
        "outputId": "ad42948d-e0aa-4854-833d-72edf6726304"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 76k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40.8M/40.8M [00:02<00:00, 17.0MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the zipfile...\n",
            "Loading completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nequip.data import AtomicData, Collater, dataset_from_config, register_fields, AtomicDataDict\n",
        "from nequip.data.transforms import TypeMapper\n",
        "# c = Collater.for_dataset(a, exclude_keys=[])\n",
        "a = AtomicData.from_ase(ase_atoms,5)\n",
        "data = AtomicData.to_AtomicDataDict(a)\n",
        "# tm = TypeMapper(chemical_symbol_to_type={\"Si\": 0})\n",
        "tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "data = tm(data)\n",
        "out = model(data)\n"
      ],
      "metadata": {
        "id": "0B5JNf8r7Ijz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "if not os.path.exists('jarvis_leaderboard'):\n",
        "  !git clone https://github.com/usnistgov/jarvis_leaderboard.git\n",
        "# os.chdir('jarvis_leaderboard')\n",
        "# !pip install -e .\n",
        "os.chdir('/content/jarvis_leaderboard/jarvis_leaderboard/contributions/')\n",
        "os.makedirs('allegro_si')\n",
        "os.chdir('allegro_si')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQaB-lCuPvwg",
        "outputId": "7379d9ad-d3a3-4f13-b618-a15ec57071c5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jarvis_leaderboard'...\n",
            "remote: Enumerating objects: 44059, done.\u001b[K\n",
            "remote: Counting objects: 100% (12867/12867), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1354/1354), done.\u001b[K\n",
            "remote: Total 44059 (delta 6886), reused 12655 (delta 6839), pack-reused 31192\u001b[K\n",
            "Receiving objects: 100% (44059/44059), 314.17 MiB | 39.62 MiB/s, done.\n",
            "Resolving deltas: 100% (23055/23055), done.\n",
            "Updating files: 100% (2568/2568), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://figshare.com/ndownloader/files/40357663 -O mlearn.json.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXXN8_JlPbdr",
        "outputId": "a773a09e-3f80-4b91-bfe6-20d80f3dc866"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 16:19:04--  https://figshare.com/ndownloader/files/40357663\n",
            "Resolving figshare.com (figshare.com)... 63.34.138.122, 18.203.227.49, 2a05:d018:1f4:d000:8ff3:f995:dc4f:1c8d, ...\n",
            "Connecting to figshare.com (figshare.com)|63.34.138.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230722/eu-west-1/s3/aws4_request&X-Amz-Date=20230722T161904Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=33b3d4b3862af36a869779fc3942dbc01820e0a545a31326bcf4bba490d3830d [following]\n",
            "--2023-07-22 16:19:04--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230722/eu-west-1/s3/aws4_request&X-Amz-Date=20230722T161904Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=33b3d4b3862af36a869779fc3942dbc01820e0a545a31326bcf4bba490d3830d\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.93.163, 52.218.110.67, 52.92.32.184, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.93.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542319 (2.4M) [application/zip]\n",
            "Saving to: ‘mlearn.json.zip’\n",
            "\n",
            "mlearn.json.zip     100%[===================>]   2.42M  3.32MB/s    in 0.7s    \n",
            "\n",
            "2023-07-22 16:19:06 (3.32 MB/s) - ‘mlearn.json.zip’ saved [2542319/2542319]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json,zipfile\n",
        "#This dataset should have same test split (with ids) as the mlearn dataset built above\n",
        "df = pd.DataFrame(\n",
        "    json.loads(\n",
        "        zipfile.ZipFile(\"mlearn.json.zip\").read(\n",
        "            \"mlearn.json\"\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Jw72urwrQG_4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJa4Uh62QfUC",
        "outputId": "ae2251d7-26f5-40ea-94dc-388a6216754f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jarvis_leaderboard/jarvis_leaderboard/contributions/allegro_si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_allegro_forces(model=[],atoms=[],cutoff=5):\n",
        "    ase_atoms = atoms.ase_converter()\n",
        "    a = AtomicData.from_ase(ase_atoms,cutoff)\n",
        "    data = AtomicData.to_AtomicDataDict(a)\n",
        "    tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "    data = tm(data)\n",
        "    out = model(data)\n",
        "    pen=out['total_energy'].squeeze().cpu().detach().numpy().tolist()\n",
        "    num_atoms=atoms.num_atoms\n",
        "    pf=out['forces'].squeeze().cpu().detach().numpy()\n",
        "    return pen,pf,_"
      ],
      "metadata": {
        "id": "5i1-CsYHQvGg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "for i in glob.glob(\"../../benchmarks/AI/MLFF/*energy*.zip\"):\n",
        "    if \"mlearn\" in i and \"Si\" in i:\n",
        "        fname_e = (\n",
        "            \"AI-MLFF-energy-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-mae.csv\"\n",
        "        )\n",
        "        fname_f = (\n",
        "            \"AI-MLFF-forces-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-multimae.csv\"\n",
        "        )\n",
        "        fname_s = (\n",
        "            \"AI-MLFF-stresses-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-multimae.csv\"\n",
        "        )\n",
        "        f_e = open(fname_e, \"w\")\n",
        "        f_f = open(fname_f, \"w\")\n",
        "        #f_s = open(fname_s, \"w\")\n",
        "\n",
        "        f_e.write(\"id,target,prediction\\n\")\n",
        "        f_f.write(\"id,prediction\\n\")\n",
        "        #f_s.write(\"id,prediction\\n\")\n",
        "\n",
        "        print(i)\n",
        "        dat = json.loads(\n",
        "            zipfile.ZipFile(i).read(i.split(\"/\")[-1].split(\".zip\")[0])\n",
        "        )\n",
        "        print(dat[\"test\"])\n",
        "        for key, val in dat[\"test\"].items():\n",
        "            entry = df[df[\"jid\"] == key]\n",
        "            atoms = Atoms.from_dict(entry.atoms.values[0])\n",
        "            # print(key,val,df[df['jid']==key],atoms)\n",
        "            # energy,forces=get_alignn_forces(atoms)\n",
        "            energy, forces, stress = get_allegro_forces(model=model,atoms=atoms)\n",
        "            print(key, val, energy, atoms.num_atoms)\n",
        "            line = key +\",\"+ str(entry.energy.values[0])+\",\" + str(energy) + \"\\n\"\n",
        "            f_e.write(line)\n",
        "            line = (\n",
        "                key\n",
        "                + \",\"\n",
        "                + str(\";\".join(map(str, np.array(forces).flatten())))\n",
        "                + \"\\n\"\n",
        "            )\n",
        "            f_f.write(line)\n",
        "            # line = (\n",
        "            #     key\n",
        "            #     + \",\"\n",
        "            #     + str(\";\".join(map(str, np.array(stress).flatten())))\n",
        "            #     + \"\\n\"\n",
        "            # )\n",
        "            # f_s.write(line)\n",
        "        f_e.close()\n",
        "        f_f.close()\n",
        "        # f_s.close()\n",
        "        zname = fname_e + \".zip\"\n",
        "        with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            myzip.write(fname_e)\n",
        "\n",
        "        zname = fname_f + \".zip\"\n",
        "        with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            myzip.write(fname_f)\n",
        "\n",
        "        # zname = fname_s + \".zip\"\n",
        "        # with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "        #     myzip.write(fname_s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZJz8BOPGYB",
        "outputId": "f9f0b7cd-5580-4212-afc2-764d2d082fef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../../benchmarks/AI/MLFF/mlearn_Si_energy.json.zip\n",
            "{'Si-215': -297.62773938, 'Si-216': -295.77170067, 'Si-217': -291.28958206, 'Si-218': -296.24088456, 'Si-219': -294.41361742, 'Si-220': -334.75283939, 'Si-221': -334.69215136, 'Si-222': -184.71808052, 'Si-223': -121.41180043, 'Si-224': -338.93899696, 'Si-225': -338.83557056, 'Si-226': -335.68901422, 'Si-227': -333.7064957, 'Si-228': -344.85564046, 'Si-229': -344.81108268, 'Si-230': -298.83222646, 'Si-231': -298.96501782, 'Si-232': -295.20943762, 'Si-233': -291.86293882, 'Si-234': -344.74080048, 'Si-235': -344.74080047, 'Si-236': -344.74080046, 'Si-237': -341.22165747, 'Si-238': -341.22165734, 'Si-239': -341.22165747}\n",
            "Si-215 -297.62773938 -328.6075439453125 63\n",
            "Si-216 -295.77170067 -324.45965576171875 63\n",
            "Si-217 -291.28958206 -327.27740478515625 63\n",
            "Si-218 -296.24088456 -322.92529296875 63\n",
            "Si-219 -294.41361742 -324.0484313964844 63\n",
            "Si-220 -334.75283939 -366.2948913574219 63\n",
            "Si-221 -334.69215136 -366.56170654296875 63\n",
            "Si-222 -184.71808052 -208.6649169921875 36\n",
            "Si-223 -121.41180043 -138.18006896972656 24\n",
            "Si-224 -338.93899696 -366.9707336425781 64\n",
            "Si-225 -338.83557056 -366.8772888183594 64\n",
            "Si-226 -335.68901422 -364.18890380859375 64\n",
            "Si-227 -333.7064957 -362.1649475097656 64\n",
            "Si-228 -344.85564046 -372.6488342285156 64\n",
            "Si-229 -344.81108268 -372.5396423339844 64\n",
            "Si-230 -298.83222646 -334.6480712890625 64\n",
            "Si-231 -298.96501782 -335.65496826171875 64\n",
            "Si-232 -295.20943762 -331.1341247558594 64\n",
            "Si-233 -291.86293882 -328.0296325683594 64\n",
            "Si-234 -344.74080048 -366.642578125 64\n",
            "Si-235 -344.74080047 -366.642578125 64\n",
            "Si-236 -344.74080046 -366.642578125 64\n",
            "Si-237 -341.22165747 -367.0040588378906 64\n",
            "Si-238 -341.22165734 -367.00408935546875 64\n",
            "Si-239 -341.22165747 -367.0040588378906 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entry.energy.values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya4Pak_cSBHU",
        "outputId": "95f23eb3-7834-427b-c39b-f547f6f41ad2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-341.22165747"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -altr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK1rdjyGRqGD",
        "outputId": "1572d077-23a0-4d62-bbbf-606baa43b0f9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2620\n",
            "-rw-r--r--   1 root root 2542319 Apr 29 01:14 mlearn.json.zip\n",
            "drwxr-xr-x 158 root root   12288 Jul 22 16:19 ..\n",
            "-rw-r--r--   1 root root   53506 Jul 22 16:19 AI-MLFF-forces-mlearn_Si-test-multimae.csv\n",
            "-rw-r--r--   1 root root    1013 Jul 22 16:19 AI-MLFF-energy-mlearn_Si-test-mae.csv\n",
            "-rw-r--r--   1 root root    1185 Jul 22 16:19 AI-MLFF-energy-mlearn_Si-test-mae.csv.zip\n",
            "drwxr-xr-x   2 root root    4096 Jul 22 16:19 .\n",
            "-rw-r--r--   1 root root   53688 Jul 22 16:19 AI-MLFF-forces-mlearn_Si-test-multimae.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_df = pd.read_csv('AI-MLFF-energy-mlearn_Si-test-mae.csv.zip')"
      ],
      "metadata": {
        "id": "FU4jyfg-Rtjl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "rGyr7zZGRwKB",
        "outputId": "2d6c70d3-5240-43b3-f2cc-70c0a2e023ce"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id      target  prediction\n",
              "0   Si-215 -297.627739 -328.607544\n",
              "1   Si-216 -295.771701 -324.459656\n",
              "2   Si-217 -291.289582 -327.277405\n",
              "3   Si-218 -296.240885 -322.925293\n",
              "4   Si-219 -294.413617 -324.048431\n",
              "5   Si-220 -334.752839 -366.294891\n",
              "6   Si-221 -334.692151 -366.561707\n",
              "7   Si-222 -184.718081 -208.664917\n",
              "8   Si-223 -121.411800 -138.180069\n",
              "9   Si-224 -338.938997 -366.970734\n",
              "10  Si-225 -338.835571 -366.877289\n",
              "11  Si-226 -335.689014 -364.188904\n",
              "12  Si-227 -333.706496 -362.164948\n",
              "13  Si-228 -344.855640 -372.648834\n",
              "14  Si-229 -344.811083 -372.539642\n",
              "15  Si-230 -298.832226 -334.648071\n",
              "16  Si-231 -298.965018 -335.654968\n",
              "17  Si-232 -295.209438 -331.134125\n",
              "18  Si-233 -291.862939 -328.029633\n",
              "19  Si-234 -344.740800 -366.642578\n",
              "20  Si-235 -344.740800 -366.642578\n",
              "21  Si-236 -344.740800 -366.642578\n",
              "22  Si-237 -341.221657 -367.004059\n",
              "23  Si-238 -341.221657 -367.004089\n",
              "24  Si-239 -341.221657 -367.004059"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-04a153ad-c240-4e0f-8467-01a839900d5a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Si-215</td>\n",
              "      <td>-297.627739</td>\n",
              "      <td>-328.607544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Si-216</td>\n",
              "      <td>-295.771701</td>\n",
              "      <td>-324.459656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Si-217</td>\n",
              "      <td>-291.289582</td>\n",
              "      <td>-327.277405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Si-218</td>\n",
              "      <td>-296.240885</td>\n",
              "      <td>-322.925293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Si-219</td>\n",
              "      <td>-294.413617</td>\n",
              "      <td>-324.048431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Si-220</td>\n",
              "      <td>-334.752839</td>\n",
              "      <td>-366.294891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Si-221</td>\n",
              "      <td>-334.692151</td>\n",
              "      <td>-366.561707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Si-222</td>\n",
              "      <td>-184.718081</td>\n",
              "      <td>-208.664917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Si-223</td>\n",
              "      <td>-121.411800</td>\n",
              "      <td>-138.180069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Si-224</td>\n",
              "      <td>-338.938997</td>\n",
              "      <td>-366.970734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Si-225</td>\n",
              "      <td>-338.835571</td>\n",
              "      <td>-366.877289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Si-226</td>\n",
              "      <td>-335.689014</td>\n",
              "      <td>-364.188904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Si-227</td>\n",
              "      <td>-333.706496</td>\n",
              "      <td>-362.164948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Si-228</td>\n",
              "      <td>-344.855640</td>\n",
              "      <td>-372.648834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Si-229</td>\n",
              "      <td>-344.811083</td>\n",
              "      <td>-372.539642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Si-230</td>\n",
              "      <td>-298.832226</td>\n",
              "      <td>-334.648071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Si-231</td>\n",
              "      <td>-298.965018</td>\n",
              "      <td>-335.654968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Si-232</td>\n",
              "      <td>-295.209438</td>\n",
              "      <td>-331.134125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Si-233</td>\n",
              "      <td>-291.862939</td>\n",
              "      <td>-328.029633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Si-234</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-366.642578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Si-235</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-366.642578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Si-236</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-366.642578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Si-237</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-367.004059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Si-238</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-367.004089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Si-239</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-367.004059</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04a153ad-c240-4e0f-8467-01a839900d5a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-ef1726a5-940b-4763-9a52-7c31ea6bf3a3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef1726a5-940b-4763-9a52-7c31ea6bf3a3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-ef1726a5-940b-4763-9a52-7c31ea6bf3a3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04a153ad-c240-4e0f-8467-01a839900d5a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04a153ad-c240-4e0f-8467-01a839900d5a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(en_df['target'],en_df['prediction'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJDW6nbNSMyC",
        "outputId": "56fef825-beed-43b4-fdc5-53f3432d054a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.492192422510936"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_en=[]\n",
        "pred_en=[]\n",
        "actual_forces=[]\n",
        "pred_forces=[]\n",
        "\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    ase_atoms = atoms.ase_converter()\n",
        "    a = AtomicData.from_ase(ase_atoms,5)\n",
        "    data = AtomicData.to_AtomicDataDict(a)\n",
        "    tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "    data = tm(data)\n",
        "    out = model(data)\n",
        "    pen=out['total_energy'].squeeze().cpu().detach().numpy().tolist()\n",
        "    print(j,pen)\n",
        "    num_atoms=atoms.num_atoms\n",
        "    actual_en.append(j/num_atoms)\n",
        "    pred_en.append(pen/num_atoms)\n",
        "    actual_forces.append(np.array(k).flatten())\n",
        "    pf=out['forces'].squeeze().cpu().detach().numpy()\n",
        "    pred_forces.append(pf.flatten())\n",
        "    #break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyOyUFYQLfbr",
        "outputId": "ed1188e8-ab6b-4a28-971e-b412ad1eb26d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-297.62773938 -328.6075439453125\n",
            "-295.77170067 -324.459716796875\n",
            "-291.28958206 -327.2773742675781\n",
            "-296.24088456 -322.92529296875\n",
            "-294.41361742 -324.0484313964844\n",
            "-334.75283939 -366.2948913574219\n",
            "-334.69215136 -366.56170654296875\n",
            "-184.71808052 -208.6649169921875\n",
            "-121.41180043 -138.18006896972656\n",
            "-338.93899696 -366.9707336425781\n",
            "-338.83557056 -366.8772888183594\n",
            "-335.68901422 -364.18890380859375\n",
            "-333.7064957 -362.1649475097656\n",
            "-344.85564046 -372.6488342285156\n",
            "-344.81108268 -372.5396423339844\n",
            "-298.83222646 -334.6480712890625\n",
            "-298.96501782 -335.65496826171875\n",
            "-295.20943762 -331.1341247558594\n",
            "-291.86293882 -328.0296325683594\n",
            "-344.74080048 -366.642578125\n",
            "-344.74080047 -366.642578125\n",
            "-344.74080046 -366.642578125\n",
            "-341.22165747 -367.0040588378906\n",
            "-341.22165734 -367.00408935546875\n",
            "-341.22165747 -367.0040588378906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(actual_en,pred_en,'.')\n",
        "plt.xlabel('DFT energy(eV/atom)')\n",
        "plt.ylabel('FF energy(eV/atom)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "zedmdoAcNmHU",
        "outputId": "a45f325b-14f7-4eea-b29c-3a15feac3b99"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'FF energy(eV/atom)')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAGxCAYAAACOSdkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAd0lEQVR4nO3de1hU5f7//9eggKCAB0A0QUAKTKkkD7uDZlmhlrm3HfxmZKYR1S4LOwh7W3kowVS0+uzd0UOWfTDLcyfLdKfB1sqwUkQFUVMsTQMVGTms3x/+mI+E4gzOMAzzfFzXXJdzz73WvGdW7Hnte91r3SbDMAwBAAC4IQ9nFwAAAOAsBCEAAOC2CEIAAMBtEYQAAIDbIggBAAC3RRACAABuiyAEAADcFkEIAAC4rebOLqCxq6qq0oEDB+Tn5yeTyeTscgAAgBUMw9CxY8fUsWNHeXice9yHIHQeBw4cUGhoqLPLAAAA9bBv3z516tTpnK+7TBAKDw/Xnj17arSlpaUpJSXlnNu8+eabev/997V582YdO3ZMR48eVevWrW16Xz8/P0mnv0h/f3+b6wYAAA2vpKREoaGhlt/xc3GZICRJkydPVmJiouX5+T5caWmpBg4cqIEDByo1NbVe71l9Oszf358gBACAiznftBaXCkJ+fn4KCQmxuv8TTzwhSVq3bp1jCgIAAC7Npa4aS09PV7t27dSjRw9Nnz5dFRUVdn8Ps9mskpKSGg8AANA0ucyI0NixYxUXF6e2bdsqKytLqampKioqUkZGhl3fJy0tTZMmTbLrPgEAQONkMgzDcNabp6SkaNq0aXX2yc3NVUxMTK32uXPnKikpScePH5e3t3ed+1i3bp2uv/56qyZLm81mmc1my/PqyVbFxcXMEQIAwEWUlJQoICDgvL/fTh0RevLJJzVq1Kg6+0RGRp61vU+fPqqoqFBhYaGio6PtVpO3t/d5gxUAAGganBqEgoKCFBQUVK9tc3Jy5OHhoeDgYDtXBQAA3IVLzBHKzs7Wxo0bdf3118vPz0/Z2dlKTk5WQkKC2rRpI0nav3+/BgwYoAULFqh3796SpIMHD+rgwYPatWuXJOmnn36Sn5+fwsLC1LZtW6d9HgAA0Di4RBDy9vZWZmamJk6cKLPZrIiICCUnJ2vcuHGWPuXl5crLy1Npaaml7fXXX68x8blfv36SpHnz5p33lBwAAGj6nDpZ2hVYO9kKAAA0Htb+frvUfYQAAADsiSAEAADcFkEIAAA7KCo+qaz8wyoqPunsUlxGY/jOXGKyNAAAjdmib/cqdclPqjIkD5OUNixWw3uFObusRq2xfGeMCAEAcAGKik9aftAlqcqQ/rHkZ0aG6tCYvjOCEAAAF2D34ROWH/RqlYahwsOlZ98Ajeo7IwgBAHABIgJbysNUs62ZyaTwQF/nFOQCGtN3RhACAOACdAjwUdqwWDUznf5lb2Yyaeqw7uoQ4OPkyhqvxvSdcUPF8+CGigAAaxQVn1Th4VKFB/oSgqzkyO/MJVafBwCgqegQ4EMAslFj+M44NQYAANwWQQgAgEagMdxc0B1xagwAACdrLDcXdEeMCAEA4ESN6eaC7oggBACAA1h7qqsx3VzQHXFqDAAAO7PlVFf1zQXPDEPckLHhMCIEAIAd2XqqqzHdXNAdMSIEAIAd1XWq61zhZnivMPW7JIgbMjoBQQgAADuq76muxnBzQXfEqTEAAOyIU12uhREhAADsjFNdroMgBACAA3CqyzVwagwAALgtghAAAHBbBCEAAOC2CEIAAECS9cuCNCVMlgYAADYtC9KUMCIEAICbs3VZkKaEIAQAgJura1mQpo4gBACAm6teFuRM1iwL0hQQhAAAcHPuvCwIk6UBAIDbLgtCEAIAAJLcc1kQTo0BAAC3RRACAABuiyAEAICbccc7SJ8Lc4QAAHAj7noH6XNxmRGh8PBwmUymGo/09PRz9j9y5Igee+wxRUdHy8fHR2FhYRo7dqyKi4sbsGoAABoPd76D9Lm41IjQ5MmTlZiYaHnu5+d3zr4HDhzQgQMHNGPGDF166aXas2ePHnroIR04cEAffvhhQ5QLAECjUtcdpN3tarFqLhWE/Pz8FBISYlXf7t2766OPPrI879Kli1588UUlJCSooqJCzZu71EcHAOCCVd9B+sww5C53kD4Xlzk1Jknp6elq166devTooenTp6uiosKm7YuLi+Xv719nCDKbzSopKanxAACgKXDnO0ifi8sMi4wdO1ZxcXFq27atsrKylJqaqqKiImVkZFi1/eHDhzVlyhQ9+OCDdfZLS0vTpEmT7FEyAACNjrveQfpcTIZhGOfv5hgpKSmaNm1anX1yc3MVExNTq33u3LlKSkrS8ePH5e3tXec+SkpKdNNNN6lt27ZasWKFPD09z9nXbDbLbDbX2DY0NNQymgQAABq/kpISBQQEnPf326lB6NChQ/r999/r7BMZGSkvL69a7Vu3blX37t21fft2RUdHn3P7Y8eOKT4+Xr6+vlq1apVatGhhU43WfpEAAKDxsPb326mnxoKCghQUFFSvbXNycuTh4aHg4OBz9ikpKVF8fLy8vb21YsUKm0MQAABo2lxisnR2drZmz56tLVu2qKCgQAsXLlRycrISEhLUpk0bSdL+/fsVExOjTZs2STodgm6++WadOHFCc+bMUUlJiQ4ePKiDBw+qsrLSmR8HAAA0Ei4xWdrb21uZmZmaOHGizGazIiIilJycrHHjxln6lJeXKy8vT6WlpZKkzZs3a+PGjZKkqKioGvvbvXu3wsPDG6x+AIDrKSo+qd2HTygisKXbTyhuypw6R8gVMEcIANwPy1C4Pmt/v13i1BgAAA2FZSjcC0EIAIAz1LUMBZoeghAAAGeoXobiTO6+DEVTRhACAOAMLEPhXlziqjEAABqSM5eh4Gq1hkUQAgDgLDoE+DR4EOFqtYbHqTEAABoBrlZzDoIQAACNAFerOQdBCACARoCr1ZyDIAQAQCPA1WrOwWRpAAAaCWdereauGBECAKCRqL50nhDUcBgRAgCgEeDSeedgRAgAACfj0nnnIQgBAOBkXDrvPAQhAACcjEvnnYcgBACAk3HpvPMwWRoAgEaAS+edgyAEAEAj4YyFXt0dp8YAAIDbIggBAAC3RRACAABuiyAEAADcFkEIAAC4LYIQAABwWwQhAADgtghCAADAbRGEAACA2yIIAQAAt0UQAgAAbosgBAAA3BZBCAAAuC2CEAAAcFsEIQAA4LYIQgAAwG0RhAAAgNsiCAEAALflMkEoPDxcJpOpxiM9Pb3ObZKSktSlSxf5+PgoKChIQ4cO1fbt2xuoYgAA0Ni5TBCSpMmTJ6uoqMjyeOyxx+rsf+WVV2revHnKzc3V559/LsMwdPPNN6uysrKBKgYAAI1Zc2cXYAs/Pz+FhIRY3f/BBx+0/Ds8PFwvvPCCLr/8chUWFqpLly6OKBEAALgQlxoRSk9PV7t27dSjRw9Nnz5dFRUVVm974sQJzZs3TxEREQoNDT1nP7PZrJKSkhoPAADQNLlMEBo7dqwyMzO1du1aJSUlaerUqXrmmWfOu92///1vtWrVSq1atdKnn36qL774Ql5eXufsn5aWpoCAAMujrtAEAABcm8kwDMNZb56SkqJp06bV2Sc3N1cxMTG12ufOnaukpCQdP35c3t7e59y+uLhYv/32m4qKijRjxgzt379f33zzjVq0aHHW/mazWWaz2fK8pKREoaGhKi4ulr+/v5WfDAAAOFNJSYkCAgLO+/vt1CB06NAh/f7773X2iYyMPOsIztatW9W9e3dt375d0dHRVr3fqVOn1KZNG7399tu6++67rdrG2i8SAAA0Htb+fjt1snRQUJCCgoLqtW1OTo48PDwUHBxs9TaGYcgwjBojPgAAwH25xByh7OxszZ49W1u2bFFBQYEWLlyo5ORkJSQkqE2bNpKk/fv3KyYmRps2bZIkFRQUKC0tTd9//7327t2rrKws3XnnnfLx8dHgwYOd+XEAAEAj4RKXz3t7eyszM1MTJ06U2WxWRESEkpOTNW7cOEuf8vJy5eXlqbS0VJLUokULrV+/XrNnz9bRo0fVvn179evXT1lZWTaNIgEAgKbLqXOEXAFzhAAAcD3W/n67xKkxAAAARyAIAQAAt0UQAgAAbosgBAAA3BZBCAAAuC2CEAAAcFsEIQAA4LYIQgAAwG0RhAAAgNsiCAEAALdVr7XGysvLdfDgQZWWliooKEht27a1d10AAAAOZ/WI0LFjx/Taa6/puuuuk7+/v8LDw9W1a1cFBQWpc+fOSkxM1LfffuvIWgEAAOzKqiCUkZGh8PBwzZs3TzfeeKOWLVumnJwc7dixQ9nZ2Xr++edVUVGhm2++WQMHDtTOnTsdXTcAAMAFs2r1+bvvvlsTJkxQt27d6uxnNps1b948eXl5afTo0XYr0plYfR4AANdj7e+3VUHInRGEAABwPdb+fnPVGAAAcFs2XzVWVlamV199VWvXrtVvv/2mqqqqGq9v3rzZbsUBAAA4ks1BaMyYMVq9erXuuOMO9e7dWyaTyRF1AQAAOJzNQWjVqlX65JNPdM011ziiHgAAgAZj8xyhiy66SH5+fo6oBQAAoEHZHIRmzpyp8ePHa8+ePY6oBwAAoMHYfGqsZ8+eKisrU2RkpHx9feXp6Vnj9SNHjtitOAAAAEeyOQjdfffd2r9/v6ZOnar27dszWRoAALgsm4NQVlaWsrOzdfnllzuiHgAAgAZj8xyhmJgYnTx50hG1AAAANCibg1B6erqefPJJrVu3Tr///rtKSkpqPAAAAFyFzWuNeXiczk5/nhtkGIZMJpMqKyvtV10jwFpjAAC4Hmt/v22eI7R27doLKgwAAKCxsDkIXXfddY6oAwAAoMHZHIQk6Y8//tCcOXOUm5srSerWrZtGjx6tgIAAuxYHAADgSDZPlv7uu+/UpUsXzZo1S0eOHNGRI0eUkZGhLl26sPI8AABwKTZPlu7bt6+ioqL01ltvqXnz0wNKFRUVeuCBB1RQUKCvv/7aIYU6C5OlAQBwPdb+ftschHx8fPTDDz8oJiamRvu2bdvUs2dPlZaW1q/iRoogBACA67H299vmU2P+/v7au3dvrfZ9+/axKj0AAHApNgeh4cOHa8yYMVq0aJH27dunffv2KTMzUw888IDuvvtuR9QIAADgEDZfNTZjxgyZTCaNHDlSFRUVkiRPT089/PDDSk9Pt3uBAAAAjmLzHKFqpaWlys/PlyR16dJFvr6+di2ssWCOEAAArsdhc4RGjx6tY8eOydfXV7GxsYqNjZWvr69OnDih0aNHX1DRAAAADcnmIPTOO++cdfX5kydPasGCBXYp6mzCw8NlMplqPKw9FWcYhgYNGiSTyaRly5Y5rEYAAOBarJ4jVFJSIsMwZBiGjh07phYtWlheq6ys1CeffKLg4GCHFFlt8uTJSkxMtDy39iq12bNn11okFgAAwOog1Lp1a8tIzCWXXFLrdZPJpEmTJtm1uD/z8/NTSEiITdvk5ORo5syZ+u6779ShQ4fz9jebzTKbzZbnJSUlNtcJAABcg9VBaO3atTIMQzfccIM++ugjtW3b1vKal5eXOnfurI4dOzqkyGrp6emaMmWKwsLCNGLECCUnJ1vubn02paWlGjFihP71r39ZHaDS0tIcHugAAEDjYPNVY3v27FFoaKg8PGyeXnRBMjIyFBcXp7Zt2yorK0upqam6//77lZGRcc5tkpKSVFlZqbffflvS6VGrpUuX6q9//es5tznbiFBoaChXjQEA4EKsvWrM5vsIde7cWdLp0Za9e/fq1KlTNV6/7LLLrN5XSkqKpk2bVmef3NxcxcTEaNy4cTXew8vLS0lJSUpLS5O3t3et7VasWKGvvvpKP/zwg9X1SJK3t/dZ9wcAAJoem4PQoUOHdP/99+vTTz896+uVlZVW7+vJJ5/UqFGj6uwTGRl51vY+ffqooqJChYWFio6OrvX6V199pfz8fLVu3bpG++23366+fftq3bp1VtcJAACaJpuD0BNPPKE//vhDGzduVP/+/bV06VL9+uuveuGFFzRz5kyb9hUUFKSgoCBbS5B0ehK0h4fHOa9US0lJ0QMPPFCjLTY2VrNmzdKQIUPq9Z4AAKBpsTkIffXVV1q+fLl69uwpDw8Pde7cWTfddJP8/f2VlpamW265xe5FZmdna+PGjbr++uvl5+en7OxsJScnKyEhQW3atJEk7d+/XwMGDNCCBQvUu3dvhYSEnHWCdFhYmCIiIuxeIwAAcD02B6ETJ05YRmHatGmjQ4cO6ZJLLlFsbKw2b95s9wKl0/N2MjMzNXHiRJnNZkVERCg5ObnGvKHy8nLl5eWptLTUITUAAICmx+YgFB0drby8PIWHh+vyyy/XG2+8ofDwcL3++utW3aenPuLi4vTf//63zj7h4eE63wVw9VxWDQAANFE2B6HHH39cRUVFkqTnn39eAwcO1MKFC+Xl5aX58+fbuz4AAACHqffq89VKS0u1fft2hYWFKTAw0F51NRqsPg8AgOux++rzffv21YwZM7Rjx44a7b6+voqLi2uSIQgAADRtVgehxMREZWdn68orr1TXrl01fvx4ffPNN8y7AQAALsvmU2Nms1lr1qzR8uXLtXLlSlVWVuqWW27Rbbfdpvj4ePn4+DiqVqfg1BgAAK7H7qfGqnl7e2vw4MF64403dODAAa1YsUIdOnTQs88+q3bt2unWW2/VN998c0HFAwAANASrR4QqKyvVrFmzOvvk5+drxYoVCg0N1R133GGXAp2NESEAAFyPtb/fVgehkJAQjRo1SqNHj9Yll1xit0IbO4IQAACux+6nxv7+97/rww8/VNeuXdW3b1/Nnz+fuzgDAACXZnUQevbZZ7Vr1y6tWbNGkZGRevTRR9WhQwclJiZq48aNjqwRAADAIWyeLN2/f3+98847OnjwoGbOnKnc3FxdddVV6tatmzIyMhxRIwAAgENc8J2lJenjjz/WyJEj9ccff6iystIedTUazBECAMD1OOzy+WqlpaWaP3++rrvuOt12221q166dXnzxxfruDgAAoMHZvOhqVlaW5s6dq8WLF6uiokJ33HGHpkyZon79+jmiPgAAAIexOgi99NJLmjdvnnbs2KGePXtq+vTpuvvuu+Xn5+fI+gAAABzG6iA0ffp0JSQkaPHixerevbsjawIAAGgQVgehAwcOyNPTs0ZbWVmZWrRoYfeiAAAAGoLVk6WrQ1BVVZWmTJmiiy66SK1atVJBQYGk0/cZmjNnjmOqBAAAcACbrxp74YUXNH/+fL300kvy8vKytHfv3l1vv/22XYsDAABwJJuD0IIFC/Tmm2/qnnvuqbEI6+WXX67t27fbtTgAAABHsjkI7d+/X1FRUbXaq6qqVF5ebpeiAAAAGoLNQejSSy/V+vXra7V/+OGH6tGjh12KAgAAaAg231Dxueee03333af9+/erqqpKS5YsUV5enhYsWKBVq1Y5okYAAACHsHlEaOjQoVq5cqW+/PJLtWzZUs8995xyc3O1cuVK3XTTTY6oEQAAwCHssuhqU8aiqwAAuB67LrpKVgIAAE2RVUGoW7duyszM1KlTp+rst3PnTj388MNKT0+3S3EAAACOZNVk6VdffVXjx4/XI488optuukk9e/ZUx44d1aJFCx09elTbtm3Thg0btHXrVj366KN6+OGHHV03AADABbNpjtCGDRu0aNEirV+/Xnv27NHJkycVGBioHj16KD4+Xvfcc4/atGnjyHobHHOEAABwPdb+fjNZ+jwIQgAAuB67TpY+U/UiqwAAAK7O5iAUFRWl66+/Xu+9957KysocURMAAECDsDkIbd68WZdddpnGjRunkJAQJSUladOmTY6oDQAAwKFsDkJXXHGFXn75ZR04cEBz585VUVGRrr32WnXv3l0ZGRk6dOiQI+oEAACwO5uDULXmzZtr2LBhWrx4saZNm6Zdu3bpqaeeUmhoqEaOHKmioiJ71gkAAGB39Q5C3333nR555BF16NBBGRkZeuqpp5Sfn68vvvhCBw4c0NChQ+1ZJwAAgN3ZvPp8RkaG5s2bp7y8PA0ePFgLFizQ4MGD5eFxOlNFRERo/vz5Cg8Pt3etAAAAdmXziNBrr72mESNGaM+ePVq2bJluvfVWSwiqFhwcrDlz5titSEkKDw+XyWSq8TjfUh79+/evtc1DDz1k17oAAIDrsnlEaOfOneft4+Xlpfvuu69eBdVl8uTJSkxMtDz38/M77zaJiYmaPHmy5bmvr6/d6wIAAK7J5iD0448/nrXdZDKpRYsWCgsLk7e39wUXdjZ+fn4KCQmxaRtfX1+btjGbzTKbzZbnJSUlNr0fAABwHTYvseHh4SGTyXTO1z09PTV8+HC98cYbatGixQUXWC08PFxlZWUqLy9XWFiYRowYoeTkZDVvfu4s179/f23dulWGYSgkJERDhgzRs88+W+eo0MSJEzVp0qRa7SyxAQCA63DYWmPLly/X+PHj9fTTT6t3796SpE2bNmnmzJl6/vnnVVFRoZSUFA0fPlwzZsy4sE9xhoyMDMXFxalt27bKyspSamqq7r//fmVkZJxzmzfffFOdO3dWx44d9eOPP2r8+PHq3bu3lixZcs5tzjYiFBoaShACAMCFWL1WqGGjXr16GZ999lmt9s8++8zo1auXYRiGsXTpUiMyMvK8+xo/frwhqc5Hbm7uWbedM2eO0bx5c6OsrMzq2tesWWNIMnbt2mX1NsXFxYYko7i42OptAACAc1n7+23zHKGffvpJnTt3rtXeuXNn/fTTT5JO333amhsqPvnkkxo1alSdfSIjI8/a3qdPH1VUVKiwsFDR0dHnL/z/30aSdu3apS5duli1DQAAaLpsDkIxMTFKT0/Xm2++KS8vL0lSeXm50tPTFRMTI0nav3+/2rdvf959BQUFKSgoyNYSJEk5OTny8PBQcHCwTdtIUocOHer1ngAAoGmxOQj961//0m233aZOnTrpsssuk3R6lKiyslKrVq2SJBUUFOiRRx6xW5HZ2dnauHGjrr/+evn5+Sk7O1vJyclKSEhQmzZtJJ0OXwMGDNCCBQvUu3dv5efn6/3339fgwYPVrl07/fjjj0pOTla/fv0sdQMAAPdm82RpSTp27JgWLlyoHTt2SJKio6M1YsQIq+7rUx+bN2/WI488ou3bt8tsNisiIkL33nuvxo0bZ7lUv7CwUBEREVq7dq369++vffv2KSEhQT///LNOnDih0NBQ/e1vf9OECRNsmvRs9WQrAADQaDjkqrHy8nLFxMRo1apV6tq1q10KbewIQgAAuB5rf79tWmLD09NTZWVlF1wcAABAY2DzWmN///vfNW3aNFVUVDiiHgAAgAZj82Tpb7/9VmvWrNHq1asVGxurli1b1ni9rpsVAgAANCY2B6HWrVvr9ttvd0QtAAAADcrmIDRv3jxH1AEAANDgbJ4jJEkVFRX68ssv9cYbb+jYsWOSpAMHDuj48eN2LQ4AAMCRbB4R2rNnjwYOHKi9e/fKbDbrpptukp+fn6ZNmyaz2azXX3/dEXUCAADYnc0jQo8//rh69uypo0ePysfHx9L+t7/9TWvWrLFrcQAAAI5k84jQ+vXrlZWVZVlnrFp4eLj2799vt8IAAAAczeYRoaqqKlVWVtZq/+WXXxy2xAYAAIAj2ByEbr75Zs2ePdvy3GQy6fjx43r++ec1ePBge9YGAADgUDYvuvrLL78oPj5ehmFo586d6tmzp3bu3KnAwEB9/fXXCg4OdlStTsFaYwAAuB6HLLparaKiQpmZmfrxxx91/PhxxcXF6Z577qkxebqpIAgBAOB6rP39tnmytCQ1b95cCQkJ9S4OAACgMahXENq5c6fWrl2r3377TVVVVTVee+655+xSGAAAgKPZHITeeustPfzwwwoMDFRISIhMJpPlNZPJRBACAAAuw+Yg9MILL+jFF1/U+PHjHVEPAABAg7H58vmjR4/qzjvvdEQtAAAADcrmIHTnnXdq9erVjqgFAACgQdl8aiwqKkrPPvus/vvf/yo2Nlaenp41Xh87dqzdigMAAHAkm+8jFBERce6dmUwqKCi44KIaE+4jBACA63HYfYR27959QYUBAAA0FjbPEap26tQp5eXlqaKiwp71AAAANBibg1BpaanGjBkjX19fdevWTXv37pUkPfbYY0pPT7d7gQAAAI5icxBKTU3Vli1btG7dOrVo0cLSfuONN2rRokV2LQ4AAMCRbJ4jtGzZMi1atEh/+ctfatxVulu3bsrPz7drcQAAAI5k84jQoUOHFBwcXKv9xIkTNYIRAABAY2dzEOrZs6c+/vhjy/Pq8PP222/rqquusl9lAAAADmbzqbGpU6dq0KBB2rZtmyoqKvTyyy9r27ZtysrK0n/+8x9H1AgAAOAQNo8IXXvttcrJyVFFRYViY2O1evVqBQcHKzs7W1deeaUjagQAAHAIm+8s7W64szQAAK7H2t/vet9QEQAAwNURhAAAgNsiCAEAALdFEAIAAG7L6iBUUFAg5lUDAICmxOogdPHFF+vQoUOW58OHD9evv/7qkKIAAAAagtVB6M+jQZ988olOnDhh94LOJTw8XCaTqcbDmtXus7OzdcMNN6hly5by9/dXv379dPLkyQaoGAAANHY231namSZPnqzExETLcz8/vzr7Z2dna+DAgUpNTdWrr76q5s2ba8uWLfLwYGoUAACwIQhVj8L8ua0h+fn5KSQkxOr+ycnJGjt2rFJSUixt0dHRjigNAAC4IKvvLO3h4aFBgwbJ29tbkrRy5UrLKaczLVmyxP5V6vSpsbKyMpWXlyssLEwjRoxQcnKymjc/e5b77bff1L59e73yyiv63//9X+Xn5ysmJkYvvviirr322nO+j9lsltlstjwvKSlRaGgod5YGAMCFWHtnaatHhEaOHFljBCghIeHCKrTR2LFjFRcXp7Zt2yorK0upqakqKipSRkbGWfsXFBRIkiZOnKgZM2boiiuu0IIFCzRgwAD9/PPPuvjii8+6XVpamiZNmuSwzwEAABoPp641lpKSomnTptXZJzc3VzExMbXa586dq6SkJB0/ftwySnWmrKwsXXPNNUpNTdXUqVMt7ZdddpluueUWpaWlnfX9GBECAMD12X1EqKCgQBEREXadF/Tkk09q1KhRdfaJjIw8a3ufPn1UUVGhwsLCs8776dChgyTp0ksvrdHetWtX7d2795zv5+3tfdZgBQAAmh6rg9DFF1+soqIiBQcHSzp9H6FXXnlF7du3r/ebBwUFKSgoqF7b5uTkyMPDw1LPn4WHh6tjx47Ky8ur0b5jxw4NGjSoXu8JAACaFpe4j1B2drZmz56tLVu2qKCgQAsXLlRycrISEhLUpk0bSdL+/fsVExOjTZs2STp9RdvTTz+tV155RR9++KF27dqlZ599Vtu3b9eYMWMapG4AANC4ucR9hLy9vZWZmamJEyfKbDYrIiJCycnJGjdunKVPeXm58vLyVFpaaml74oknVFZWpuTkZB05ckSXX365vvjiC3Xp0sUZHwMAADQyVk+WbtasmQ4ePGg5leXn56cff/xRERERDi3Q2aydbAUAABoPu0+WNgxDo0aNskwkLisr00MPPdRg9xECAACwN6uD0H333VfjeUPfRwgAAMDerA5C8+bNc2QdAAAADY7VRwEAgNsiCAEAALdFEAIAAG6LIAQAANwWQQgAALgtghAAAHBbBCEAAOC2CEIAAMBtEYQAAIDbIggBAAC3RRACAABuiyAEAADcFkEIAAC4LYIQAABwWwQhAADgtghCcDtFxSeVlX9YRcUnnV0KAMDJmju7AKAhLfp2r1KX/KQqQ/IwSWnDYjW8V5izywIAOAkjQnAbRcUnLSFIkqoM6R9LfmZkCADcGEEIbmP34ROWEFSt0jBUeLjUOQUBAJyOIAS3ERHYUh6mmm3NTCaFB/o6pyAAgNMRhOA2OgT4KG1YrJqZTqehZiaTpg7rrg4BPk6uDADgLEyWhlsZ3itM/S4JUuHhUoUH+hKCAMDNEYTgdjoE+BCAAACSODUGAADcGEEIAAC4LYIQAABwWwQhAADgtghCAADAbRGEAACA2yIIAQAAt0UQgkspKj6prPzDLJQKALALbqgIl7Ho272W1eM9TFLasFgN7xXm7LIAAC6MESG4hKLik5YQJElVhvSPJT8zMgQAuCAEIbiE3YdPWEJQtUrDUOHhUucUBABoElwmCIWHh8tkMtV4pKenn7N/YWFhrf7Vj8WLFzdg5bCHiMCW8jDVbGtmMik80Nc5BQEAmgSXCUKSNHnyZBUVFVkejz322Dn7hoaG1uhbVFSkSZMmqVWrVho0aFADVg176BDgo7RhsWpmOp2GmplMmjqsO4unAgAuiEtNlvbz81NISIhVfZs1a1ar79KlS3XXXXepVatWjigPDja8V5j6XRKkwsOlCg/0JQQBAC6YS40Ipaenq127durRo4emT5+uiooKq7f9/vvvlZOTozFjxtTZz2w2q6SkpMYDjUeHAB9d1aUdIQgAYBcuMyI0duxYxcXFqW3btsrKylJqaqqKioqUkZFh1fZz5sxR165ddfXVV9fZLy0tTZMmTbJHyQAAoJEzGYZhnL+bY6SkpGjatGl19snNzVVMTEyt9rlz5yopKUnHjx+Xt7d3nfs4efKkOnTooGeffVZPPvlknX3NZrPMZrPleUlJiUJDQ1VcXCx/f/86twUAAI1DSUmJAgICzvv77dQRoSeffFKjRo2qs09kZORZ2/v06aOKigoVFhYqOjq6zn18+OGHKi0t1ciRI89bk7e393mDFQAAaBqcGoSCgoIUFBRUr21zcnLk4eGh4ODg8/adM2eObrvttnq/FwAAaJpcYrJ0dna2Zs+erS1btqigoEALFy5UcnKyEhIS1KZNG0nS/v37FRMTo02bNtXYdteuXfr666/1wAMPOKN0AADQiLnEZGlvb29lZmZq4sSJMpvNioiIUHJyssaNG2fpU15erry8PJWW1rzT8Ny5c9WpUyfdfPPNDV02AABo5Jw6WdoVWDvZCgAANB7W/n67xKkxAAAARyAIAQAAt0UQAgAAbosgBAAA3BZByInW5B7UP5f+qDW5B51dCgAAbsklLp9viob9+xtt3vuHJGnhxn2KC2utJY9c49yiLkBR8UntPnxCEYEtWRAVAOAyCEJOsCb3oCUEVdu89w+tyT2oAV1DGuT9v9r+m26ICbbL+73xdb7SP90uw5A8TFLasFgN7xVmh0oBAHAsgpATfLX9t7O2r8s75PAgZO+RqDf+k6+0T7dbnlcZ0j+W/Kx+lwQxMgQAaPSYI+QEN8ScfX20/tGOXQutrpGo+igqPqn0M0JQtUrDUOHh0rNsAQBA40IQcoIBXUMUF9a6RltcWGuHjwbVNRJVH7sPn9DZbkvuYZLCA33rtU8AABoSp8acZMkj12hN7kGtyzuk/tFBDTI36IaYYC3cuK9We31HoiICW8rDdPp02JnGD4rhtBgAwCUwIuREA7qGaMpfYxskBFW/nz1HojoE+ChtWKyamUySTv/HlDooRkn9ulxgpQAANAwWXT2Pprjoqr1HooqKT6rwcKnCA30ZCQIANArW/n4ThM6jKQYhAACaOlafBwAAOA+CEAAAcFsEIQAA4LYIQgAAwG0RhABcsKLik8rKP6yi4pPOLgUAbMINFQFckEXf7lXqkp9UxaK7AFwQI0IA6q2o+KQlBEn/t+guI0MAXAVBCEC97T58otYSKyy6C8CVEIQA1Fv1enNnamYyseguAJdBEAJQb39eb66ZyaSpw7qz1AoAl8FkaQAXZHivMPW7JIj15gC4JIIQgAvWIcCHAATAJXFqDAAAuC2CEAAAcFsEIQAA4LYIQgAAwG0RhAAAgNsiCAEAALdFEAIAAG6LIAQAANwWQQgAALgtghAAAHBbBCHARRQVn1RW/mEVFZ90dikA0GS4TBAKDw+XyWSq8UhPT69zm4MHD+ree+9VSEiIWrZsqbi4OH300UcNVDFgP4u+3atr0r/SiLc26pr0r7To273OLgkAmgSXWnR18uTJSkxMtDz38/Ors//IkSP1xx9/aMWKFQoMDNT777+vu+66S99995169Ojh6HIBuygqPqnUJT+pyjj9vMqQ/rHkZ/W7JIiFTgHgArnMiJB0OviEhIRYHi1btqyzf1ZWlh577DH17t1bkZGRmjBhglq3bq3vv/++gSoGLtzuwycsIahapWGo8HCpcwoCgCbEpYJQenq62rVrpx49emj69OmqqKios//VV1+tRYsW6ciRI6qqqlJmZqbKysrUv3//c25jNptVUlJS4wE4U0RgS3mYarY1M5kUHujrnIIAoAlxmSA0duxYZWZmau3atUpKStLUqVP1zDPP1LnNBx98oPLycrVr107e3t5KSkrS0qVLFRUVdc5t0tLSFBAQYHmEhoba+6MANukQ4KO0YbFqZjqdhpqZTJo6rDunxQDADkyGYRjn7+YYKSkpmjZtWp19cnNzFRMTU6t97ty5SkpK0vHjx+Xt7X3WbR977DFt2rRJU6dOVWBgoJYtW6ZZs2Zp/fr1io2NPes2ZrNZZrPZ8rykpEShoaEqLi6Wv7+/DZ8OsK+i4pMqPFyq8EBfQhAAnEdJSYkCAgLO+/vt1CB06NAh/f7773X2iYyMlJeXV632rVu3qnv37tq+fbuio6NrvZ6fn6+oqCj9/PPP6tatm6X9xhtvVFRUlF5//XWrarT2iwQAAI2Htb/fTr1qLCgoSEFBQfXaNicnRx4eHgoODj7r66WlpyeSenjUPPvXrFkzVVVV1es9AQBA0+ISc4Sys7M1e/ZsbdmyRQUFBVq4cKGSk5OVkJCgNm3aSJL279+vmJgYbdq0SZIUExOjqKgoJSUladOmTcrPz9fMmTP1xRdf6K9//asTP43r4AZ+AICmziXuI+Tt7a3MzExNnDhRZrNZERERSk5O1rhx4yx9ysvLlZeXZxkJ8vT01CeffKKUlBQNGTJEx48fV1RUlN555x0NHjzYWR/FZSz6dq/l3jUeJiltWKyG9wpzdlkAANiVU+cIuQJ3nCNUVHxS16R/VePeNR6Svkm9gUm6AACXYO3vt0ucGmuqGuupp7PdwK9K0rwNhc4oBwAAh3GJU2NN0Rtf5yv90+0yGuGpp4jAljJJ+vNQ4dsbCnT/teGMCgEAmgxGhJzgjf/kK+2T0yFI+r+1oxrLyFCHAB8l9o2o1V5liGUdAABNCkGogRUVn1T6p9trtTe2taPuvzaCZR0AAE0eQaiB7T58otYpJ+n06bHGFDJY1gEA4A6YI9TAqhfQ/PNk5PGDYhpdyBjeK0z9LgliWQcAQJPFiFAD+/NIi4ek1EExSurXxbmFnUOHAB9d1aUdIQgA0CQxIuQEjLQAANA4EIScpEOADwEIAAAn49QYAABwWwQhAADgtghCAADAbRGEAACA2yIIAQAAt0UQAgAAbosgBAAA3BZBCAAAuC2CEAAAcFsEIQAA4LYIQgAAwG2x1th5GIYhSSopKXFyJQAAwFrVv9vVv+PnQhA6j2PHjkmSQkNDnVwJAACw1bFjxxQQEHDO103G+aKSm6uqqtKBAwfk5+cnk8lk8/YlJSUKDQ3Vvn375O/v74AKYW8cM9fDMXM9HDPX42rHzDAMHTt2TB07dpSHx7lnAjEidB4eHh7q1KnTBe/H39/fJf7Dwf/hmLkejpnr4Zi5Hlc6ZnWNBFVjsjQAAHBbBCEAAOC2CEIO5u3treeff17e3t7OLgVW4pi5Ho6Z6+GYuZ6mesyYLA0AANwWI0IAAMBtEYQAAIDbIggBAAC3RRACAABuiyDkAOHh4TKZTDUe6enpVm1rGIYGDRokk8mkZcuWObZQWNh6zI4cOaLHHntM0dHR8vHxUVhYmMaOHavi4uIGrNq91efvrKysTH//+9/Vrl07tWrVSrfffrt+/fXXBqoY1cxms6644gqZTCbl5OTU2ffgwYO69957FRISopYtWyouLk4fffRRwxQKSbYdL0nKzs7WDTfcoJYtW8rf31/9+vXTyZMnHV9oPXFnaQeZPHmyEhMTLc/9/Pys2m727Nn1WsoDF86WY3bgwAEdOHBAM2bM0KWXXqo9e/booYce0oEDB/Thhx82RLmQ7X9nycnJ+vjjj7V48WIFBATo0Ucf1bBhw/TNN984ulSc4ZlnnlHHjh21ZcuW8/YdOXKk/vjjD61YsUKBgYF6//33ddddd+m7775Tjx49GqBa2HK8srOzNXDgQKWmpurVV19V8+bNtWXLljqXuHA6A3bXuXNnY9asWTZv98MPPxgXXXSRUVRUZEgyli5davfacHb1PWZn+uCDDwwvLy+jvLzcPkWhTrYesz/++MPw9PQ0Fi9ebGnLzc01JBnZ2dkOqBBn88knnxgxMTHG1q1bDUnGDz/8UGf/li1bGgsWLKjR1rZtW+Ott95yYJWoZuvx6tOnjzFhwoSGKc5OGnFEc23p6elq166devTooenTp6uioqLO/qWlpRoxYoT+9a9/KSQkpIGqxJlsPWZ/VlxcLH9/fzVvzkBrQ7HlmH3//fcqLy/XjTfeaGmLiYlRWFiYsrOzG6Jct/frr78qMTFR7777rnx9fa3a5uqrr9aiRYt05MgRVVVVKTMzU2VlZerfv79ji4XNx+u3337Txo0bFRwcrKuvvlrt27fXddddpw0bNjRAtfXH/2I7wNixYxUXF6e2bdsqKytLqampKioqUkZGxjm3SU5O1tVXX62hQ4c2YKWoVp9jdqbDhw9rypQpevDBBx1cKarZeswOHjwoLy8vtW7dukZ7+/btdfDgwQao2L0ZhqFRo0bpoYceUs+ePVVYWGjVdh988IGGDx+udu3aqXnz5vL19dXSpUsVFRXl2ILdXH2OV0FBgSRp4sSJmjFjhq644gotWLBAAwYM0M8//6yLL77YwVXXk7OHpFzF+PHjDUl1PnJzc8+67Zw5c4zmzZsbZWVlZ319+fLlRlRUlHHs2DFLmzg1dsEceczOVFxcbPTu3dsYOHCgcerUKXt/DLfiyGO2cOFCw8vLq1Z7r169jGeeecaun8OdWHvMXn75ZeOaa64xKioqDMMwjN27d1t1quXRRx81evfubXz55ZdGTk6OMXHiRCMgIMD48ccfG+DTNT2OPF7ffPONIclITU2t0R4bG2ukpKQ48mNdEJbYsNKhQ4f0+++/19knMjJSXl5etdq3bt2q7t27a/v27YqOjq71+hNPPKFXXnmlxmSyyspKeXh4qG/fvlq3bt0F1++OHHnMqh07dkzx8fHy9fXVqlWr1KJFiwuu25058ph99dVXGjBggI4ePVpjVKhz58564oknlJycfMH1uyNrj9ldd92llStX1rgYpLKyUs2aNdM999yjd955p9Z2+fn5ioqK0s8//6xu3bpZ2m+88UZFRUXp9ddft98HcROOPF67d+9WZGSk3n33XSUkJFjahw8frubNm2vhwoX2+yB2xKkxKwUFBSkoKKhe2+bk5MjDw0PBwcFnfT0lJUUPPPBAjbbY2FjNmjVLQ4YMqdd7wrHHTJJKSkoUHx8vb29vrVixghBkB448ZldeeaU8PT21Zs0a3X777ZKkvLw87d27V1dddVW9a3Z31h6zV155RS+88ILl+YEDBxQfH69FixapT58+Z92mtLRUkmpdcdSsWTNVVVVdQNXuy5HHKzw8XB07dlReXl6N9h07dmjQoEEXVrgjOXtIqqnJysoyZs2aZeTk5Bj5+fnGe++9ZwQFBRkjR4609Pnll1+M6OhoY+PGjefcjzg11mDqc8yKi4uNPn36GLGxscauXbuMoqIiy6N6KBmOU9+/s4ceesgICwszvvrqK+O7774zrrrqKuOqq65yxkdwe2c71fLnY3bq1CkjKirK6Nu3r7Fx40Zj165dxowZMwyTyWR8/PHHTqrcPVlzvAzDMGbNmmX4+/sbixcvNnbu3GlMmDDBaNGihbFr1y4nVG0dRoTszNvbW5mZmZo4caLMZrMiIiKUnJyscePGWfqUl5crLy/P8v924Fz1OWabN2/Wxo0bJanWpM3du3crPDy8wep3R/X9O5s1a5Y8PDx0++23y2w2Kz4+Xv/+97+d8RFwFn8+Zp6envrkk0+UkpKiIUOG6Pjx44qKitI777yjwYMHO7lanO1v7IknnlBZWZmSk5N15MgRXX755friiy/UpUsXJ1ZaN+YIAQAAt8V9hAAAgNsiCAEAALdFEAIAAG6LIAQAANwWQQgAALgtghAAAHBbBCEAAOC2CEIAAMBtEYQA4BzWrFmjrl27qrKy0tmlNLj/9//+n2bOnOnsMgCHIwgBTdSoUaNkMplkMpnk6emp9u3b66abbtLcuXNrLVgZHh5u6Vv96NSpkyZOnFir/c+PpuyZZ57RhAkT1KxZs/P2PXXqlAIDA5Wenn7W16dMmaL27durvLzc0vbOO+/o2muvtaqWdevWyWQy6Y8//rCq/4WaMGGCXnzxRRUXFzfI+wHOQhACmrCBAweqqKhIhYWF+vTTT3X99dfr8ccf16233qqKiooafSdPnqyioiLL44cfftBTTz1Vo61Tp061+jUmp06dstu+NmzYoPz8fMtK9efj5eWlhIQEzZs3r9ZrhmFo/vz5GjlypDw9PS3ty5cv12233Wa3mu2pe/fu6tKli9577z1nlwI4FEEIaMK8vb0VEhKiiy66SHFxcfrHP/6h5cuX69NPP9X8+fNr9PXz81NISIjlERQUpFatWtVoa9asWa1+57Jhwwb17dtXPj4+Cg0N1dixY3XixAnL6+Hh4Zo6dapGjx4tPz8/hYWF6c0336yxj3379umuu+5S69at1bZtWw0dOlSFhYWW10eNGqW//vWvevHFF9WxY0dFR0dLkrKysnTFFVeoRYsW6tmzp5YtWyaTyaScnBwZhqGoqCjNmDGjxnvl5OTIZDJp165dkqTMzEzddNNNatGiRY1+y5cvV1xcnFq0aKHIyEhNmjTJEirHjBmjHTt2aMOGDTW2+c9//qOCggKNGTPG0lZWVqbVq1dbgtC7776rnj17Wr7fESNG6LfffpMkFRYW6vrrr5cktWnTRiaTSaNGjZIkmc1mjR07VsHBwWrRooWuvfZaffvtt5b3qR5J+vzzz9WjRw/5+Pjohhtu0G+//aZPP/1UXbt2lb+/v0aMGFFrIeghQ4YoMzPzXIcYaBIIQoCbueGGG3T55ZdryZIlDnuP/Px8DRw4ULfffrt+/PFHLVq0SBs2bNCjjz5ao9/MmTPVs2dP/fDDD3rkkUf08MMPKy8vT9Lpla3j4+Pl5+en9evX65tvvlGrVq00cODAGiM/a9asUV5enr744gutWrVKJSUlGjJkiGJjY7V582ZNmTJF48ePt/Q3mUwaPXp0rZGbefPmqV+/foqKipIkrV+/Xj179qzRZ/369Ro5cqQef/xxbdu2TW+88Ybmz5+vF198UZIUGxurXr16ae7cubX2ffXVVysmJqZG3RdddJGlrby8XFOmTNGWLVu0bNkyFRYWWsJOaGioPvroI0lSXl6eioqK9PLLL0s6ffruo48+0jvvvKPNmzcrKipK8fHxOnLkSI0aJk6cqP/5n/9RVlaWJWDOnj1b77//vj7++GOtXr1ar776ao1tevfurU2bNslsNp/1OANNggGgSbrvvvuMoUOHnvW14cOHG127drU879y5s+Hl5WW0bNnS8nj55Zdrbde5c2dj1qxZ533vMWPGGA8++GCNtvXr1xseHh7GyZMnLftKSEiwvF5VVWUEBwcbr732mmEYhvHuu+8a0dHRRlVVlaWP2Ww2fHx8jM8//9zyGdu3b2+YzWZLn9dee81o166d5X0MwzDeeustQ5Lxww8/GIZhGPv37zeaNWtmbNy40TAMwzh16pQRGBhozJ8/37JNQECAsWDBghqfYcCAAcbUqVNrtL377rtGhw4dLM9ff/11o1WrVsaxY8cMwzCMkpISw9fX13j77bdrbJeYmGg89dRT5/wOv/32W0OSZT9r1641JBlHjx619Dl+/Ljh6elpLFy40NJ26tQpo2PHjsZLL71UY7svv/zS0ictLc2QZOTn51vakpKSjPj4+Bo1bNmyxZBkFBYWnrNOwNUxIgS4IcMwak10fvrpp5WTk2N5jBw5st7737Jli+bPn69WrVpZHvHx8aqqqtLu3bst/S677DLLv00mk0JCQiyng7Zs2aJdu3bJz8/Pso+2bduqrKxM+fn5lu1iY2Pl5eVleZ6Xl6fLLrusximt3r1716ivY8eOuuWWWywjNytXrpTZbNadd95p6XPy5Mlap8W2bNmiyZMn1/hciYmJKioqspxWuvvuu1VZWakPPvhAkrRo0SJ5eHho+PDhlv0YhqGVK1fWmB/0/fffa8iQIQoLC5Ofn5+uu+46SdLevXvP+T3n5+ervLxc11xzjaXN09NTvXv3Vm5ubo2+Z37X7du3l6+vryIjI2u0VX/31Xx8fCSp1ikzoClp7uwCADS83NxcRURE1GgLDAy0nBa6UMePH1dSUpLGjh1b67WwsDDLv8+cOCydDkPVV7QdP35cV155pRYuXFhrH0FBQZZ/t2zZsl41PvDAA7r33ns1a9YszZs3T8OHD5evr6/l9cDAQB09erTW55o0aZKGDRtWa3/Vocnf31933HGH5s2bZzkFd9ddd6lVq1aWvps2bVJFRYWuvvpqSdKJEycUHx+v+Ph4LVy4UEFBQdq7d6/i4+PtNgH8zO+6+krCM5353VerPr125vcNNDUEIcDNfPXVV/rpp5+UnJzssPeIi4vTtm3bLihYxcXFadGiRQoODpa/v7/V20VHR+u9996T2WyWt7e3JNWYPFxt8ODBatmypV577TV99tln+vrrr2u83qNHD23btq1WTXl5eef9XGPGjFH//v21atUqZWVlafr06TVeX758uW655RbLZfnbt2/X77//rvT0dIWGhkqSvvvuuxrbVI96nXlPoy5dusjLy0vffPONOnfuLOn0XKNvv/1WTzzxRJ01WuPnn39Wp06dFBgYeMH7AhorTo0BTZjZbNbBgwe1f/9+bd68WVOnTtXQoUN16623XtCpr/MZP368srKy9OijjyonJ0c7d+7U8uXLa02Wrss999yjwMBADR06VOvXr9fu3bu1bt06jR07Vr/88ss5txsxYoSqqqr04IMPKjc3V59//rnlCrEzTwc2a9ZMo0aNUmpqqi6++GJdddVVNfYTHx9f6+qv5557TgsWLNCkSZO0detW5ebmKjMzUxMmTKjRr3rS9ciRIxUTE2MZ+am2YsWKGqfFwsLC5OXlpVdffVUFBQVasWKFpkyZUmObzp07y2QyadWqVTp06JCOHz+uli1b6uGHH9bTTz+tzz77TNu2bVNiYqJKS0trXKFWX+vXr9fNN998wfsBGjOCENCEffbZZ+rQoYPCw8M1cOBArV27Vq+88oqWL19u1U0C6+uyyy7Tf/7zH+3YsUN9+/ZVjx499Nxzz6ljx45W78PX11dff/21wsLCNGzYMHXt2lVjxoxRWVlZnSNE/v7+WrlypXJycnTFFVfon//8p5577jlJqjXnZ8yYMTp16pTuv//+Wvu55557tHXrVstVbNLpcLRq1SqtXr1avXr10l/+8hfNmjXLMhpTrfrKtKNHj2r06NE1XsvPz9euXbsUHx9vaQsKCtL8+fO1ePFiXXrppUpPT691ef9FF12kSZMmKSUlRe3bt7eEyvT0dN1+++269957FRcXp127dunzzz9XmzZt6vp6z6usrEzLli1TYmLiBe0HaOxMhmEYzi4CABxp4cKFuv/++1VcXGyZACydHvEYMGCA9u3bp/bt29fa7umnn1ZJSYneeOMNu9WSkZGhL7/8Up988ond9ukIr732mpYuXarVq1c7uxTAoRgRAtDkLFiwQBs2bNDu3bu1bNkyjR8/XnfddZclBJnNZv3yyy+aOHGi7rzzzrOGIEn65z//qc6dO9eaRHwhOnXqpNTUVLvtz1E8PT1r3VcIaIoYEQLQ5Lz00kv697//rYMHD6pDhw6Wu09XXxU2f/58jRkzRldccYVWrFihiy66yMkVA3AWghAAAHBbnBoDAABuiyAEAADcFkEIAAC4LYIQAABwWwQhAADgtghCAADAbRGEAACA2yIIAQAAt/X/AU8j0BK8eWtnAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(actual_en,pred_en)#energy error MAE per atom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgJemMF5N3Wd",
        "outputId": "58bc56de-6f36-446a-807f-7b053e3c1f1c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47643506561811505"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_forces = np.concatenate(actual_forces)\n",
        "pred_forces = np.concatenate(pred_forces)"
      ],
      "metadata": {
        "id": "ms8fXomtOVsP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(actual_forces,pred_forces,'.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "R8DdStiMOlM0",
        "outputId": "7c03a097-2be7-4cf6-9532-357394c67b1c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7a07c406d7e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_error(actual_forces,pred_forces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NICx2BStO1Az",
        "outputId": "4b321790-80e9-4ebe-bd5b-604d2d6cbecf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16588544067946293"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvbS6NaC7U0w",
        "outputId": "8467c103-12cf-469b-9e58-5bca5482d7dc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['edge_index', 'pos', 'cell', 'edge_cell_shift', 'pbc', 'atom_types', 'node_attrs', 'node_features', 'edge_vectors', 'edge_lengths', 'edge_embedding', 'edge_attrs', 'edge_features', 'edge_energy', 'atomic_energy', 'batch', 'total_energy', 'forces'])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out['total_energy'].squeeze().cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4QLSATpJmIz",
        "outputId": "dd6ca853-476a-4960-b1f7-2773c88a4982"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-367.0040588378906"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out['forces'].squeeze().cpu().detach().numpy() #.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFIOTBuOJqWu",
        "outputId": "f28fde98-2977-467e-9b35-022ed826660e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.61515772e+00, -6.80820085e-06, -1.13735441e-05],\n",
              "       [-1.61518312e+00,  1.86236575e-05,  6.12670556e-06],\n",
              "       [-1.61514759e+00, -2.56691128e-05, -1.82315707e-05],\n",
              "       [-1.61517525e+00, -5.94649464e-07, -6.99888915e-07],\n",
              "       [-1.61517143e+00, -4.60422598e-06, -9.44850035e-06],\n",
              "       [-1.61519456e+00,  1.89463608e-05,  4.59002331e-06],\n",
              "       [-1.61516237e+00, -2.87981238e-05, -1.95782632e-05],\n",
              "       [-1.61518645e+00, -1.21444464e-06, -2.22027302e-06],\n",
              "       [ 1.61519849e+00,  4.01865691e-07,  9.20146704e-07],\n",
              "       [ 1.61517382e+00,  2.76596984e-05,  1.74047891e-05],\n",
              "       [ 1.61520576e+00, -1.73524022e-05, -5.71738929e-06],\n",
              "       [ 1.61517668e+00,  1.00303441e-05,  8.72369856e-06],\n",
              "       [ 1.61518931e+00,  2.70130113e-06,  8.57282430e-07],\n",
              "       [ 1.61516142e+00,  2.90463213e-05,  1.77004840e-05],\n",
              "       [ 1.61519659e+00, -1.61172356e-05, -6.57909550e-06],\n",
              "       [ 1.61517215e+00,  9.41334292e-06,  7.17490911e-06],\n",
              "       [-1.61519635e+00,  1.94204040e-05,  5.93042932e-06],\n",
              "       [-1.61519587e+00,  2.19694339e-05,  1.16575975e-05],\n",
              "       [-1.61518836e+00, -7.19912350e-07, -8.55885446e-07],\n",
              "       [-1.61518824e+00,  4.53460962e-06,  5.57815656e-06],\n",
              "       [-1.61521125e+00,  1.85775571e-05,  7.56606460e-06],\n",
              "       [-1.61521053e+00,  2.18357891e-05,  1.28047541e-05],\n",
              "       [-1.61520243e+00,  7.60774128e-07, -2.02655792e-06],\n",
              "       [-1.61520207e+00,  4.88944352e-06,  2.77161598e-06],\n",
              "       [-1.61517644e+00, -1.54483132e-07,  1.65309757e-07],\n",
              "       [-1.61517608e+00,  3.20363324e-06,  3.79281119e-06],\n",
              "       [-1.61517668e+00, -6.84650149e-06, -2.49990262e-06],\n",
              "       [-1.61517549e+00, -2.02073716e-06,  3.32265677e-06],\n",
              "       [-1.61518812e+00, -2.34134495e-06, -2.05053948e-06],\n",
              "       [-1.61518848e+00,  1.79139897e-06,  3.73879448e-06],\n",
              "       [-1.61518717e+00, -6.38654456e-06, -2.25286931e-06],\n",
              "       [-1.61518407e+00, -2.25752592e-06,  2.88423803e-06],\n",
              "       [-1.61516249e+00, -2.81489920e-05, -1.96543988e-05],\n",
              "       [-1.61519098e+00, -9.08039510e-07, -1.27178168e-06],\n",
              "       [-1.61516666e+00, -3.87171749e-05, -2.38078646e-05],\n",
              "       [-1.61519217e+00, -5.57163730e-06, -5.00958413e-06],\n",
              "       [-1.61517668e+00, -2.59703957e-05, -1.77205075e-05],\n",
              "       [-1.61520290e+00, -7.71600753e-07, -1.11642294e-06],\n",
              "       [-1.61518049e+00, -3.81347490e-05, -2.23834068e-05],\n",
              "       [-1.61520743e+00, -5.50970435e-06, -3.81268592e-06],\n",
              "       [ 1.61518931e+00,  6.82834070e-06,  1.66799873e-06],\n",
              "       [ 1.61516297e+00,  3.36355297e-05,  2.16457993e-05],\n",
              "       [ 1.61518836e+00, -2.62353569e-06,  1.26659870e-06],\n",
              "       [ 1.61516035e+00,  2.47559510e-05,  1.71952415e-05],\n",
              "       [ 1.61517930e+00,  5.58490865e-06,  2.67382711e-06],\n",
              "       [ 1.61515486e+00,  3.10484320e-05,  1.82397198e-05],\n",
              "       [ 1.61517954e+00, -5.49713150e-06,  2.55182385e-07],\n",
              "       [ 1.61515284e+00,  2.39596702e-05,  1.77863985e-05],\n",
              "       [ 1.61518884e+00, -1.98872294e-06, -4.03725426e-06],\n",
              "       [ 1.61518872e+00,  1.71177089e-06,  4.42021064e-06],\n",
              "       [ 1.61519885e+00, -1.90585852e-05, -8.17139517e-06],\n",
              "       [ 1.61519885e+00, -1.52122229e-05, -2.90766820e-06],\n",
              "       [ 1.61517704e+00, -3.52784991e-06, -2.46448303e-06],\n",
              "       [ 1.61517954e+00,  1.57952309e-06,  1.96634119e-06],\n",
              "       [ 1.61518931e+00, -1.89671991e-05, -8.29805413e-06],\n",
              "       [ 1.61518931e+00, -1.67712569e-05, -4.90599723e-06],\n",
              "       [ 1.61519778e+00,  1.24052167e-06, -1.89199704e-06],\n",
              "       [ 1.61519933e+00,  7.03451224e-06,  5.46330102e-06],\n",
              "       [ 1.61519670e+00, -6.68410212e-06, -4.53904886e-06],\n",
              "       [ 1.61519742e+00, -1.48720574e-06,  1.02438503e-06],\n",
              "       [ 1.61518931e+00,  3.58745456e-06, -2.83360032e-06],\n",
              "       [ 1.61519027e+00,  7.63137359e-06,  4.19297703e-06],\n",
              "       [ 1.61519217e+00, -4.14252281e-06, -6.26199562e-06],\n",
              "       [ 1.61519003e+00, -1.33551657e-06,  3.41485725e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7lR1ljg9Fum",
        "outputId": "07799c3b-e2c0-44a0-d945-a2fd51aa7dce"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37352"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ_0eoNe9j8d",
        "outputId": "ff19a328-f9cb-4770-80ec-1bf9c2948232"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_jit_bailout_depth': 2, '_jit_fusion_strategy': [('DYNAMIC', 3)], 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'allow_tf32': False, 'append': True, 'ase_args': {'format': 'extxyz'}, 'avg_num_neighbors': 27.008066177368164, 'batch_size': 1, 'chemical_symbol_to_type': {'Si': 0}, 'code_commits': {}, 'dataloader_num_workers': 0, 'dataset': 'ase', 'dataset_extra_fixed_fields': {'r_max': 5.0}, 'dataset_file_name': './Si_data/sitraj.xyz', 'dataset_seed': 123456, 'dataset_statistics_stride': 1, 'default_dtype': 'float32', 'device': 'cuda', 'e3nn_version': '0.5.1', 'early_stopping': None, 'early_stopping_kwargs': None, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_patiences': {'validation_loss': 100}, 'edge_eng_mlp_initialization': 'uniform', 'edge_eng_mlp_latent_dimensions': [32], 'edge_eng_mlp_nonlinearity': None, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'embed_initial_edge': True, 'end_of_batch_callbacks': [], 'end_of_epoch_callbacks': [], 'end_of_train_callbacks': [], 'env_embed_mlp_initialization': 'uniform', 'env_embed_mlp_latent_dimensions': [], 'env_embed_mlp_nonlinearity': None, 'env_embed_multiplicity': 8, 'equivariance_test': 1, 'exclude_keys': [], 'final_callbacks': [], 'grad_anomaly_mode': False, 'init_callbacks': [], 'irreps_edge_sh': '1x0e+1x1o', 'l_max': 1, 'latent_mlp_initialization': 'uniform', 'latent_mlp_latent_dimensions': [128], 'latent_mlp_nonlinearity': 'silu', 'latent_resnet': True, 'learning_rate': 0.002, 'log_batch_freq': 10, 'log_epoch_freq': 1, 'loss_coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}, 'lr_scheduler_factor': 0.5, 'lr_scheduler_kwargs': {'cooldown': 0, 'eps': 1e-08, 'factor': 0.5, 'min_lr': 0, 'mode': 'min', 'patience': 50, 'threshold': 0.0001, 'threshold_mode': 'rel', 'verbose': False}, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 50, 'max_epochs': 100, 'max_gradient_norm': inf, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'metrics_key': 'validation_loss', 'model_builders': ['allegro.model.Allegro', 'PerSpeciesRescale', 'ForceOutput', 'RescaleEnergyEtc'], 'model_debug_mode': False, 'n_train': 50, 'n_val': 10, 'nequip_version': '0.5.6', 'nonscalars_include_parity': True, 'num_layers': 1, 'num_types': 1, 'optimizer_kwargs': {'amsgrad': False, 'betas': (0.9, 0.999), 'capturable': False, 'eps': 1e-08, 'foreach': None, 'maximize': False, 'weight_decay': 0.0}, 'optimizer_name': 'Adam', 'optimizer_params': {'amsgrad': False, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0}, 'parity': 'o3_full', 'r_max': 5.0, 'report_init_validation': True, 'root': 'results/silicon-tutorial', 'run_id': '8gf60lsk', 'run_name': 'si', 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'seed': 123456, 'shuffle': True, 'torch_version': '1.12.1+cu102', 'train_idcs': tensor([28045, 23299, 27958, 15848, 28572, 16227, 26309, 20556,  8247, 11326,\n",
              "        10229, 18453, 21268, 21200, 27260,  8500, 29063,  3917, 34709, 31459,\n",
              "        33334, 24133,  8430, 25716,  3258, 12116, 16166, 13173, 30094,  1264,\n",
              "        16616,  5969, 11209, 11514,  1183,   935, 29944, 24324, 23657, 34168,\n",
              "        32145, 25833,  6923, 29876, 31671, 13592, 18533, 31474,  1024, 26994]), 'train_on_keys': ['forces', 'total_energy'], 'train_val_split': 'random', 'two_body_latent_mlp_initialization': 'uniform', 'two_body_latent_mlp_latent_dimensions': [32, 64, 128], 'two_body_latent_mlp_nonlinearity': 'silu', 'type_names': ['Si'], 'use_ema': True, 'val_idcs': tensor([24842, 20434, 17763, 17511, 17148,  2219,  1916,  1688,  9305, 32557]), 'validation_batch_size': 5, 'var_num_neighbors': 1.911603569984436, 'verbose': 'info', 'wandb': True, 'wandb_project': 'allegro-tutorial'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqfWeB-TIJXs"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}