{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSZNRzRE9wPY2PncbDNeqB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knc6/jarvis-tools-notebooks/blob/master/jarvis-tools-notebooks/Train_MLFF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install wandb\n",
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!pip install nequip==0.5.5 torch==1.11\n",
        "\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJl9OzZZKWh_",
        "outputId": "8a3a6c4b-fcbb-481b-bd1d-cb84e8967b98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=7e3076af0c06c4fbf0ceae693911d668c6209cc392a477a03f4f78043f9b12d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n",
            "Collecting nequip==0.5.5\n",
            "  Downloading nequip-0.5.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.11\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (1.22.4)\n",
            "Collecting ase (from nequip==0.5.5)\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (4.65.0)\n",
            "Collecting e3nn<0.6.0,>=0.3.5 (from nequip==0.5.5)\n",
            "  Downloading e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip==0.5.5)\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip==0.5.5)\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting scikit-learn<=1.0.1 (from nequip==0.5.5)\n",
            "  Downloading scikit-learn-1.0.1.tar.gz (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 28 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.97 KiB | 23.99 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nequip>=0.5.3 (from mir-allegro==0.2.0)\n",
            "  Downloading nequip-0.5.6-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.22.4)\n",
            "Collecting ase (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.65.0)\n",
            "Collecting torch!=1.9.0,<1.13,>=1.10.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting e3nn<0.6.0,>=0.4.4 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27439 sha256=9dd4197683a0bfd82447a42968dd5201fad9cd171b5a8428ff447fe458025647\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x82r3yrm/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: torch-runstats, torch, torch-ema, opt-einsum-fx, e3nn, ase, nequip, mir-allegro\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ase-3.22.1 e3nn-0.5.1 mir-allegro-0.2.0 nequip-0.5.6 opt-einsum-fx-0.1.4 torch-1.12.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruL2JdaB5hbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r1_3LmhI8y3",
        "outputId": "e2a6db8d-c350-4f3b-b657-db4f1719a0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 13:35:25--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ni/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/test.json [following]\n",
            "--2023-07-22 13:35:26--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611126 (597K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 596.80K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-07-22 13:35:26 (87.9 MB/s) - ‘test.json’ saved [611126/611126]\n",
            "\n",
            "--2023-07-22 13:35:26--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Cu/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/test.json [following]\n",
            "--2023-07-22 13:35:26--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 608400 (594K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 594.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:27 (55.0 MB/s) - ‘test.json’ saved [608400/608400]\n",
            "\n",
            "--2023-07-22 13:35:27--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Mo/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/test.json [following]\n",
            "--2023-07-22 13:35:27--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250629 (245K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 244.75K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-22 13:35:27 (72.7 MB/s) - ‘test.json’ saved [250629/250629]\n",
            "\n",
            "--2023-07-22 13:35:27--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ge/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/test.json [following]\n",
            "--2023-07-22 13:35:28--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 311848 (305K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 304.54K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-22 13:35:28 (114 MB/s) - ‘test.json’ saved [311848/311848]\n",
            "\n",
            "--2023-07-22 13:35:28--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Si/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/test.json [following]\n",
            "--2023-07-22 13:35:28--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 309803 (303K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 302.54K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-07-22 13:35:29 (63.5 MB/s) - ‘test.json’ saved [309803/309803]\n",
            "\n",
            "--2023-07-22 13:35:29--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Li/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/test.json [following]\n",
            "--2023-07-22 13:35:29--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279684 (273K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 273.13K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-22 13:35:29 (86.8 MB/s) - ‘test.json’ saved [279684/279684]\n",
            "\n",
            "allegro       Ge_test.json  Mo_test.json  sample_data\n",
            "Cu_test.json  Li_test.json  Ni_test.json  Si_test.json\n"
          ]
        }
      ],
      "source": [
        "# get data from mlearn\n",
        "#!rm *.json\n",
        "!for m in Ni Cu Mo Ge Si Li; do wget https://github.com/materialsvirtuallab/mlearn/raw/master/data/${m}/test.json; mv test.json ${m}_test.json; done;\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data from mlearn\n",
        "#!rm *.json\n",
        "!for m in Ni Cu Mo Ge Si Li; do wget https://github.com/materialsvirtuallab/mlearn/raw/master/data/${m}/training.json; mv training.json ${m}_training.json; done;\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soGwt8gCJUx4",
        "outputId": "c1ee3535-8b74-41ea-91f9-382de6c1b4c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 13:35:30--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ni/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/training.json [following]\n",
            "--2023-07-22 13:35:30--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5289402 (5.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   5.04M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-22 13:35:31 (184 MB/s) - ‘training.json’ saved [5289402/5289402]\n",
            "\n",
            "--2023-07-22 13:35:31--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Cu/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/training.json [following]\n",
            "--2023-07-22 13:35:31--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5246466 (5.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   5.00M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 13:35:32 (326 MB/s) - ‘training.json’ saved [5246466/5246466]\n",
            "\n",
            "--2023-07-22 13:35:32--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Mo/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/training.json [following]\n",
            "--2023-07-22 13:35:32--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2127325 (2.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.03M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:33 (182 MB/s) - ‘training.json’ saved [2127325/2127325]\n",
            "\n",
            "--2023-07-22 13:35:33--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ge/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/training.json [following]\n",
            "--2023-07-22 13:35:33--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2796663 (2.7M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.67M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:34 (219 MB/s) - ‘training.json’ saved [2796663/2796663]\n",
            "\n",
            "--2023-07-22 13:35:34--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Si/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/training.json [following]\n",
            "--2023-07-22 13:35:34--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2669946 (2.5M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.55M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:35 (171 MB/s) - ‘training.json’ saved [2669946/2669946]\n",
            "\n",
            "--2023-07-22 13:35:35--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Li/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/training.json [following]\n",
            "--2023-07-22 13:35:35--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446522 (2.3M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:36 (185 MB/s) - ‘training.json’ saved [2446522/2446522]\n",
            "\n",
            "allegro\t\t  Ge_training.json  Mo_training.json  Si_test.json\n",
            "Cu_test.json\t  Li_test.json\t    Ni_test.json      Si_training.json\n",
            "Cu_training.json  Li_training.json  Ni_training.json\n",
            "Ge_test.json\t  Mo_test.json\t    sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymatgen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O3hJ4WVJDyf",
        "outputId": "9266397a-fd59-4aef-a45e-81405c210708"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymatgen\n",
            "  Downloading pymatgen-2023.7.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.7.1)\n",
            "Collecting monty>=3.0.2 (from pymatgen)\n",
            "  Downloading monty-2023.5.8-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mp-api>=0.27.3 (from pymatgen)\n",
            "  Downloading mp_api-0.33.3-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.1)\n",
            "Requirement already satisfied: numpy>=1.20.1 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.22.4)\n",
            "Requirement already satisfied: palettable>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.5.3)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (5.13.1)\n",
            "Collecting pybtex (from pymatgen)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.10.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pymatgen) (2.27.1)\n",
            "Collecting ruamel.yaml>=0.17.0 (from pymatgen)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.10.1)\n",
            "Collecting spglib>=2.0.2 (from pymatgen)\n",
            "  Downloading spglib-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (515 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.11.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from pymatgen) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pymatgen) (4.65.0)\n",
            "Collecting uncertainties>=3.1.4 (from pymatgen)\n",
            "  Downloading uncertainties-3.1.7-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (67.7.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (4.7.1)\n",
            "Collecting emmet-core>=0.54.0 (from mp-api>=0.27.3->pymatgen)\n",
            "  Downloading emmet_core-0.63.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.5.0->pymatgen) (8.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (3.4)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.0->pymatgen)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from uncertainties>=3.1.4->pymatgen) (0.18.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pymatgen) (2022.7.1)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.10/dist-packages (from pybtex->pymatgen) (6.0.1)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex->pymatgen)\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pybtex->pymatgen) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->pymatgen) (1.3.0)\n",
            "Installing collected packages: uncertainties, spglib, ruamel.yaml.clib, monty, latexcodec, ruamel.yaml, pybtex, emmet-core, mp-api, pymatgen\n",
            "Successfully installed emmet-core-0.63.0 latexcodec-2.0.1 monty-2023.5.8 mp-api-0.33.3 pybtex-0.24.0 pymatgen-2023.7.20 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 spglib-2.0.2 uncertainties-3.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FqDN7vTJ8ds",
        "outputId": "ffa1c5e5-28c9-4ebe-a124-085e9e59ef66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'allegro' already exists and is not an empty directory.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.5.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.22.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.22.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.9.0,<1.13,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27439 sha256=60dd7bf49ae0a56b9bd1d8099e07d5c17c47d4807275e4df424c63bc088e3b4e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fd3p20ar/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "  Attempting uninstall: mir-allegro\n",
            "    Found existing installation: mir-allegro 0.2.0\n",
            "    Uninstalling mir-allegro-0.2.0:\n",
            "      Successfully uninstalled mir-allegro-0.2.0\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jarvis-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_vtPWpNY7Tl",
        "outputId": "451cbf04-9271-40b5-9635-8c3b1e8f16b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jarvis-tools\n",
            "  Downloading jarvis_tools-2023.5.26-py2.py3-none-any.whl (974 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/974.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/974.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (3.7.1)\n",
            "Requirement already satisfied: spglib>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (2.0.2)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (2.27.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (0.12.0)\n",
            "Collecting xmltodict>=0.11.0 (from jarvis-tools)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (4.65.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->jarvis-tools) (1.16.0)\n",
            "Installing collected packages: xmltodict, jarvis-tools\n",
            "Successfully installed jarvis-tools-2023.5.26 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from jarvis.core.atoms import pmg_to_atoms\n",
        "from jarvis.db.jsonutils import dumpjson\n",
        "from jarvis.db.jsonutils import loadjson\n",
        "from monty.serialization import loadfn, MontyEncoder, MontyDecoder\n",
        "from ase.stress import voigt_6_to_full_3x3_stress\n",
        "# Ref: https://github.com/materialsvirtuallab/mlearn\n",
        "data = loadfn(\n",
        "    \"Si_training.json\",\n",
        "    cls=MontyDecoder,\n",
        ")\n",
        "train_structures = [d[\"structure\"] for d in data]\n",
        "train_energies = [d[\"outputs\"][\"energy\"] for d in data]\n",
        "train_forces = [d[\"outputs\"][\"forces\"] for d in data]\n",
        "train_stresses = [d[\"outputs\"][\"virial_stress\"] for d in data]\n",
        "\n",
        "\n",
        "data = loadfn(\n",
        "    \"Si_test.json\",\n",
        "    cls=MontyDecoder,\n",
        ")\n",
        "test_structures = [d[\"structure\"] for d in data]\n",
        "test_energies = [d[\"outputs\"][\"energy\"] for d in data]\n",
        "test_forces = [d[\"outputs\"][\"forces\"] for d in data]\n",
        "test_stresses = [d[\"outputs\"][\"virial_stress\"] for d in data]\n",
        "\n"
      ],
      "metadata": {
        "id": "uQcYsDbgJIiC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "os.makedirs('Si_data')"
      ],
      "metadata": {
        "id": "huyOqxYmY5G7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"Si_data/sitraj.xyz\", \"w\")\n",
        "mem = []\n",
        "count = 0\n",
        "line = \"\"\n",
        "for i, j, k, l in zip(\n",
        "    train_structures, train_energies, train_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    #print(atoms)\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    #info[\"stresses\"] = l\n",
        "    #info[\"stresses\"] = voigt_6_to_full_3x3_stress(l)\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(l).flatten()))\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    #info[\"stresses\"] = l\n",
        "    #info[\"stresses\"] = voigt_6_to_full_3x3_stress(l)\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(l).flatten()))\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "f.close()\n",
        "dumpjson(data=mem, filename=\"Si_data/id_prop.json\")\n"
      ],
      "metadata": {
        "id": "e3Vk8wEOZOdg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!rm -rf ./results\n",
        "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiIbyRqIZtIW",
        "outputId": "6d2f3951-55a1-42f0-e42b-86f616f2d907"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230722_133612-bh3o27yv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-26092402475183654/allegro-tutorial?apiKey=5e4262c24ac1ed30e3b7b22a99893c835272cdb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-26092402475183654/allegro-tutorial/runs/bh3o27yv?apiKey=5e4262c24ac1ed30e3b7b22a99893c835272cdb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[2139211, 1], batch=[2139211], cell=[34980, 3, 3], edge_cell_shift=[57841530, 3], edge_index=[2, 57841530], forces=[2139211, 3], free_energy=[34980], pbc=[34980, 3], pos=[2139211, 3], ptr=[34981], stress=[34980, 3, 3], total_energy=[34980, 1])\n",
            "    processed data size: ~1629.18 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(34980)...\n",
            "Replace string dataset_forces_rms to 0.9137988090515137\n",
            "Replace string dataset_per_atom_total_energy_mean to -5.076940059661865\n",
            "Atomic outputs are scaled by: [Si: 0.913799], shifted by [Si: -5.076940].\n",
            "Replace string dataset_forces_rms to 0.9137988090515137\n",
            "Initially outputs are globally scaled by: 0.9137988090515137, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:276: UserWarning: operator() profile_node %884 : int[] = prim::profile_ivalue(%882)\n",
            " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:108.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Equivariance test passed; equivariance errors:\n",
            "   Errors are in real units, where relevant.\n",
            "   Please note that the large scale of the typical\n",
            "   shifts to the (atomic) energy can cause\n",
            "   catastrophic cancellation and give incorrectly\n",
            "   the equivariance error as zero for those fields.\n",
            "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field free_energy          -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field stress               -> max error=0.000e+00\n",
            "   node permutation equivariance of field forces                     -> max error=1.974e-07\n",
            "   edge & node permutation invariance for field pbc                  -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field total_energy         -> max error=6.104e-05\n",
            "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_embedding             -> max error=2.831e-07\n",
            "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_features              -> max error=1.192e-07\n",
            "   edge permutation equivariance of field edge_energy                -> max error=1.043e-07\n",
            "   node permutation equivariance of field atomic_energy              -> max error=4.768e-07\n",
            "   node permutation equivariance of field batch                      -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=4.678e-07\n",
            "   (parity_k=0, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=7.477e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=2.147e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=4.686e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=6.557e-07\n",
            "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=forces              )     -> max error=1.570e-06\n",
            "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=9.413e-07\n",
            "   (parity_k=0, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=7.423e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=3.855e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=6.437e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=7.451e-07\n",
            "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=forces              )     -> max error=2.282e-06\n",
            "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=4.721e-07\n",
            "   (parity_k=1, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=7.423e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.839e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=4.590e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=5.066e-07\n",
            "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=forces              )     -> max error=1.730e-06\n",
            "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=9.385e-07\n",
            "   (parity_k=1, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=8.649e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=2.132e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=4.590e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=forces              )     -> max error=1.300e-06\n",
            "Number of weights: 37352\n",
            "Number of trainable weights: 37352\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2         1.09        0.836        0.251         0.54        0.835         26.2        0.414\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.410    0.002         0.82        0.212         1.03        0.539        0.828         22.9         0.36\n",
            "Wall time: 4.4110590350001075\n",
            "! Best model        0    1.032\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.383        0.265        0.118         0.37        0.471         20.1        0.314\n",
            "      1    20        0.275        0.202       0.0727        0.329        0.411         15.5        0.246\n",
            "      1    30        0.337        0.274       0.0631        0.381        0.478         14.7        0.229\n",
            "      1    40        0.189         0.11       0.0798        0.239        0.302         16.3        0.258\n",
            "      1    50        0.496        0.419       0.0774        0.481        0.591           16        0.254\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2        0.366        0.274        0.092        0.327        0.478         17.4        0.274\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   16.587    0.002        0.429        0.133        0.561        0.399        0.605         18.7        0.297\n",
            "! Validation          1   16.587    0.002        0.263       0.0874        0.351        0.314        0.469         16.7        0.262\n",
            "Wall time: 16.587881341000184\n",
            "! Best model        1    0.351\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10       0.0777       0.0703      0.00735        0.194        0.242         4.94       0.0784\n",
            "      2    20        0.759        0.522        0.237         0.47         0.66         28.5        0.445\n",
            "      2    30        0.365        0.322       0.0425         0.41        0.519         11.9        0.188\n",
            "      2    40        0.818        0.621        0.197        0.544         0.72         25.9        0.405\n",
            "      2    50        0.331        0.145        0.186        0.274        0.348         25.2        0.394\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2        0.276        0.213       0.0625        0.289        0.422           14         0.22\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   17.911    0.002        0.223       0.0815        0.305        0.292        0.436         14.8        0.234\n",
            "! Validation          2   17.911    0.002          0.2       0.0632        0.264        0.276        0.409         14.2        0.223\n",
            "Wall time: 17.912243662000037\n",
            "! Best model        2    0.264\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10       0.0842       0.0627       0.0215        0.183        0.229         8.43        0.134\n",
            "      3    20        0.159       0.0583          0.1        0.174        0.221         18.2        0.289\n",
            "      3    30         0.19        0.187      0.00277        0.312        0.396         3.08       0.0481\n",
            "      3    40       0.0748       0.0525       0.0224        0.167        0.209         8.61        0.137\n",
            "      3    50        0.319        0.221       0.0988        0.248        0.429         18.4        0.287\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2        0.234        0.168        0.065        0.261        0.375         14.4        0.226\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   19.215    0.002        0.179       0.0707         0.25        0.268        0.391         13.3        0.212\n",
            "! Validation          3   19.215    0.002        0.158       0.0613        0.219        0.249        0.363         13.7        0.216\n",
            "Wall time: 19.215972834000013\n",
            "! Best model        3    0.219\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0661       0.0512       0.0149        0.164        0.207         7.03        0.112\n",
            "      4    20         0.37        0.212        0.158        0.328        0.421         22.9        0.364\n",
            "      4    30        0.253         0.25      0.00283        0.353        0.457         3.06       0.0486\n",
            "      4    40        0.227        0.168       0.0593        0.298        0.375         14.2        0.223\n",
            "      4    50        0.125        0.119      0.00565        0.245        0.315          4.4       0.0687\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2         0.19        0.133       0.0565        0.235        0.334         13.4        0.211\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   20.529    0.002        0.145       0.0494        0.194        0.245        0.351         10.8        0.173\n",
            "! Validation          4   20.529    0.002        0.131       0.0537        0.184        0.227         0.33         12.9        0.202\n",
            "Wall time: 20.530231636000053\n",
            "! Best model        4    0.184\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0644       0.0466       0.0178        0.158        0.197         7.69        0.122\n",
            "      5    20       0.0228       0.0115       0.0114       0.0373       0.0979         1.17       0.0975\n",
            "      5    30        0.258        0.129         0.13        0.261        0.328         20.7        0.329\n",
            "      5    40        0.233        0.168       0.0653        0.298        0.374         14.7        0.234\n",
            "      5    50        0.171     6.53e-11        0.171     4.53e-06     7.38e-06         24.2        0.377\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2        0.159        0.123       0.0361        0.225        0.321         10.7        0.169\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   21.817    0.002        0.126       0.0426        0.169        0.232        0.328         9.94        0.158\n",
            "! Validation          5   21.817    0.002        0.119       0.0359        0.155        0.218        0.315         10.7        0.167\n",
            "Wall time: 21.818174574000068\n",
            "! Best model        5    0.155\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0373       0.0312      0.00611        0.133        0.161         4.57       0.0714\n",
            "      6    20        0.151         0.11       0.0404        0.245        0.304         11.8        0.184\n",
            "      6    30        0.297        0.197          0.1        0.333        0.405         18.5        0.289\n",
            "      6    40       0.0658       0.0532       0.0126        0.172        0.211         6.47        0.103\n",
            "      6    50       0.0921       0.0771        0.015        0.147        0.254         7.15        0.112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2         0.15        0.119       0.0314        0.221        0.315         9.89        0.156\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   23.111    0.002        0.114       0.0305        0.145        0.222        0.311         8.88         0.14\n",
            "! Validation          6   23.111    0.002        0.116       0.0308        0.147        0.215        0.312         9.59         0.15\n",
            "Wall time: 23.112080949000074\n",
            "! Best model        6    0.147\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0119      0.00907      0.00287       0.0368        0.087        0.587       0.0489\n",
            "      7    20         0.23        0.188       0.0414        0.325        0.396         11.9        0.186\n",
            "      7    30       0.0414         0.04      0.00138        0.144        0.183         2.14       0.0339\n",
            "      7    40        0.263        0.244       0.0186        0.361        0.451         7.98        0.125\n",
            "      7    50        0.113        0.082       0.0309        0.211        0.262         10.3        0.161\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2        0.151         0.13       0.0213        0.229        0.329         7.99        0.126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   24.622    0.002        0.119       0.0197        0.139        0.225        0.318         6.53        0.105\n",
            "! Validation          7   24.622    0.002        0.123       0.0213        0.144        0.219         0.32          7.8        0.122\n",
            "Wall time: 24.62356043700015\n",
            "! Best model        7    0.144\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10        0.169        0.144       0.0241        0.275        0.347         8.94        0.142\n",
            "      8    20       0.0409       0.0374      0.00345        0.142        0.177         3.38       0.0537\n",
            "      8    30        0.259        0.187       0.0717        0.311        0.396         15.7        0.245\n",
            "      8    40       0.0587       0.0516      0.00712        0.162        0.207         4.86       0.0771\n",
            "      8    50       0.0501       0.0433      0.00673         0.15         0.19         4.72        0.075\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2        0.142         0.12       0.0222        0.221        0.317         8.14        0.128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   26.383    0.002        0.109       0.0229        0.132        0.218        0.304          7.3        0.116\n",
            "! Validation          8   26.383    0.002        0.116       0.0222        0.138        0.214        0.311         7.93        0.124\n",
            "Wall time: 26.385614086000032\n",
            "! Best model        8    0.138\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10        0.152          0.1       0.0517        0.167        0.289         13.3        0.208\n",
            "      9    20        0.138        0.113       0.0259        0.244        0.307         9.26        0.147\n",
            "      9    30        0.121        0.115       0.0062         0.25        0.309         4.61        0.072\n",
            "      9    40      0.00977      0.00757      0.00221       0.0328       0.0795        0.515       0.0429\n",
            "      9    50       0.0504       0.0304         0.02        0.124        0.159         8.26        0.129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2        0.136        0.117       0.0192        0.218        0.312         7.56        0.119\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   28.320    0.002        0.104        0.022        0.126        0.214        0.297         7.31        0.116\n",
            "! Validation          9   28.320    0.002        0.113       0.0197        0.133        0.211        0.307          7.5        0.118\n",
            "Wall time: 28.321278563000078\n",
            "! Best model        9    0.133\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0417       0.0408     0.000837        0.146        0.185         1.67       0.0264\n",
            "     10    20       0.0658       0.0338       0.0321        0.134        0.168         10.3        0.164\n",
            "     10    30        0.159        0.159     0.000403        0.296        0.364         1.17       0.0183\n",
            "     10    40        0.211        0.168       0.0431          0.3        0.374         12.1         0.19\n",
            "     10    50        0.138        0.137     0.000344        0.266        0.338         1.07       0.0169\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2        0.133        0.115       0.0181        0.217         0.31         7.22        0.114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   30.259    0.002        0.104       0.0218        0.126        0.214        0.298         6.58        0.107\n",
            "! Validation         10   30.259    0.002        0.111       0.0186         0.13        0.209        0.305         7.13        0.112\n",
            "Wall time: 30.261285066000028\n",
            "! Best model       10    0.130\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10        0.113        0.101       0.0113        0.232        0.291          6.2       0.0969\n",
            "     11    20        0.274        0.163         0.11         0.29        0.369         19.4        0.303\n",
            "     11    30        0.132        0.132     3.01e-05        0.258        0.332        0.316      0.00501\n",
            "     11    40        0.278        0.197       0.0813        0.318        0.406         16.7         0.26\n",
            "     11    50        0.313        0.218       0.0944        0.336        0.427           18        0.281\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2        0.128        0.111       0.0177        0.214        0.304         7.13        0.112\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   31.725    0.002       0.0986       0.0216         0.12        0.209         0.29         6.79        0.108\n",
            "! Validation         11   31.725    0.002        0.108       0.0184        0.126        0.207          0.3         7.09        0.111\n",
            "Wall time: 31.72636620900016\n",
            "! Best model       11    0.126\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10       0.0613       0.0605     0.000861        0.179        0.225         1.69       0.0268\n",
            "     12    20       0.0675       0.0266       0.0409         0.12        0.149         11.8        0.185\n",
            "     12    30         0.23        0.178       0.0526        0.312        0.385         13.4         0.21\n",
            "     12    40       0.0315        0.031     0.000483        0.125        0.161         1.26       0.0201\n",
            "     12    50        0.063       0.0334       0.0296        0.133        0.167         10.1        0.157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2        0.126        0.111       0.0146        0.214        0.305         6.36          0.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   33.021    0.002       0.0971       0.0162        0.113        0.206        0.287         6.14       0.0987\n",
            "! Validation         12   33.021    0.002        0.107       0.0156        0.123        0.206        0.299         6.43        0.101\n",
            "Wall time: 33.021972776999974\n",
            "! Best model       12    0.123\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10        0.176        0.123       0.0528        0.255         0.32         13.2         0.21\n",
            "     13    20       0.0518       0.0418      0.00999        0.145        0.187         5.75       0.0913\n",
            "     13    30        0.161         0.15       0.0109        0.281        0.354         6.01       0.0953\n",
            "     13    40         0.15        0.148      0.00159        0.274        0.352         2.29       0.0364\n",
            "     13    50        0.234        0.207       0.0277        0.325        0.415         9.73        0.152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2        0.125        0.112       0.0133        0.215        0.305         5.94       0.0934\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   34.307    0.002        0.107       0.0153        0.122        0.215        0.301         5.83       0.0943\n",
            "! Validation         13   34.307    0.002        0.107       0.0144        0.121        0.205        0.299         6.02       0.0943\n",
            "Wall time: 34.30741511700012\n",
            "! Best model       13    0.121\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10          0.2        0.185       0.0154        0.316        0.393         7.13        0.113\n",
            "     14    20        0.129        0.112       0.0173        0.245        0.306         7.68         0.12\n",
            "     14    30       0.0857       0.0817        0.004          0.2        0.261          3.7       0.0578\n",
            "     14    40        0.202        0.178       0.0239        0.304        0.385         8.89        0.141\n",
            "     14    50        0.101       0.0952      0.00591        0.225        0.282          4.5       0.0703\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2         0.12        0.107        0.013        0.211        0.299         5.83       0.0917\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   35.596    0.002       0.0974       0.0203        0.118        0.207        0.287          6.8        0.108\n",
            "! Validation         14   35.596    0.002        0.104       0.0142        0.118        0.202        0.295         5.97       0.0935\n",
            "Wall time: 35.59654124200006\n",
            "! Best model       14    0.118\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.104       0.0825       0.0211        0.202        0.262          8.5        0.133\n",
            "     15    20       0.0774       0.0457       0.0317        0.154        0.195         10.3        0.163\n",
            "     15    30        0.163        0.155      0.00743        0.284         0.36         4.96       0.0788\n",
            "     15    40        0.121        0.117      0.00412        0.246        0.312         3.75       0.0586\n",
            "     15    50        0.143        0.138      0.00515        0.282        0.339         4.13       0.0656\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2        0.119        0.105       0.0134        0.209        0.296         5.84       0.0918\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   36.892    0.002       0.0905       0.0191         0.11        0.202        0.277         6.32        0.102\n",
            "! Validation         15   36.892    0.002        0.103       0.0147        0.117        0.201        0.293         5.92       0.0928\n",
            "Wall time: 36.89333569700011\n",
            "! Best model       15    0.117\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10        0.116       0.0722       0.0436        0.142        0.246         12.2        0.191\n",
            "     16    20       0.0469       0.0467     0.000247        0.114        0.197        0.918       0.0143\n",
            "     16    30       0.0429       0.0328       0.0101        0.129        0.165         5.79       0.0919\n",
            "     16    40        0.151        0.147      0.00412        0.276         0.35          3.7       0.0587\n",
            "     16    50        0.216        0.183       0.0326        0.318        0.391         10.6        0.165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2        0.117        0.105       0.0119        0.209        0.296          5.4       0.0848\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   38.194    0.002       0.0927       0.0124        0.105        0.202         0.28         5.23       0.0847\n",
            "! Validation         16   38.194    0.002        0.102       0.0134        0.115        0.201        0.292         5.63       0.0882\n",
            "Wall time: 38.19511750300012\n",
            "! Best model       16    0.115\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10        0.129        0.128      0.00129        0.264        0.327         2.06       0.0328\n",
            "     17    20       0.0435       0.0412      0.00228        0.146        0.185         2.75       0.0437\n",
            "     17    30       0.0346       0.0328      0.00183        0.132        0.165         2.46        0.039\n",
            "     17    40        0.239         0.19       0.0489        0.317        0.398         12.9        0.202\n",
            "     17    50        0.152        0.124       0.0278        0.256        0.322         9.76        0.152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2        0.113        0.101        0.012        0.205        0.291         5.43       0.0854\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   39.491    0.002       0.0934       0.0177        0.111        0.202        0.282         6.49        0.104\n",
            "! Validation         17   39.491    0.002       0.0992       0.0136        0.113        0.198        0.288          5.7       0.0894\n",
            "Wall time: 39.49201717699998\n",
            "! Best model       17    0.113\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10        0.121         0.12     8.69e-05        0.254        0.317        0.545      0.00852\n",
            "     18    20        0.015        0.014      0.00105       0.0428        0.108        0.355       0.0296\n",
            "     18    30       0.0541       0.0367       0.0174        0.143        0.175         7.59        0.121\n",
            "     18    40        0.193        0.137       0.0563        0.269        0.338         13.9        0.217\n",
            "     18    50       0.0968       0.0731       0.0238        0.197        0.247         9.02        0.141\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2        0.112        0.101       0.0117        0.205         0.29         5.28        0.083\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   40.812    0.002       0.0893       0.0163        0.106        0.198        0.275         6.11       0.0973\n",
            "! Validation         18   40.812    0.002       0.0985       0.0134        0.112        0.197        0.287         5.63       0.0882\n",
            "Wall time: 40.81330694799999\n",
            "! Best model       18    0.112\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10        0.122       0.0938       0.0281        0.222         0.28          9.8        0.153\n",
            "     19    20        0.157        0.115       0.0414         0.25         0.31         11.9        0.186\n",
            "     19    30        0.103        0.103     0.000308        0.169        0.293         1.03        0.016\n",
            "     19    40        0.157         0.14       0.0163        0.275        0.342         7.35        0.117\n",
            "     19    50       0.0303       0.0301     0.000151        0.129        0.159        0.719       0.0112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2         0.11       0.0986       0.0117        0.202        0.287         5.22       0.0819\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   42.555    0.002       0.0869       0.0134          0.1        0.196        0.271         5.55       0.0899\n",
            "! Validation         19   42.555    0.002       0.0966       0.0136         0.11        0.195        0.284         5.64       0.0884\n",
            "Wall time: 42.556312154000125\n",
            "! Best model       19    0.110\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.142        0.132         0.01        0.266        0.332         5.86       0.0916\n",
            "     20    20         0.14        0.138      0.00125        0.278         0.34         2.03       0.0323\n",
            "     20    30       0.0462       0.0353       0.0109        0.134        0.172         6.01       0.0954\n",
            "     20    40       0.0867       0.0866     0.000157        0.209        0.269        0.732       0.0114\n",
            "     20    50        0.197        0.152       0.0441        0.284        0.357         12.3        0.192\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2        0.109       0.0985       0.0106        0.202        0.287         4.84        0.076\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   44.432    0.002        0.089       0.0159        0.105        0.198        0.274         5.62       0.0908\n",
            "! Validation         20   44.432    0.002       0.0963       0.0125        0.109        0.194        0.284         5.39       0.0845\n",
            "Wall time: 44.43392191900011\n",
            "! Best model       20    0.109\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10      0.00958      0.00958     5.25e-06       0.0516       0.0894        0.134      0.00209\n",
            "     21    20        0.171        0.162      0.00896        0.293        0.368         5.53       0.0865\n",
            "     21    30        0.202        0.131       0.0711        0.269        0.331         15.6        0.244\n",
            "     21    40      0.00939     5.37e-11      0.00939     4.17e-06      6.7e-06         5.67       0.0886\n",
            "     21    50        0.133        0.125       0.0083        0.257        0.323         5.24       0.0832\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2        0.108       0.0962       0.0114          0.2        0.283         5.03       0.0791\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   46.366    0.002       0.0819       0.0165       0.0984        0.191        0.263         5.65       0.0891\n",
            "! Validation         21   46.366    0.002       0.0947       0.0134        0.108        0.193        0.281          5.6       0.0877\n",
            "Wall time: 46.36777843300001\n",
            "! Best model       21    0.108\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10        0.189        0.166       0.0231        0.215        0.373         8.88        0.139\n",
            "     22    20        0.163         0.15       0.0132        0.286        0.354          6.6        0.105\n",
            "     22    30       0.0829       0.0775       0.0054        0.203        0.254          4.3       0.0672\n",
            "     22    40       0.0587       0.0559      0.00279        0.172        0.216         3.04       0.0483\n",
            "     22    50        0.241        0.168       0.0727        0.296        0.374         15.8        0.246\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2        0.106       0.0952       0.0109        0.199        0.282         4.82       0.0756\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   48.065    0.002       0.0876       0.0149        0.102        0.195        0.272         5.73       0.0924\n",
            "! Validation         22   48.065    0.002       0.0936        0.013        0.107        0.192         0.28         5.48       0.0858\n",
            "Wall time: 48.06626284700019\n",
            "! Best model       22    0.107\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10       0.0955       0.0943      0.00129        0.225        0.281          2.1       0.0328\n",
            "     23    20        0.161         0.15       0.0117        0.278        0.354         6.24        0.099\n",
            "     23    30        0.155        0.136       0.0192        0.278        0.336         8.09        0.126\n",
            "     23    40        0.164         0.12       0.0437        0.253        0.317           12        0.191\n",
            "     23    50       0.0764       0.0434        0.033        0.149         0.19         10.5        0.166\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2        0.104       0.0933       0.0104        0.197        0.279         4.66       0.0732\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   49.370    0.002       0.0839       0.0163          0.1        0.192        0.266         5.87        0.095\n",
            "! Validation         23   49.370    0.002       0.0922       0.0126        0.105         0.19        0.277          5.4       0.0846\n",
            "Wall time: 49.37044075300014\n",
            "! Best model       23    0.105\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10         0.13        0.121      0.00897        0.253        0.318         5.45       0.0866\n",
            "     24    20       0.0491       0.0207       0.0284        0.106        0.131         9.86        0.154\n",
            "     24    30       0.0763       0.0742      0.00213        0.201        0.249          2.7       0.0422\n",
            "     24    40        0.198        0.129        0.069        0.262        0.328         15.1         0.24\n",
            "     24    50       0.0745       0.0357       0.0388       0.0997        0.173         11.5         0.18\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2        0.103       0.0928      0.00994        0.196        0.278         4.42       0.0694\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   50.668    0.002       0.0816       0.0148       0.0964        0.188        0.262         5.45       0.0877\n",
            "! Validation         24   50.668    0.002       0.0913       0.0122        0.103        0.189        0.276         5.26       0.0823\n",
            "Wall time: 50.66854643900001\n",
            "! Best model       24    0.103\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10        0.203        0.143       0.0597        0.278        0.345         14.3        0.223\n",
            "     25    20         0.11         0.11     5.47e-05        0.251        0.303        0.426      0.00676\n",
            "     25    30       0.0319         0.03      0.00187        0.123        0.158         2.49       0.0395\n",
            "     25    40        0.192        0.166       0.0258        0.298        0.373          9.4        0.147\n",
            "     25    50      0.00611     5.84e-11      0.00611     4.66e-06     6.98e-06         4.57       0.0714\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2        0.102       0.0921      0.00958        0.195        0.277          4.2       0.0659\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   51.952    0.002        0.082       0.0134       0.0954         0.19        0.263         5.19       0.0842\n",
            "! Validation         25   51.952    0.002       0.0904        0.012        0.102        0.188        0.275         5.14       0.0805\n",
            "Wall time: 51.95260084200004\n",
            "! Best model       25    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10        0.118        0.106       0.0122        0.239        0.297         6.35        0.101\n",
            "     26    20       0.0522        0.022       0.0303        0.109        0.135         10.2        0.159\n",
            "     26    30        0.102       0.0896       0.0124        0.158        0.273         6.51        0.102\n",
            "     26    40        0.125        0.115      0.00975        0.251         0.31         5.78       0.0902\n",
            "     26    50        0.134        0.127      0.00698        0.256        0.326         4.81       0.0763\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2          0.1       0.0899       0.0102        0.193        0.274         4.41       0.0691\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   53.227    0.002       0.0802       0.0237        0.104        0.189         0.26         6.82        0.108\n",
            "! Validation         26   53.227    0.002       0.0889       0.0126        0.102        0.186        0.272         5.32       0.0833\n",
            "Wall time: 53.22782578700003\n",
            "! Best model       26    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10        0.229        0.227      0.00192        0.342        0.435         2.56       0.0401\n",
            "     27    20         0.14        0.108       0.0323        0.241          0.3         10.3        0.164\n",
            "     27    30        0.127       0.0906       0.0362         0.22        0.275         11.1        0.174\n",
            "     27    40       0.0362        0.036     0.000178        0.134        0.173        0.768       0.0122\n",
            "     27    50       0.0747       0.0747     3.01e-09        0.202         0.25       0.0032     5.01e-05\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2       0.0995         0.09      0.00952        0.192        0.274         4.18       0.0655\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   54.511    0.002       0.0837       0.0207        0.104        0.189        0.266         6.39        0.105\n",
            "! Validation         27   54.511    0.002       0.0883       0.0119          0.1        0.185        0.271         5.15       0.0807\n",
            "Wall time: 54.51344873300013\n",
            "! Best model       27    0.100\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10        0.067       0.0226       0.0445        0.108        0.137         12.3        0.193\n",
            "     28    20       0.0737       0.0712      0.00243        0.195        0.244         2.88       0.0451\n",
            "     28    30        0.212        0.164       0.0479        0.292         0.37         12.8          0.2\n",
            "     28    40        0.239        0.161       0.0777        0.297        0.367         16.3        0.255\n",
            "     28    50        0.119        0.117      0.00225        0.252        0.313         2.73       0.0434\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2       0.0983       0.0893      0.00901        0.192        0.273         4.01       0.0629\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   55.826    0.002       0.0793       0.0209          0.1        0.186        0.259         6.48        0.106\n",
            "! Validation         28   55.826    0.002       0.0875       0.0113       0.0988        0.184         0.27         5.03       0.0787\n",
            "Wall time: 55.827243898999996\n",
            "! Best model       28    0.099\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10       0.0135     5.86e-11       0.0135     4.64e-06     6.99e-06         6.79        0.106\n",
            "     29    20        0.161        0.127       0.0342         0.26        0.325         10.8        0.169\n",
            "     29    30        0.134        0.134     0.000124        0.264        0.334        0.641       0.0102\n",
            "     29    40        0.103       0.0694       0.0337        0.186        0.241         10.7        0.168\n",
            "     29    50       0.0551        0.051      0.00409        0.119        0.206         3.74       0.0584\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2       0.0981       0.0893      0.00875        0.192        0.273         3.85       0.0604\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   57.110    0.002       0.0805       0.0163       0.0968        0.186        0.261         5.84       0.0948\n",
            "! Validation         29   57.110    0.002       0.0871       0.0111       0.0982        0.184         0.27         4.91       0.0768\n",
            "Wall time: 57.11108945500018\n",
            "! Best model       29    0.098\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10        0.108       0.0893       0.0186         0.22        0.273         7.97        0.125\n",
            "     30    20        0.024       0.0194      0.00461        0.102        0.127         3.97        0.062\n",
            "     30    30       0.0447       0.0139       0.0308       0.0867        0.108         10.3         0.16\n",
            "     30    40        0.102        0.101     0.000203        0.235        0.291        0.833        0.013\n",
            "     30    50       0.0363        0.033      0.00327        0.131        0.166         3.29       0.0523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2       0.0965       0.0875      0.00906         0.19         0.27         4.06       0.0636\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   58.721    0.002       0.0738       0.0174       0.0913         0.18        0.249         6.18       0.0992\n",
            "! Validation         30   58.721    0.002       0.0856       0.0114        0.097        0.182        0.267         5.06       0.0792\n",
            "Wall time: 58.72161733899998\n",
            "! Best model       30    0.097\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10       0.0199       0.0162      0.00367       0.0944        0.116         3.54       0.0554\n",
            "     31    20       0.0548       0.0183       0.0364       0.0472        0.124         2.09        0.174\n",
            "     31    30         0.16        0.119       0.0407        0.254        0.316         11.8        0.184\n",
            "     31    40        0.108        0.099       0.0088        0.221        0.288          5.4       0.0857\n",
            "     31    50      0.00543     5.88e-11      0.00543      4.7e-06     7.01e-06         4.31       0.0674\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2       0.0945       0.0857      0.00881        0.188        0.268         4.01        0.063\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   60.528    0.002       0.0745       0.0129       0.0874         0.18        0.251         4.89       0.0811\n",
            "! Validation         31   60.528    0.002       0.0841       0.0111       0.0952        0.181        0.265         5.02       0.0786\n",
            "Wall time: 60.528525446\n",
            "! Best model       31    0.095\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10        0.124        0.106       0.0171        0.172        0.298         7.64        0.119\n",
            "     32    20       0.0097     5.46e-11       0.0097     4.47e-06     6.75e-06         5.76         0.09\n",
            "     32    30        0.125        0.109       0.0153        0.246        0.302         7.12        0.113\n",
            "     32    40         0.07       0.0674      0.00262        0.193        0.237         2.99       0.0468\n",
            "     32    50       0.0196       0.0173       0.0023       0.0946         0.12          2.8       0.0438\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2       0.0928       0.0837      0.00903        0.186        0.264         4.09       0.0641\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   62.464    0.002       0.0725       0.0121       0.0847        0.178        0.247         5.16       0.0834\n",
            "! Validation         32   62.464    0.002       0.0825       0.0114       0.0939        0.179        0.262          5.1       0.0798\n",
            "Wall time: 62.464894721000064\n",
            "! Best model       32    0.094\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10        0.172        0.154       0.0184        0.207        0.358         7.94        0.124\n",
            "     33    20       0.0416       0.0408     0.000748        0.144        0.185         1.57        0.025\n",
            "     33    30        0.124        0.114      0.00973        0.242        0.309         5.68       0.0901\n",
            "     33    40        0.209        0.149       0.0604        0.275        0.353         14.4        0.225\n",
            "     33    50       0.0334        0.033     0.000404        0.131        0.166         1.16       0.0184\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2       0.0919        0.083      0.00894        0.185        0.263            4       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   64.362    0.002       0.0781       0.0165       0.0947        0.182        0.256         5.63       0.0927\n",
            "! Validation         33   64.362    0.002       0.0818       0.0113       0.0931        0.179        0.261         5.04       0.0789\n",
            "Wall time: 64.36281347500017\n",
            "! Best model       33    0.093\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10        0.129        0.117       0.0124        0.252        0.312         6.51        0.102\n",
            "     34    20       0.0526       0.0171       0.0355       0.0964         0.12           11        0.172\n",
            "     34    30        0.127        0.114       0.0131        0.249        0.308         6.58        0.105\n",
            "     34    40         0.17        0.116       0.0538         0.18        0.312         13.6        0.212\n",
            "     34    50       0.0785       0.0785     6.71e-06        0.148        0.256        0.151      0.00237\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2       0.0897        0.081      0.00867        0.183         0.26            4       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   65.644    0.002       0.0728       0.0175       0.0903        0.177        0.247         6.01        0.095\n",
            "! Validation         34   65.644    0.002         0.08        0.011        0.091        0.177        0.258         5.01       0.0784\n",
            "Wall time: 65.64460954000015\n",
            "! Best model       34    0.091\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10         0.12        0.101       0.0184        0.232        0.291          7.8        0.124\n",
            "     35    20      0.00786      5.7e-11      0.00786     4.67e-06      6.9e-06         5.19        0.081\n",
            "     35    30       0.0928       0.0602       0.0326        0.179        0.224         10.6        0.165\n",
            "     35    40        0.117        0.108      0.00984        0.243          0.3          5.8       0.0906\n",
            "     35    50     0.000189     7.52e-11     0.000189     5.07e-06     7.92e-06        0.804       0.0126\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2       0.0882       0.0797      0.00852        0.181        0.258         3.97       0.0623\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   66.928    0.002       0.0706       0.0179       0.0884        0.174        0.244         5.94       0.0966\n",
            "! Validation         35   66.928    0.002       0.0787       0.0108       0.0895        0.175        0.256         4.98       0.0779\n",
            "Wall time: 66.92894804399998\n",
            "! Best model       35    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10        0.225        0.195       0.0304        0.308        0.403         10.2        0.159\n",
            "     36    20       0.0361       0.0357     0.000431        0.131        0.173          1.2        0.019\n",
            "     36    30        0.122        0.105       0.0161        0.238        0.297         7.31        0.116\n",
            "     36    40       0.0257       0.0253     0.000448        0.112        0.145         1.22       0.0193\n",
            "     36    50        0.122        0.114       0.0078        0.243        0.309         5.08       0.0807\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2       0.0874       0.0795      0.00796        0.181        0.258         3.72       0.0583\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   68.212    0.002        0.073        0.012       0.0849        0.177        0.248            5       0.0816\n",
            "! Validation         36   68.212    0.002       0.0781       0.0102       0.0883        0.175        0.255         4.79        0.075\n",
            "Wall time: 68.21317980700019\n",
            "! Best model       36    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10       0.0442       0.0437     0.000519        0.151        0.191         1.33       0.0208\n",
            "     37    20       0.0498       0.0381       0.0117        0.138        0.178         6.32       0.0988\n",
            "     37    30       0.0281       0.0273     0.000792        0.117        0.151         1.62       0.0257\n",
            "     37    40       0.0277       0.0249      0.00278        0.111        0.144         3.04       0.0482\n",
            "     37    50       0.0806       0.0599       0.0207         0.18        0.224         8.42        0.132\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2        0.086       0.0786       0.0074         0.18        0.256         3.53       0.0553\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   69.496    0.002       0.0718       0.0108       0.0826        0.174        0.245         4.61        0.074\n",
            "! Validation         37   69.496    0.002       0.0771      0.00955       0.0867        0.174        0.254         4.64       0.0727\n",
            "Wall time: 69.49695629200005\n",
            "! Best model       37    0.087\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10       0.0481       0.0481     3.06e-05        0.156          0.2        0.324      0.00506\n",
            "     38    20        0.119        0.113      0.00519         0.25        0.308         4.21       0.0658\n",
            "     38    30       0.0567        0.045       0.0117        0.112        0.194         6.33       0.0989\n",
            "     38    40        0.115        0.109       0.0057        0.241        0.302         4.35        0.069\n",
            "     38    50       0.0256       0.0254     0.000155         0.11        0.146        0.717       0.0114\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2       0.0837       0.0758      0.00784        0.177        0.252         3.72       0.0584\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   70.790    0.002       0.0661       0.0139         0.08        0.169        0.236         5.31       0.0858\n",
            "! Validation         38   70.790    0.002        0.075       0.0101        0.085        0.172         0.25          4.8       0.0751\n",
            "Wall time: 70.79053929700012\n",
            "! Best model       38    0.085\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10       0.0708       0.0706     0.000185        0.196        0.243        0.795       0.0124\n",
            "     39    20       0.0442       0.0295       0.0147        0.127        0.157         6.97        0.111\n",
            "     39    30        0.193        0.153       0.0392        0.274        0.358         11.6        0.181\n",
            "     39    40       0.0391       0.0304      0.00868        0.122        0.159         5.36       0.0851\n",
            "     39    50         0.12        0.108        0.012        0.241          0.3          6.3          0.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2       0.0819       0.0746      0.00731        0.175         0.25         3.54       0.0554\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   72.087    0.002       0.0677       0.0129       0.0806         0.17        0.238         5.42        0.087\n",
            "! Validation         39   72.087    0.002       0.0738      0.00949       0.0833        0.171        0.248         4.67        0.073\n",
            "Wall time: 72.087811999\n",
            "! Best model       39    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10       0.0194       0.0151      0.00429       0.0896        0.112         3.83       0.0598\n",
            "     40    20       0.0306       0.0255      0.00502        0.111        0.146         4.08       0.0648\n",
            "     40    30       0.0813       0.0813        4e-05        0.203        0.261        0.364      0.00578\n",
            "     40    40        0.103       0.0996       0.0039        0.166        0.288         3.65       0.0571\n",
            "     40    50         0.14        0.123       0.0173        0.263         0.32          7.7         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2       0.0805       0.0731      0.00735        0.174        0.247         3.53       0.0553\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   73.379    0.002       0.0667       0.0131       0.0798        0.169        0.237         4.88       0.0805\n",
            "! Validation         40   73.379    0.002       0.0724      0.00957        0.082        0.169        0.246         4.66       0.0728\n",
            "Wall time: 73.37965994500018\n",
            "! Best model       40    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10       0.0295       0.0291     0.000388        0.124        0.156         1.13        0.018\n",
            "     41    20       0.0824       0.0677       0.0146        0.192        0.238         7.07        0.111\n",
            "     41    30       0.0955       0.0938      0.00163         0.22         0.28         2.33       0.0369\n",
            "     41    40       0.0584       0.0163       0.0421        0.092        0.117           12        0.187\n",
            "     41    50        0.181        0.117       0.0645        0.254        0.312         14.8        0.232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2       0.0799       0.0728      0.00708        0.173        0.247         3.42       0.0536\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   74.819    0.002       0.0672       0.0128         0.08         0.17        0.237          5.1       0.0819\n",
            "! Validation         41   74.819    0.002        0.072      0.00926       0.0812        0.169        0.245         4.56       0.0714\n",
            "Wall time: 74.82038251000017\n",
            "! Best model       41    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10        0.123        0.118      0.00508         0.25        0.314          4.1       0.0651\n",
            "     42    20       0.0931       0.0781        0.015        0.199        0.255         7.04        0.112\n",
            "     42    30        0.028       0.0142       0.0139       0.0859        0.109         6.89        0.108\n",
            "     42    40     3.37e-05     7.42e-11     3.37e-05     5.31e-06     7.87e-06        0.339       0.0053\n",
            "     42    50        0.128        0.121      0.00654        0.184        0.318         4.73       0.0739\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2       0.0779       0.0713      0.00655        0.172        0.244         3.25        0.051\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   76.572    0.002       0.0681      0.00945       0.0776        0.169        0.239         4.69       0.0771\n",
            "! Validation         42   76.572    0.002       0.0704      0.00867       0.0791        0.167        0.242         4.43       0.0693\n",
            "Wall time: 76.57404710500009\n",
            "! Best model       42    0.079\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10        0.114        0.111      0.00356        0.236        0.304         3.44       0.0545\n",
            "     43    20        0.059       0.0565      0.00249        0.176        0.217         2.92       0.0456\n",
            "     43    30       0.0872       0.0871     0.000131        0.212         0.27        0.659       0.0105\n",
            "     43    40        0.193        0.116       0.0773        0.241        0.311         16.3        0.254\n",
            "     43    50        0.142        0.114       0.0282        0.178        0.309         9.82        0.153\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2       0.0768       0.0704      0.00637        0.171        0.243         3.24       0.0507\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   78.574    0.002       0.0683       0.0136       0.0819        0.169         0.24         5.12       0.0832\n",
            "! Validation         43   78.574    0.002       0.0698      0.00841       0.0782        0.166        0.241         4.39       0.0687\n",
            "Wall time: 78.57575307700017\n",
            "! Best model       43    0.078\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10       0.0162       0.0141      0.00211       0.0876        0.109         2.68       0.0419\n",
            "     44    20        0.029       0.0234      0.00553        0.108         0.14         4.28        0.068\n",
            "     44    30       0.0396       0.0389     0.000698        0.139         0.18         1.54       0.0241\n",
            "     44    40        0.127        0.122      0.00487        0.252         0.32         4.08       0.0638\n",
            "     44    50       0.0359       0.0348      0.00112        0.132         0.17         1.93       0.0306\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2        0.077       0.0709      0.00612        0.171        0.243         3.08       0.0482\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   80.501    0.002       0.0658      0.00955       0.0753        0.167        0.235         4.39       0.0723\n",
            "! Validation         44   80.501    0.002       0.0699      0.00817       0.0781        0.167        0.242         4.27       0.0668\n",
            "Wall time: 80.50148832100012\n",
            "! Best model       44    0.078\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10        0.018       0.0109      0.00712       0.0754       0.0954         4.94       0.0771\n",
            "     45    20        0.118         0.11      0.00819        0.175        0.303         5.29       0.0827\n",
            "     45    30       0.0667       0.0648       0.0019        0.187        0.233         2.55       0.0399\n",
            "     45    40       0.0435       0.0325        0.011        0.134        0.165         6.03       0.0957\n",
            "     45    50        0.129        0.123       0.0055        0.249        0.321         4.27       0.0678\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2       0.0763       0.0702      0.00608        0.171        0.242         3.07       0.0481\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   82.031    0.002       0.0657       0.0127       0.0784        0.166        0.234         4.89       0.0797\n",
            "! Validation         45   82.031    0.002       0.0693      0.00815       0.0774        0.166         0.24         4.27       0.0667\n",
            "Wall time: 82.03215943999999\n",
            "! Best model       45    0.077\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10          0.1       0.0995      0.00057        0.232        0.288         1.37       0.0218\n",
            "     46    20       0.0139       0.0138     0.000104       0.0864        0.107        0.596      0.00931\n",
            "     46    30       0.0463       0.0343        0.012         0.13        0.169         6.29       0.0999\n",
            "     46    40       0.0834       0.0694        0.014        0.139        0.241         6.91        0.108\n",
            "     46    50       0.0868       0.0775      0.00932        0.205        0.254         5.65       0.0882\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2       0.0751       0.0694      0.00571         0.17        0.241         2.93       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   83.322    0.002       0.0666       0.0111       0.0777        0.167        0.236         4.45       0.0738\n",
            "! Validation         46   83.322    0.002       0.0686      0.00767       0.0763        0.166        0.239         4.14       0.0648\n",
            "Wall time: 83.32287125100015\n",
            "! Best model       46    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10        0.113        0.095       0.0181        0.224        0.282         7.74        0.123\n",
            "     47    20        0.274        0.182        0.092        0.302         0.39         17.7        0.277\n",
            "     47    30       0.0363       0.0347      0.00159        0.141         0.17         2.29       0.0364\n",
            "     47    40        0.112       0.0954       0.0164        0.226        0.282         7.37        0.117\n",
            "     47    50        0.153        0.128       0.0248        0.189        0.328         9.22        0.144\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2       0.0736       0.0674       0.0062        0.167        0.237         3.29       0.0516\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   84.592    0.002       0.0667       0.0149       0.0816        0.167        0.237         5.86       0.0949\n",
            "! Validation         47   84.592    0.002       0.0675      0.00812       0.0756        0.164        0.237         4.38       0.0685\n",
            "Wall time: 84.59280485199997\n",
            "! Best model       47    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10       0.0901       0.0898      0.00027        0.214        0.274        0.946        0.015\n",
            "     48    20         0.13        0.108       0.0216        0.163        0.301         4.84        0.134\n",
            "     48    30       0.0306       0.0227      0.00786        0.106        0.138          5.1        0.081\n",
            "     48    40       0.0518       0.0517     3.56e-05        0.163        0.208        0.343      0.00545\n",
            "     48    50       0.0824       0.0807      0.00169        0.206         0.26          2.4       0.0376\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2       0.0738       0.0681      0.00572        0.168        0.238         3.11       0.0487\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   85.916    0.002       0.0646       0.0114        0.076        0.165        0.233         5.06       0.0826\n",
            "! Validation         48   85.916    0.002        0.068      0.00752       0.0755        0.165        0.238         4.19       0.0656\n",
            "Wall time: 85.9168381290001\n",
            "! Best model       48    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10       0.0305        0.011       0.0195       0.0756       0.0957         8.16        0.128\n",
            "     49    20        0.123        0.119      0.00417        0.249        0.315         3.72        0.059\n",
            "     49    30        0.157        0.107       0.0499        0.238        0.299         13.1        0.204\n",
            "     49    40        0.135        0.124       0.0109        0.186        0.321         6.11       0.0955\n",
            "     49    50       0.0662       0.0564      0.00974        0.125        0.217         5.77       0.0902\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2       0.0732       0.0677      0.00548        0.167        0.238         3.01       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   87.200    0.002       0.0656       0.0137       0.0793        0.165        0.234         5.57       0.0906\n",
            "! Validation         49   87.200    0.002       0.0673      0.00727       0.0746        0.164        0.237          4.1       0.0641\n",
            "Wall time: 87.20047465800008\n",
            "! Best model       49    0.075\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10        0.117        0.117     6.55e-05        0.249        0.313        0.466       0.0074\n",
            "     50    20       0.0366       0.0329      0.00365        0.134        0.166         3.48       0.0552\n",
            "     50    30        0.103       0.0984      0.00427        0.231        0.287         3.76       0.0597\n",
            "     50    40       0.0159       0.0115      0.00442       0.0779        0.098         3.89       0.0608\n",
            "     50    50       0.0329       0.0225       0.0103        0.106        0.137         5.85       0.0929\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2       0.0737       0.0684      0.00526        0.168        0.239         2.82       0.0442\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   88.491    0.002        0.066       0.0095       0.0755        0.166        0.235         4.23       0.0698\n",
            "! Validation         50   88.491    0.002       0.0677      0.00707       0.0748        0.165        0.238         3.98       0.0622\n",
            "Wall time: 88.49148783500004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10        0.078       0.0747      0.00336        0.192         0.25         3.34        0.053\n",
            "     51    20        0.138          0.1       0.0373        0.229        0.289         11.3        0.177\n",
            "     51    30       0.0934       0.0885      0.00492        0.217        0.272         4.04       0.0641\n",
            "     51    40       0.0314       0.0309     0.000458         0.12        0.161         1.23       0.0196\n",
            "     51    50       0.0121       0.0112      0.00089       0.0763       0.0968         1.74       0.0273\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2       0.0725       0.0673      0.00513        0.167        0.237         2.82       0.0443\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   89.767    0.002       0.0648       0.0139       0.0787        0.165        0.233         5.47       0.0887\n",
            "! Validation         51   89.767    0.002        0.067       0.0069       0.0739        0.164        0.237         3.97       0.0622\n",
            "Wall time: 89.76769117399999\n",
            "! Best model       51    0.074\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10        0.145        0.125       0.0206        0.247        0.323         8.26        0.131\n",
            "     52    20        0.103       0.0733       0.0296        0.197        0.247          9.9        0.157\n",
            "     52    30         0.13       0.0958       0.0345        0.225        0.283         10.9         0.17\n",
            "     52    40         0.22        0.151       0.0687        0.287        0.355         15.3         0.24\n",
            "     52    50        0.093        0.077        0.016        0.199        0.254         7.28        0.116\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2       0.0715       0.0655        0.006        0.165        0.234         3.36       0.0526\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   91.048    0.002       0.0644       0.0234       0.0878        0.165        0.233          7.2        0.119\n",
            "! Validation         52   91.048    0.002       0.0658      0.00782       0.0736        0.162        0.234         4.36       0.0682\n",
            "Wall time: 91.04904389300009\n",
            "! Best model       52    0.074\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10       0.0486        0.017       0.0316        0.047        0.119         1.95        0.162\n",
            "     53    20       0.0568       0.0542      0.00262        0.171        0.213         2.95       0.0468\n",
            "     53    30       0.0892       0.0856      0.00359        0.211        0.267         3.45       0.0547\n",
            "     53    40        0.218        0.161       0.0574        0.296        0.366           14        0.219\n",
            "     53    50        0.119        0.102       0.0172         0.23        0.292         7.68         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2       0.0715       0.0657      0.00585        0.164        0.234         3.31       0.0519\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   92.733    0.002       0.0652       0.0142       0.0794        0.165        0.234         5.69       0.0924\n",
            "! Validation         53   92.733    0.002       0.0661      0.00757       0.0737        0.162        0.235         4.29       0.0671\n",
            "Wall time: 92.73529282599998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10       0.0907       0.0907     1.29e-05         0.22        0.275         0.21      0.00328\n",
            "     54    20       0.0939       0.0829        0.011        0.152        0.263         6.14       0.0959\n",
            "     54    30       0.0283       0.0276     0.000695        0.108        0.152         1.52       0.0241\n",
            "     54    40        0.141        0.109       0.0321        0.239        0.301         10.5        0.164\n",
            "     54    50       0.0512       0.0317       0.0195        0.122        0.163         8.04        0.128\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2        0.072       0.0665       0.0055        0.165        0.236         3.12       0.0489\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   94.609    0.002       0.0641       0.0105       0.0747        0.163        0.232         4.68       0.0768\n",
            "! Validation         54   94.609    0.002       0.0665       0.0072       0.0737        0.163        0.236         4.14       0.0648\n",
            "Wall time: 94.61142167700018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10        0.132       0.0855        0.047        0.215        0.267         12.5        0.198\n",
            "     55    20        0.118       0.0931       0.0249        0.226        0.279         9.09        0.144\n",
            "     55    30        0.129         0.12      0.00872        0.174        0.317         3.07       0.0854\n",
            "     55    40        0.167        0.152       0.0154        0.271        0.356         7.25        0.113\n",
            "     55    50       0.0136       0.0113      0.00232        0.077       0.0971         2.81        0.044\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2       0.0711       0.0654      0.00571        0.164        0.234         3.23       0.0506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   96.576    0.002       0.0622       0.0121       0.0743        0.162        0.228          5.2       0.0845\n",
            "! Validation         55   96.576    0.002       0.0656       0.0075       0.0731        0.162        0.234         4.25       0.0665\n",
            "Wall time: 96.57761163999999\n",
            "! Best model       55    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10         0.11        0.103      0.00684        0.233        0.293         4.76       0.0756\n",
            "     56    20       0.0327       0.0306      0.00205         0.12         0.16         2.61       0.0414\n",
            "     56    30        0.144        0.136      0.00803        0.186        0.337         2.95       0.0819\n",
            "     56    40       0.0279       0.0273     0.000596        0.115        0.151         1.41       0.0223\n",
            "     56    50       0.0294       0.0294     4.34e-05        0.116        0.157        0.379      0.00602\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2       0.0707       0.0651      0.00562        0.164        0.233         3.15       0.0493\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   98.412    0.002       0.0638       0.0112        0.075        0.164        0.231         4.78       0.0788\n",
            "! Validation         56   98.412    0.002       0.0652      0.00745       0.0727        0.162        0.233         4.21       0.0658\n",
            "Wall time: 98.41263649400003\n",
            "! Best model       56    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10        0.131        0.103       0.0282         0.24        0.293         9.83        0.154\n",
            "     57    20       0.0348       0.0261      0.00877         0.11        0.148         5.39       0.0856\n",
            "     57    30       0.0315       0.0312     0.000291        0.131        0.161        0.982       0.0156\n",
            "     57    40       0.0335       0.0326     0.000985        0.122        0.165         1.81       0.0287\n",
            "     57    50        0.198        0.147       0.0514         0.28         0.35         13.3        0.207\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2       0.0705       0.0649      0.00566        0.164        0.233         3.15       0.0495\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   99.721    0.002       0.0647      0.00963       0.0744        0.165        0.233         4.53        0.074\n",
            "! Validation         57   99.721    0.002       0.0652      0.00749       0.0727        0.161        0.233         4.21       0.0659\n",
            "Wall time: 99.722359826\n",
            "! Best model       57    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10       0.0459       0.0341       0.0119         0.13        0.169         6.28       0.0996\n",
            "     58    20        0.119        0.115      0.00384        0.247         0.31         3.57       0.0566\n",
            "     58    30       0.0944       0.0894        0.005        0.222        0.273         4.07       0.0646\n",
            "     58    40       0.0955       0.0876      0.00787         0.21        0.271         5.11       0.0811\n",
            "     58    50        0.138       0.0892       0.0491        0.219        0.273         12.8        0.202\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2       0.0703       0.0647       0.0056        0.163        0.233         3.08       0.0483\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58  101.025    0.002       0.0638       0.0114       0.0752        0.163        0.231         4.74       0.0801\n",
            "! Validation         58  101.025    0.002       0.0647      0.00749       0.0722        0.161        0.232         4.17       0.0653\n",
            "Wall time: 101.02611851400002\n",
            "! Best model       58    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10        0.115         0.11      0.00504        0.249        0.303         4.15       0.0649\n",
            "     59    20       0.0715       0.0695      0.00199         0.19        0.241         2.61       0.0408\n",
            "     59    30        0.113        0.109       0.0039        0.251        0.302         3.65       0.0571\n",
            "     59    40       0.0274       0.0264        0.001        0.108        0.148         1.82       0.0289\n",
            "     59    50       0.0806       0.0792      0.00144        0.209        0.257         2.19       0.0347\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2       0.0698        0.064      0.00586        0.162        0.231         3.27       0.0513\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59  102.315    0.002       0.0627       0.0114       0.0741        0.163        0.229         5.11       0.0829\n",
            "! Validation         59  102.315    0.002       0.0644      0.00774       0.0722         0.16        0.232         4.31       0.0675\n",
            "Wall time: 102.31536403400014\n",
            "! Best model       59    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10       0.0772       0.0745      0.00265          0.2        0.249         3.01        0.047\n",
            "     60    20        0.131        0.107        0.024        0.236        0.299         9.05        0.141\n",
            "     60    30        0.144        0.119       0.0257         0.26        0.315         9.38        0.147\n",
            "     60    40       0.0401        0.038      0.00208        0.129        0.178         2.62       0.0416\n",
            "     60    50       0.0536       0.0241       0.0295        0.109        0.142         9.89        0.157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2       0.0695       0.0639      0.00561        0.162        0.231          3.2       0.0501\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60  103.608    0.002       0.0617       0.0112       0.0729        0.162        0.228         4.68       0.0777\n",
            "! Validation         60  103.608    0.002       0.0645      0.00741       0.0719         0.16        0.232         4.23       0.0662\n",
            "Wall time: 103.609156343\n",
            "! Best model       60    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10        0.182        0.118       0.0637        0.244        0.314         14.8        0.231\n",
            "     61    20       0.0211       0.0139      0.00719       0.0849        0.108         4.96       0.0775\n",
            "     61    30        0.115        0.109       0.0055        0.242        0.302         4.27       0.0678\n",
            "     61    40        0.177        0.157       0.0197        0.288        0.363          8.2        0.128\n",
            "     61    50       0.0658       0.0148        0.051       0.0874        0.111         13.2        0.206\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2       0.0694       0.0642      0.00517        0.162        0.232            3       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61  104.906    0.002       0.0641       0.0161       0.0802        0.166        0.232         6.14        0.101\n",
            "! Validation         61  104.906    0.002       0.0646      0.00695       0.0716        0.161        0.232         4.08       0.0639\n",
            "Wall time: 104.90735393500017\n",
            "! Best model       61    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10     5.34e-07     8.92e-11     5.34e-07     5.51e-06     8.63e-06       0.0427     0.000668\n",
            "     62    20        0.152        0.134       0.0183        0.273        0.334         7.91        0.124\n",
            "     62    30       0.0562        0.052      0.00418        0.166        0.208         3.72       0.0591\n",
            "     62    40       0.0276       0.0275     8.21e-05         0.11        0.151        0.522      0.00828\n",
            "     62    50       0.0571        0.055       0.0021        0.169        0.214         2.68       0.0419\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2       0.0696       0.0647      0.00493        0.163        0.232         2.83       0.0444\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62  106.213    0.002       0.0655       0.0135        0.079        0.166        0.234         4.97        0.082\n",
            "! Validation         62  106.213    0.002       0.0646      0.00679       0.0713        0.161        0.232         3.99       0.0624\n",
            "Wall time: 106.21407049100003\n",
            "! Best model       62    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10       0.0831       0.0814      0.00169        0.208        0.261         2.36       0.0375\n",
            "     63    20        0.145        0.104       0.0411        0.236        0.294         11.9        0.185\n",
            "     63    30       0.0159       0.0112      0.00465       0.0764       0.0968         3.99       0.0623\n",
            "     63    40        0.084       0.0685       0.0155        0.194        0.239         7.27        0.114\n",
            "     63    50       0.0544       0.0515      0.00295         0.12        0.207         3.17       0.0496\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2       0.0691       0.0635      0.00557        0.162         0.23         3.11       0.0488\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63  107.492    0.002        0.062       0.0114       0.0734        0.161        0.228         4.65       0.0762\n",
            "! Validation         63  107.492    0.002       0.0638      0.00753       0.0713         0.16        0.231         4.21       0.0659\n",
            "Wall time: 107.49281202800012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10       0.0222       0.0219     0.000301        0.103        0.135        0.998       0.0158\n",
            "     64    20       0.0424       0.0264        0.016        0.109        0.148         7.27        0.115\n",
            "     64    30       0.0415       0.0197       0.0218       0.0502        0.128         1.62        0.135\n",
            "     64    40       0.0198       0.0137       0.0061       0.0841        0.107         4.57       0.0714\n",
            "     64    50         0.15        0.142      0.00846        0.277        0.344         5.38       0.0841\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2       0.0689       0.0636      0.00533        0.162         0.23         2.99       0.0469\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64  109.028    0.002       0.0622       0.0101       0.0723        0.162        0.228         4.59       0.0755\n",
            "! Validation         64  109.028    0.002       0.0639      0.00723       0.0711         0.16        0.231         4.11       0.0643\n",
            "Wall time: 109.02958173399998\n",
            "! Best model       64    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10       0.0973        0.096      0.00129        0.219        0.283         2.07       0.0328\n",
            "     65    20       0.0171       0.0151        0.002         0.09        0.112         2.62       0.0409\n",
            "     65    30       0.0349       0.0254      0.00956       0.0538        0.146         1.07       0.0894\n",
            "     65    40        0.184        0.183     0.000229        0.226        0.391        0.885       0.0138\n",
            "     65    50        0.127        0.123      0.00385        0.265        0.321         3.63       0.0567\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2       0.0679       0.0629      0.00504         0.16        0.229         2.89       0.0453\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65  110.849    0.002       0.0627      0.00977       0.0725        0.161        0.229          4.7       0.0759\n",
            "! Validation         65  110.849    0.002       0.0635      0.00684       0.0703        0.159         0.23         4.01       0.0627\n",
            "Wall time: 110.85093146400004\n",
            "! Best model       65    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10        0.202        0.135       0.0671        0.257        0.336         15.1        0.237\n",
            "     66    20       0.0291       0.0291     6.66e-06        0.109        0.156        0.149      0.00236\n",
            "     66    30        0.133        0.115       0.0173        0.247         0.31         7.69         0.12\n",
            "     66    40        0.102       0.0914       0.0104        0.221        0.276         5.87       0.0931\n",
            "     66    50       0.0701        0.053       0.0171        0.165         0.21         7.53        0.119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2       0.0684       0.0634      0.00499        0.161         0.23          2.9       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  112.792    0.002       0.0647       0.0113        0.076        0.166        0.233         4.96       0.0793\n",
            "! Validation         66  112.792    0.002       0.0641      0.00676       0.0708         0.16        0.231         3.99       0.0625\n",
            "Wall time: 112.79411164300018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10       0.0902       0.0777       0.0125          0.2        0.255         6.43        0.102\n",
            "     67    20       0.0649       0.0398       0.0251        0.141        0.182         9.27        0.145\n",
            "     67    30       0.0186      0.00985      0.00875       0.0524       0.0907         5.47       0.0855\n",
            "     67    40       0.0479       0.0449      0.00307        0.112        0.194         3.24       0.0507\n",
            "     67    50       0.0301       0.0298     0.000364        0.115        0.158          1.1       0.0174\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2       0.0683       0.0634       0.0049        0.161         0.23         2.77       0.0434\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  114.738    0.002       0.0611      0.00938       0.0705        0.161        0.226         4.24       0.0708\n",
            "! Validation         67  114.738    0.002        0.064      0.00667       0.0707         0.16        0.231         3.91       0.0611\n",
            "Wall time: 114.74009094200005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10       0.0568       0.0417       0.0151        0.108        0.187         7.17        0.112\n",
            "     68    20        0.152        0.117       0.0351        0.255        0.312           11        0.171\n",
            "     68    30        0.096       0.0878      0.00821        0.217        0.271         5.22       0.0828\n",
            "     68    40       0.0321       0.0321     1.09e-05        0.134        0.164         0.19      0.00302\n",
            "     68    50        0.134        0.132      0.00237        0.259        0.332         2.85       0.0445\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2       0.0681       0.0632       0.0049         0.16         0.23         2.76       0.0432\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  116.190    0.002       0.0628        0.008       0.0708        0.161        0.229         3.95       0.0651\n",
            "! Validation         68  116.190    0.002       0.0637      0.00671       0.0704        0.159        0.231         3.92       0.0613\n",
            "Wall time: 116.19116897900017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10       0.0325       0.0306      0.00184         0.12         0.16         2.47       0.0392\n",
            "     69    20       0.0567       0.0425       0.0142         0.15        0.188         6.96        0.109\n",
            "     69    30        0.179        0.142       0.0373        0.275        0.344         11.3        0.176\n",
            "     69    40       0.0297       0.0225      0.00717        0.106        0.137         4.87       0.0774\n",
            "     69    50         0.03       0.0258      0.00423        0.109        0.147         3.74       0.0594\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2       0.0674       0.0626      0.00482         0.16        0.229         2.75       0.0432\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  117.520    0.002       0.0603       0.0146        0.075         0.16        0.225         5.83       0.0949\n",
            "! Validation         69  117.520    0.002       0.0632      0.00663       0.0699        0.159         0.23         3.92       0.0613\n",
            "Wall time: 117.52135847400018\n",
            "! Best model       69    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10        0.038       0.0243       0.0137       0.0822        0.142         6.86        0.107\n",
            "     70    20       0.0999       0.0866       0.0133        0.211        0.269         6.65        0.106\n",
            "     70    30       0.0403       0.0381       0.0022        0.139        0.178         2.74       0.0429\n",
            "     70    40        0.162         0.14       0.0224        0.279        0.341         8.75        0.137\n",
            "     70    50       0.0557       0.0422       0.0135         0.15        0.188          6.8        0.106\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2       0.0673       0.0626       0.0047         0.16        0.229         2.73       0.0428\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  118.813    0.002       0.0598       0.0101       0.0699        0.159        0.224         4.72       0.0774\n",
            "! Validation         70  118.813    0.002       0.0632      0.00648       0.0697        0.158         0.23         3.89       0.0608\n",
            "Wall time: 118.81402507899998\n",
            "! Best model       70    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10       0.0546       0.0518      0.00276        0.165        0.208         3.02        0.048\n",
            "     71    20       0.0627       0.0184       0.0444       0.0475        0.124         2.31        0.192\n",
            "     71    30       0.0298       0.0289     0.000911        0.109        0.155         1.74       0.0276\n",
            "     71    40        0.142        0.127       0.0155        0.254        0.325         7.29        0.114\n",
            "     71    50       0.0758       0.0711      0.00472        0.196        0.244         3.96       0.0628\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2       0.0673       0.0626      0.00472         0.16        0.229         2.76       0.0433\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  120.101    0.002       0.0599      0.00967       0.0696        0.159        0.224         4.35       0.0724\n",
            "! Validation         71  120.101    0.002       0.0631      0.00648       0.0696        0.158        0.229          3.9       0.0609\n",
            "Wall time: 120.10289522100015\n",
            "! Best model       71    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10        0.038       0.0372     0.000816        0.128        0.176         1.64       0.0261\n",
            "     72    20       0.0165       0.0148      0.00168       0.0898        0.111          2.4       0.0375\n",
            "     72    30       0.0612       0.0593       0.0019        0.175        0.223         2.55       0.0398\n",
            "     72    40        0.106        0.102      0.00407        0.226        0.292         3.67       0.0583\n",
            "     72    50       0.0916       0.0757       0.0158          0.2        0.251         7.25        0.115\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2       0.0668       0.0619       0.0049        0.159        0.227         2.85       0.0447\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  121.397    0.002       0.0621      0.00817       0.0703        0.161        0.228         4.07       0.0664\n",
            "! Validation         72  121.397    0.002       0.0626      0.00668       0.0693        0.158        0.229         3.96       0.0619\n",
            "Wall time: 121.397901047\n",
            "! Best model       72    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10        0.103       0.0984      0.00434        0.165        0.287         3.85       0.0602\n",
            "     73    20        0.124        0.111       0.0136        0.252        0.304         6.82        0.107\n",
            "     73    30        0.134        0.109       0.0258        0.236        0.301         9.39        0.147\n",
            "     73    40        0.022       0.0115       0.0105       0.0775       0.0979            6       0.0937\n",
            "     73    50       0.0932       0.0891       0.0041         0.22        0.273         3.69       0.0585\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2       0.0669       0.0625      0.00447         0.16        0.228         2.62       0.0411\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  122.680    0.002       0.0626      0.00772       0.0704        0.163        0.229         4.08       0.0673\n",
            "! Validation         73  122.680    0.002       0.0629      0.00622       0.0691        0.158        0.229         3.79       0.0593\n",
            "Wall time: 122.68093548299998\n",
            "! Best model       73    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10        0.159        0.121       0.0381         0.26        0.318         11.4        0.178\n",
            "     74    20        0.102        0.102     0.000163        0.227        0.291        0.735       0.0117\n",
            "     74    30       0.0351       0.0277      0.00743        0.116        0.152         4.96       0.0787\n",
            "     74    40       0.0135        0.013     0.000529       0.0833        0.104         1.35        0.021\n",
            "     74    50       0.0915       0.0864      0.00511        0.212        0.269         4.18       0.0653\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2       0.0668       0.0623      0.00446        0.159        0.228         2.58       0.0404\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  123.970    0.002       0.0615       0.0157       0.0772        0.161        0.227         5.68       0.0932\n",
            "! Validation         74  123.970    0.002       0.0626      0.00626       0.0689        0.158        0.229         3.77        0.059\n",
            "Wall time: 123.97058949000007\n",
            "! Best model       74    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     0.000262      7.5e-11     0.000262     5.39e-06     7.91e-06        0.946       0.0148\n",
            "     75    20       0.0641       0.0534       0.0107        0.166        0.211         5.95       0.0944\n",
            "     75    30        0.109        0.107      0.00207        0.236        0.298         2.66       0.0416\n",
            "     75    40        0.115       0.0886       0.0262        0.215        0.272         9.31        0.148\n",
            "     75    50       0.0278       0.0275     0.000255        0.114        0.152        0.919       0.0146\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2        0.067       0.0625      0.00452         0.16        0.228         2.65       0.0416\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  125.337    0.002       0.0628       0.0112       0.0739        0.162        0.229         4.95       0.0799\n",
            "! Validation         75  125.337    0.002       0.0628      0.00631       0.0691        0.158        0.229         3.81       0.0596\n",
            "Wall time: 125.33795144500004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10       0.0351       0.0328      0.00234         0.12        0.165         2.79       0.0442\n",
            "     76    20       0.0617       0.0476       0.0141        0.115        0.199         6.95        0.109\n",
            "     76    30        0.119        0.109         0.01         0.24        0.302         5.86       0.0915\n",
            "     76    40        0.113        0.102       0.0111        0.241        0.292         6.16       0.0963\n",
            "     76    50       0.0738       0.0732     0.000631        0.195        0.247         1.47        0.023\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2       0.0661       0.0612      0.00486        0.158        0.226         2.89       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  127.063    0.002       0.0608      0.00787       0.0686        0.159        0.225         3.87       0.0617\n",
            "! Validation         76  127.063    0.002       0.0623       0.0066       0.0689        0.157        0.228         3.98       0.0622\n",
            "Wall time: 127.06534930200019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10       0.0266       0.0221       0.0045       0.0511        0.136        0.735       0.0613\n",
            "     77    20        0.182        0.126       0.0552        0.249        0.325         13.7        0.215\n",
            "     77    30       0.0798        0.079     0.000853          0.2        0.257         1.68       0.0267\n",
            "     77    40       0.0952       0.0952      5.1e-05        0.224        0.282        0.418      0.00652\n",
            "     77    50       0.0751       0.0751     8.46e-07        0.202         0.25       0.0529      0.00084\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2       0.0658       0.0606      0.00516        0.157        0.225         3.03       0.0475\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  128.999    0.002       0.0588      0.00914        0.068        0.158        0.222         4.29       0.0691\n",
            "! Validation         77  128.999    0.002       0.0621      0.00689        0.069        0.157        0.228         4.06       0.0636\n",
            "Wall time: 129.00006546200007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10       0.0826       0.0821     0.000475        0.211        0.262         1.25       0.0199\n",
            "     78    20        0.179        0.151       0.0279        0.271        0.355         9.77        0.153\n",
            "     78    30       0.0302       0.0275      0.00276        0.113        0.151         3.02        0.048\n",
            "     78    40        0.149        0.129       0.0199        0.256        0.329         8.25        0.129\n",
            "     78    50       0.0717         0.02       0.0517        0.047        0.129         2.49        0.208\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2       0.0658       0.0608      0.00496        0.157        0.225         2.91       0.0457\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  130.971    0.002       0.0597      0.00927        0.069        0.159        0.224         4.25       0.0713\n",
            "! Validation         78  130.971    0.002       0.0621      0.00668       0.0688        0.157        0.228         3.98       0.0622\n",
            "Wall time: 130.973061939\n",
            "! Best model       78    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10       0.0195       0.0073       0.0122       0.0451       0.0781         6.46        0.101\n",
            "     79    20       0.0289       0.0288     0.000114        0.117        0.155        0.615      0.00976\n",
            "     79    30       0.0811       0.0773      0.00378        0.205        0.254         3.54       0.0562\n",
            "     79    40       0.0186       0.0126        0.006       0.0814        0.103         4.53       0.0708\n",
            "     79    50       0.0228       0.0226     0.000199        0.105        0.137        0.811       0.0129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2       0.0657       0.0609      0.00481        0.157        0.226         2.83       0.0444\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  132.589    0.002       0.0607       0.0102       0.0709         0.16        0.226         4.57       0.0755\n",
            "! Validation         79  132.589    0.002       0.0619      0.00655       0.0684        0.157        0.227         3.91       0.0612\n",
            "Wall time: 132.58959205200017\n",
            "! Best model       79    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10       0.0514       0.0151       0.0363       0.0887        0.112         11.1        0.174\n",
            "     80    20        0.017       0.0143      0.00274       0.0862        0.109         3.06       0.0479\n",
            "     80    30      0.00142     8.44e-11      0.00142      5.4e-06      8.4e-06          2.2       0.0344\n",
            "     80    40       0.0307       0.0137        0.017       0.0844        0.107         7.63        0.119\n",
            "     80    50       0.0372       0.0291       0.0081        0.118        0.156         5.18       0.0822\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2       0.0653       0.0603      0.00508        0.156        0.224            3        0.047\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  133.884    0.002       0.0603       0.0107        0.071         0.16        0.225         4.66       0.0772\n",
            "! Validation         80  133.884    0.002       0.0615      0.00681       0.0683        0.156        0.227         4.03       0.0631\n",
            "Wall time: 133.885460427\n",
            "! Best model       80    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10        0.135        0.123       0.0126        0.259         0.32         6.58        0.103\n",
            "     81    20       0.0923       0.0887       0.0036         0.22        0.272         3.46       0.0549\n",
            "     81    30        0.115        0.113      0.00172        0.239        0.307         2.39       0.0379\n",
            "     81    40        0.102        0.102     0.000147        0.228        0.291        0.698       0.0111\n",
            "     81    50        0.136        0.101       0.0348         0.23        0.291         10.9         0.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2       0.0654       0.0606      0.00484        0.157        0.225         2.87       0.0451\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  135.162    0.002       0.0611      0.00928       0.0704         0.16        0.226         4.48       0.0727\n",
            "! Validation         81  135.162    0.002       0.0615      0.00658        0.068        0.156        0.227         3.94       0.0617\n",
            "Wall time: 135.16274150900017\n",
            "! Best model       81    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10        0.132        0.117       0.0142        0.253        0.313         6.97        0.109\n",
            "     82    20       0.0902       0.0899     0.000266        0.214        0.274        0.939       0.0149\n",
            "     82    30       0.0272       0.0266     0.000679        0.109        0.149          1.5       0.0238\n",
            "     82    40       0.0975       0.0974     0.000122        0.224        0.285        0.635       0.0101\n",
            "     82    50       0.0292       0.0271      0.00213       0.0556         0.15        0.506       0.0422\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2       0.0656       0.0611      0.00456        0.157        0.226         2.65       0.0415\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  136.432    0.002       0.0603      0.00598       0.0663         0.16        0.224         3.31       0.0539\n",
            "! Validation         82  136.432    0.002       0.0619      0.00629       0.0682        0.156        0.227         3.78       0.0592\n",
            "Wall time: 136.43317095800012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10        0.143        0.094        0.049        0.222         0.28         12.9        0.202\n",
            "     83    20       0.0275       0.0129       0.0146       0.0825        0.104         7.08        0.111\n",
            "     83    30       0.0707       0.0663      0.00431        0.187        0.235         3.78         0.06\n",
            "     83    40        0.147        0.123       0.0235        0.262        0.321         8.97         0.14\n",
            "     83    50        0.123        0.108        0.015        0.238        0.301         7.17        0.112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2       0.0651       0.0604      0.00468        0.157        0.225          2.8       0.0439\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  137.699    0.002       0.0598       0.0131        0.073        0.158        0.224         5.14       0.0849\n",
            "! Validation         83  137.699    0.002       0.0612       0.0064       0.0676        0.156        0.226         3.88       0.0608\n",
            "Wall time: 137.6996589710002\n",
            "! Best model       83    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10        0.052       0.0479      0.00413        0.163          0.2         3.76       0.0587\n",
            "     84    20       0.0353       0.0258      0.00951       0.0531        0.147         1.07       0.0891\n",
            "     84    30        0.143       0.0974       0.0454        0.231        0.285         12.5        0.195\n",
            "     84    40       0.0936       0.0922      0.00147        0.215        0.277         2.21       0.0351\n",
            "     84    50       0.0569       0.0568     0.000118        0.171        0.218        0.635      0.00992\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2       0.0651       0.0605      0.00461        0.157        0.225         2.73       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  139.050    0.002       0.0599      0.00805        0.068        0.161        0.224         4.14       0.0677\n",
            "! Validation         84  139.050    0.002       0.0614      0.00633       0.0677        0.156        0.226         3.84         0.06\n",
            "Wall time: 139.05098671200017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10       0.0906       0.0905       0.0001        0.218        0.275        0.576      0.00915\n",
            "     85    20        0.129       0.0946       0.0347        0.155        0.281         6.13         0.17\n",
            "     85    30          0.1        0.099      0.00115        0.234        0.288         1.98       0.0309\n",
            "     85    40         0.16          0.1       0.0592        0.232        0.289         14.2        0.222\n",
            "     85    50       0.0237       0.0222      0.00141        0.104        0.136         2.16       0.0343\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2       0.0652       0.0601      0.00518        0.156        0.224         3.05       0.0479\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  140.327    0.002       0.0585       0.0112       0.0697        0.159        0.222         4.29       0.0715\n",
            "! Validation         85  140.327    0.002       0.0613      0.00691       0.0682        0.156        0.226         4.07       0.0636\n",
            "Wall time: 140.32754743100008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10      0.00027     7.78e-11      0.00027     5.53e-06     8.06e-06        0.961        0.015\n",
            "     86    20        0.119        0.108       0.0106        0.241        0.301         6.03       0.0942\n",
            "     86    30       0.0317       0.0317     1.92e-05        0.121        0.163        0.252      0.00401\n",
            "     86    40        0.094        0.094     1.05e-05        0.223         0.28         0.19      0.00296\n",
            "     86    50        0.101       0.0996      0.00162        0.224        0.288         2.32       0.0368\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2       0.0651       0.0601      0.00495        0.156        0.224         2.93       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  141.638    0.002       0.0595      0.00806       0.0676        0.158        0.223         4.21       0.0686\n",
            "! Validation         86  141.638    0.002       0.0612      0.00669       0.0679        0.156        0.226         3.98       0.0623\n",
            "Wall time: 141.63882135100016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10       0.0578       0.0577      0.00012        0.173        0.219         0.64         0.01\n",
            "     87    20       0.0429       0.0259        0.017       0.0523        0.147         1.43        0.119\n",
            "     87    30        0.124        0.117      0.00712        0.246        0.313         4.86       0.0771\n",
            "     87    40        0.132        0.132     8.49e-05        0.268        0.332        0.539      0.00842\n",
            "     87    50        0.106       0.0769       0.0288        0.204        0.253         9.77        0.155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2       0.0651       0.0604      0.00468        0.157        0.225         2.81       0.0441\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  143.311    0.002       0.0667        0.011       0.0777        0.166        0.236         4.99        0.082\n",
            "! Validation         87  143.311    0.002       0.0612      0.00641       0.0676        0.156        0.226         3.89       0.0609\n",
            "Wall time: 143.31314326899997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10        0.119        0.103       0.0161        0.237        0.294         7.42        0.116\n",
            "     88    20      0.00218     7.18e-11      0.00218     4.87e-06     7.75e-06         2.73       0.0427\n",
            "     88    30       0.0911       0.0911     5.68e-08        0.226        0.276       0.0137     0.000218\n",
            "     88    40       0.0289       0.0285     0.000439        0.116        0.154         1.21       0.0192\n",
            "     88    50       0.0782       0.0767      0.00151        0.197        0.253         2.23       0.0355\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2       0.0649       0.0604      0.00457        0.156        0.225         2.64       0.0413\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  145.175    0.002       0.0615      0.00877       0.0703        0.161        0.227         4.14        0.068\n",
            "! Validation         88  145.175    0.002       0.0612      0.00633       0.0676        0.155        0.226         3.79       0.0593\n",
            "Wall time: 145.1772268510001\n",
            "! Best model       88    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10       0.0823       0.0822     0.000116        0.151        0.262        0.631      0.00985\n",
            "     89    20       0.0499       0.0366       0.0134        0.134        0.175         6.66        0.106\n",
            "     89    30        0.152         0.12       0.0324         0.25        0.316         10.4        0.165\n",
            "     89    40       0.0923       0.0911      0.00119         0.22        0.276         1.99       0.0315\n",
            "     89    50       0.0268       0.0264      0.00036       0.0544        0.149        0.208       0.0173\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2       0.0645       0.0597      0.00479        0.155        0.223         2.86       0.0449\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  147.158    0.002       0.0612      0.00959       0.0708         0.16        0.226         4.24       0.0686\n",
            "! Validation         89  147.158    0.002       0.0609      0.00648       0.0674        0.155        0.226         3.93       0.0614\n",
            "Wall time: 147.16011486600019\n",
            "! Best model       89    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10       0.0558       0.0497      0.00604        0.118        0.204         4.55        0.071\n",
            "     90    20       0.0231       0.0048       0.0183       0.0366       0.0633         7.92        0.124\n",
            "     90    30       0.0809       0.0788       0.0021        0.201        0.256         2.64       0.0419\n",
            "     90    40        0.112        0.102      0.00999         0.16        0.292         3.29       0.0913\n",
            "     90    50       0.0348       0.0333      0.00153         0.12        0.167         2.25       0.0357\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2       0.0648       0.0597      0.00503        0.156        0.223         2.96       0.0465\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  148.983    0.002       0.0608       0.0111       0.0719        0.161        0.226         4.86       0.0793\n",
            "! Validation         90  148.983    0.002       0.0611      0.00672       0.0678        0.155        0.226         3.99       0.0624\n",
            "Wall time: 148.98355756800015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10       0.0325       0.0165        0.016       0.0921        0.118          7.4        0.116\n",
            "     91    20        0.156        0.128       0.0278        0.256        0.327         9.75        0.152\n",
            "     91    30        0.114        0.105      0.00935        0.243        0.296         5.66       0.0884\n",
            "     91    40         0.12        0.103       0.0176        0.234        0.293         7.77        0.121\n",
            "     91    50       0.0758       0.0743      0.00141        0.196        0.249          2.2       0.0344\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2       0.0646       0.0597      0.00487        0.155        0.223          2.9       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  150.276    0.002       0.0606      0.00785       0.0685         0.16        0.225         3.98       0.0659\n",
            "! Validation         91  150.276    0.002       0.0611      0.00654       0.0676        0.155        0.226         3.93       0.0615\n",
            "Wall time: 150.27730461200008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10       0.0914       0.0913     0.000171        0.217        0.276        0.754        0.012\n",
            "     92    20       0.0951       0.0912      0.00387        0.216        0.276         3.58       0.0568\n",
            "     92    30       0.0251       0.0232      0.00192        0.107        0.139         2.53       0.0401\n",
            "     92    40       0.0152        0.014      0.00117       0.0868        0.108            2       0.0312\n",
            "     92    50        0.093       0.0916       0.0014        0.221        0.277         2.19       0.0342\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2       0.0647       0.0601      0.00457        0.156        0.224         2.74       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  151.556    0.002        0.059      0.00934       0.0684        0.158        0.222         4.28       0.0682\n",
            "! Validation         92  151.556    0.002       0.0613      0.00617       0.0675        0.156        0.226          3.8       0.0594\n",
            "Wall time: 151.55635815699998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10        0.109        0.102      0.00681        0.227        0.292         4.75       0.0754\n",
            "     93    20       0.0807       0.0792      0.00147        0.206        0.257         2.24       0.0351\n",
            "     93    30        0.138        0.123       0.0147        0.185        0.321         7.09        0.111\n",
            "     93    40       0.0112     8.15e-11       0.0112     5.24e-06     8.25e-06         6.19       0.0967\n",
            "     93    50        0.119        0.101       0.0181        0.239        0.291         7.86        0.123\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2       0.0644       0.0589      0.00547        0.154        0.222         3.22       0.0506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  152.849    0.002       0.0607      0.00919       0.0699        0.159        0.225         4.32       0.0717\n",
            "! Validation         93  152.849    0.002       0.0606      0.00711       0.0677        0.155        0.225         4.16       0.0651\n",
            "Wall time: 152.85003989899997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10        0.137        0.104       0.0323        0.242        0.295         10.5        0.164\n",
            "     94    20          0.1       0.0971      0.00292        0.226        0.285         3.11       0.0494\n",
            "     94    30       0.0268       0.0268     4.22e-05        0.114        0.149        0.374      0.00593\n",
            "     94    40       0.0123       0.0046      0.00769       0.0358        0.062         5.13       0.0801\n",
            "     94    50       0.0289       0.0288     8.11e-05        0.117        0.155        0.519      0.00823\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2       0.0643       0.0594      0.00492        0.155        0.223         2.95       0.0463\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  154.133    0.002        0.059      0.00879       0.0677        0.158        0.223         4.22       0.0702\n",
            "! Validation         94  154.133    0.002       0.0608      0.00655       0.0673        0.155        0.225         3.96       0.0619\n",
            "Wall time: 154.13355871099998\n",
            "! Best model       94    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10       0.0271       0.0269     0.000211        0.109         0.15        0.837       0.0133\n",
            "     95    20       0.0394       0.0327      0.00665         0.13        0.165         4.77       0.0745\n",
            "     95    30       0.0285       0.0281     0.000404        0.113        0.153         1.16       0.0184\n",
            "     95    40        0.119       0.0921        0.027         0.22        0.277         9.46         0.15\n",
            "     95    50       0.0863       0.0849      0.00135        0.209        0.266         2.15       0.0336\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2       0.0641       0.0593      0.00487        0.155        0.222         2.92       0.0458\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  155.436    0.002       0.0593      0.00989       0.0692         0.16        0.223         4.64       0.0758\n",
            "! Validation         95  155.436    0.002       0.0607      0.00653       0.0672        0.155        0.225         3.94       0.0617\n",
            "Wall time: 155.43670044500004\n",
            "! Best model       95    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10        0.164        0.155      0.00863        0.276         0.36         5.43       0.0849\n",
            "     96    20       0.0587       0.0583     0.000361        0.179        0.221         1.11       0.0174\n",
            "     96    30        0.132        0.127      0.00553        0.178        0.325         2.45        0.068\n",
            "     96    40       0.0423       0.0344      0.00786        0.133         0.17         5.18        0.081\n",
            "     96    50       0.0263       0.0263     2.98e-05        0.107        0.148        0.315      0.00499\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2       0.0643       0.0592      0.00512        0.155        0.222         2.94       0.0462\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  156.732    0.002       0.0632       0.0104       0.0736        0.162         0.23         4.24       0.0686\n",
            "! Validation         96  156.732    0.002       0.0602      0.00689       0.0671        0.154        0.224         3.98       0.0624\n",
            "Wall time: 156.73344207900004\n",
            "! Best model       96    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10      0.00231     7.89e-11      0.00231        5e-06     8.12e-06         2.81        0.044\n",
            "     97    20        0.174         0.15       0.0237        0.273        0.354         9.01        0.141\n",
            "     97    30       0.0426        0.016       0.0266       0.0925        0.116         9.53        0.149\n",
            "     97    40       0.0441       0.0439     0.000179        0.157        0.192        0.783       0.0122\n",
            "     97    50       0.0388       0.0388     2.12e-06         0.13         0.18       0.0837      0.00133\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2       0.0637       0.0589      0.00484        0.154        0.222         2.88       0.0452\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  158.025    0.002       0.0601      0.00862       0.0687        0.158        0.224         4.14       0.0654\n",
            "! Validation         97  158.025    0.002       0.0601      0.00652       0.0667        0.154        0.224         3.92       0.0613\n",
            "Wall time: 158.026151625\n",
            "! Best model       97    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10       0.0294        0.028       0.0014        0.113        0.153         2.15       0.0341\n",
            "     98    20        0.144        0.131       0.0125        0.261        0.331         6.54        0.102\n",
            "     98    30       0.0127       0.0117      0.00107       0.0783       0.0987         1.91       0.0299\n",
            "     98    40       0.0366        0.034      0.00263        0.122        0.169         2.95       0.0468\n",
            "     98    50       0.0425       0.0414      0.00108        0.148        0.186         1.92         0.03\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2       0.0643       0.0596      0.00472        0.155        0.223         2.82       0.0441\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  159.579    0.002       0.0595      0.00683       0.0664        0.159        0.224         3.62       0.0602\n",
            "! Validation         98  159.579    0.002       0.0606      0.00639        0.067        0.155        0.225         3.86       0.0604\n",
            "Wall time: 159.57946116800008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10       0.0322       0.0309      0.00125        0.127        0.161         2.03       0.0323\n",
            "     99    20     0.000172     7.18e-11     0.000172     5.38e-06     7.74e-06        0.767        0.012\n",
            "     99    30       0.0779       0.0578       0.0202        0.173         0.22         8.31         0.13\n",
            "     99    40       0.0695       0.0694     7.68e-05        0.189        0.241        0.512      0.00801\n",
            "     99    50       0.0833       0.0796       0.0037        0.203        0.258          3.5       0.0556\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2       0.0638       0.0593      0.00455        0.155        0.222         2.64       0.0413\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  161.365    0.002       0.0608      0.00859       0.0694         0.16        0.226         4.27       0.0703\n",
            "! Validation         99  161.365    0.002       0.0605       0.0062       0.0667        0.154        0.225         3.74       0.0585\n",
            "Wall time: 161.36679622200018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10       0.0311       0.0228      0.00829       0.0796        0.138         5.32       0.0832\n",
            "    100    20       0.0501       0.0487      0.00141        0.161        0.202          2.2       0.0343\n",
            "    100    30       0.0893       0.0846      0.00474        0.213        0.266         3.97       0.0629\n",
            "    100    40        0.104       0.0825       0.0213        0.206        0.263         8.53        0.133\n",
            "    100    50       0.0124       0.0123     8.88e-05       0.0798        0.101        0.551      0.00861\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2       0.0635       0.0591      0.00441        0.154        0.222         2.59       0.0406\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  163.309    0.002       0.0619       0.0124       0.0743        0.162        0.228         4.89       0.0806\n",
            "! Validation        100  163.309    0.002       0.0606      0.00597       0.0665        0.154        0.225         3.69       0.0578\n",
            "Wall time: 163.31055069000013\n",
            "! Best model      100    0.067\n",
            "! Stop training: max epochs\n",
            "Wall time: 163.3330268220002\n",
            "Cumulative wall time: 163.3330268220002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae █▆▃▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae █▆▄▃▃▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae █▄▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse █▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss █▄▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e █▅▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae █▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae █▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR 0.002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall 163.30856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae 0.08057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae 4.89087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae 0.16246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse 0.22751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss 0.07428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e 0.01237\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f 0.0619\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae 0.05778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae 3.69469\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae 0.15438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse 0.22491\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss 0.06654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e 0.00597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f 0.06057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall 163.30856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msi\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-26092402475183654/allegro-tutorial/runs/bh3o27yv?apiKey=5e4262c24ac1ed30e3b7b22a99893c835272cdb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230722_133612-bh3o27yv/logs\u001b[0m\n",
            "CPU times: user 7.61 s, sys: 905 ms, total: 8.51 s\n",
            "Wall time: 21min 40s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls results/silicon-tutorial/si #/best_model.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KRsXlG65jGH",
        "outputId": "1007ca71-d41f-4f08-cc4d-59a9b9de41df"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model.pth\tlog\t\t\t metrics_epoch.csv\n",
            "config.yaml\tmetrics_batch_train.csv  metrics_initialization.csv\n",
            "last_model.pth\tmetrics_batch_val.csv\t trainer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nequip.utils import Config\n",
        "from nequip.model import model_from_config\n",
        "from nequip.data import AtomicData, ASEDataset\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "config = Config.from_file(\"results/silicon-tutorial/si/config.yaml\")\n",
        "\n",
        "#config[\"train_on_keys\"]=[\"forces\", \"total_energy\"]\n",
        "#config[\"model_builders\"] = [\"EnergyModel\", \"PerSpeciesRescale\", \"ForceOutput\", \"RescaleEnergyEtc\"]\n",
        "model = model_from_config(config, initialize=False)\n",
        "d = torch.load('results/silicon-tutorial/si/best_model.pth',map_location=device)\n",
        "model.load_state_dict(d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg9svjLM5jI2",
        "outputId": "9b769ad1-485c-4929-a517-795f1e537b2e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0fb_mry5jLl",
        "outputId": "4fc54546-e10e-4a17-8ae3-eae5f91d1cf5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RescaleOutput(\n",
              "  (model): GradientOutput(\n",
              "    (func): SequentialGraphNetwork(\n",
              "      (one_hot): OneHotAtomEncoding()\n",
              "      (radial_basis): RadialBasisEdgeEncoding(\n",
              "        (basis): NormalizedBasis(\n",
              "          (basis): BesselBasis()\n",
              "        )\n",
              "        (cutoff): PolynomialCutoff()\n",
              "      )\n",
              "      (spharm): SphericalHarmonicEdgeAttrs(\n",
              "        (sh): SphericalHarmonics()\n",
              "      )\n",
              "      (allegro): Allegro_Module(\n",
              "        (latents): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (env_embed_mlps): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (tps): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (linears): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (env_linears): ModuleList(\n",
              "          (0): Identity()\n",
              "        )\n",
              "        (_env_weighter): MakeWeightedChannels()\n",
              "        (final_latent): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng): ScalarMLP(\n",
              "        (_module): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng_sum): EdgewiseEnergySum()\n",
              "      (per_species_rescale): PerSpeciesScaleShift()\n",
              "      (total_energy_sum): AtomwiseReduce()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jarvis.core.atoms import Atoms\n",
        "from jarvis.db.figshare import get_jid_data\n",
        "atoms = Atoms.from_dict(get_jid_data(jid='JVASP-1002',dataset='dft_3d')['atoms'])\n",
        "ase_atoms = atoms.ase_converter()\n",
        "a = ASEDataset.from_atoms_list([ase_atoms,ase_atoms],extra_fixed_fields={\"r_max\": 5.0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxCCG4xx5jOK",
        "outputId": "d777274b-2f2e-434d-e666-aa38a7bd014c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 76k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n",
            "Loading the zipfile...\n",
            "Loading completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nequip.data import AtomicData, Collater, dataset_from_config, register_fields, AtomicDataDict\n",
        "from nequip.data.transforms import TypeMapper\n",
        "# c = Collater.for_dataset(a, exclude_keys=[])\n",
        "a = AtomicData.from_ase(ase_atoms,5)\n",
        "data = AtomicData.to_AtomicDataDict(a)\n",
        "# tm = TypeMapper(chemical_symbol_to_type={\"Si\": 0})\n",
        "tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "data = tm(data)\n",
        "out = model(data)\n"
      ],
      "metadata": {
        "id": "0B5JNf8r7Ijz"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "if not os.path.exists('jarvis_leaderboard'):\n",
        "  !git clone https://github.com/usnistgov/jarvis_leaderboard.git\n",
        "# os.chdir('jarvis_leaderboard')\n",
        "# !pip install -e .\n",
        "os.chdir('/content/jarvis_leaderboard/jarvis_leaderboard/contributions/')\n",
        "os.makedirs('allegro_si')\n",
        "os.chdir('allegro_si')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQaB-lCuPvwg",
        "outputId": "2526d1a0-8822-4e70-9f2c-8b7d5f5bce2e"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jarvis_leaderboard'...\n",
            "remote: Enumerating objects: 44059, done.\u001b[K\n",
            "remote: Counting objects: 100% (12867/12867), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1354/1354), done.\u001b[K\n",
            "remote: Total 44059 (delta 6886), reused 12655 (delta 6839), pack-reused 31192\u001b[K\n",
            "Receiving objects: 100% (44059/44059), 314.17 MiB | 18.24 MiB/s, done.\n",
            "Resolving deltas: 100% (23055/23055), done.\n",
            "Updating files: 100% (2568/2568), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://figshare.com/ndownloader/files/40357663 -O mlearn.json.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXXN8_JlPbdr",
        "outputId": "41c0c7cd-31ae-484b-86e8-9f1cec259945"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 15:38:09--  https://figshare.com/ndownloader/files/40357663\n",
            "Resolving figshare.com (figshare.com)... 63.34.138.122, 18.203.227.49, 2a05:d018:1f4:d000:8ff3:f995:dc4f:1c8d, ...\n",
            "Connecting to figshare.com (figshare.com)|63.34.138.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230722/eu-west-1/s3/aws4_request&X-Amz-Date=20230722T153810Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=912ac347d90cc9a2e1e3c7041852f887e4aaee405bd3f552c323883a7a41dbfb [following]\n",
            "--2023-07-22 15:38:10--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20230722/eu-west-1/s3/aws4_request&X-Amz-Date=20230722T153810Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=912ac347d90cc9a2e1e3c7041852f887e4aaee405bd3f552c323883a7a41dbfb\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.30.59, 52.92.32.160, 52.218.28.107, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.30.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542319 (2.4M) [application/zip]\n",
            "Saving to: ‘mlearn.json.zip’\n",
            "\n",
            "mlearn.json.zip     100%[===================>]   2.42M  1.59MB/s    in 1.5s    \n",
            "\n",
            "2023-07-22 15:38:13 (1.59 MB/s) - ‘mlearn.json.zip’ saved [2542319/2542319]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json,zipfile\n",
        "df = pd.DataFrame(\n",
        "    json.loads(\n",
        "        zipfile.ZipFile(\"mlearn.json.zip\").read(\n",
        "            \"mlearn.json\"\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Jw72urwrQG_4"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJa4Uh62QfUC",
        "outputId": "7d24efbe-bb67-43dc-8243-3898affef857"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jarvis_leaderboard/jarvis_leaderboard/contributions/allegro_si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_allegro_forces(model=[],atoms=[],cutoff=5):\n",
        "    ase_atoms = atoms.ase_converter()\n",
        "    a = AtomicData.from_ase(ase_atoms,cutoff)\n",
        "    data = AtomicData.to_AtomicDataDict(a)\n",
        "    tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "    data = tm(data)\n",
        "    out = model(data)\n",
        "    pen=out['total_energy'].squeeze().cpu().detach().numpy().tolist()\n",
        "    num_atoms=atoms.num_atoms\n",
        "    pf=out['forces'].squeeze().cpu().detach().numpy()\n",
        "    return pen,pf,_"
      ],
      "metadata": {
        "id": "5i1-CsYHQvGg"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "for i in glob.glob(\"../../benchmarks/AI/MLFF/*energy*.zip\"):\n",
        "    if \"mlearn\" in i and \"Si\" in i:\n",
        "        fname_e = (\n",
        "            \"AI-MLFF-energy-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-mae.csv\"\n",
        "        )\n",
        "        fname_f = (\n",
        "            \"AI-MLFF-forces-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-multimae.csv\"\n",
        "        )\n",
        "        fname_s = (\n",
        "            \"AI-MLFF-stresses-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-multimae.csv\"\n",
        "        )\n",
        "        f_e = open(fname_e, \"w\")\n",
        "        f_f = open(fname_f, \"w\")\n",
        "        #f_s = open(fname_s, \"w\")\n",
        "\n",
        "        f_e.write(\"id,target,prediction\\n\")\n",
        "        f_f.write(\"id,prediction\\n\")\n",
        "        #f_s.write(\"id,prediction\\n\")\n",
        "\n",
        "        print(i)\n",
        "        dat = json.loads(\n",
        "            zipfile.ZipFile(i).read(i.split(\"/\")[-1].split(\".zip\")[0])\n",
        "        )\n",
        "        print(dat[\"test\"])\n",
        "        for key, val in dat[\"test\"].items():\n",
        "            entry = df[df[\"jid\"] == key]\n",
        "            atoms = Atoms.from_dict(entry.atoms.values[0])\n",
        "            # print(key,val,df[df['jid']==key],atoms)\n",
        "            # energy,forces=get_alignn_forces(atoms)\n",
        "            energy, forces, stress = get_allegro_forces(model=model,atoms=atoms)\n",
        "            print(key, val, energy, atoms.num_atoms)\n",
        "            line = key +\",\"+ str(entry.energy.values[0])+\",\" + str(energy) + \"\\n\"\n",
        "            f_e.write(line)\n",
        "            line = (\n",
        "                key\n",
        "                + \",\"\n",
        "                + str(\";\".join(map(str, np.array(forces).flatten())))\n",
        "                + \"\\n\"\n",
        "            )\n",
        "            f_f.write(line)\n",
        "            # line = (\n",
        "            #     key\n",
        "            #     + \",\"\n",
        "            #     + str(\";\".join(map(str, np.array(stress).flatten())))\n",
        "            #     + \"\\n\"\n",
        "            # )\n",
        "            # f_s.write(line)\n",
        "        f_e.close()\n",
        "        f_f.close()\n",
        "        # f_s.close()\n",
        "        zname = fname_e + \".zip\"\n",
        "        with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            myzip.write(fname_e)\n",
        "\n",
        "        zname = fname_f + \".zip\"\n",
        "        with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            myzip.write(fname_f)\n",
        "\n",
        "        # zname = fname_s + \".zip\"\n",
        "        # with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "        #     myzip.write(fname_s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZJz8BOPGYB",
        "outputId": "29a301ca-21be-48ec-8be1-de05cb65126e"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../../benchmarks/AI/MLFF/mlearn_Si_energy.json.zip\n",
            "{'Si-215': -297.62773938, 'Si-216': -295.77170067, 'Si-217': -291.28958206, 'Si-218': -296.24088456, 'Si-219': -294.41361742, 'Si-220': -334.75283939, 'Si-221': -334.69215136, 'Si-222': -184.71808052, 'Si-223': -121.41180043, 'Si-224': -338.93899696, 'Si-225': -338.83557056, 'Si-226': -335.68901422, 'Si-227': -333.7064957, 'Si-228': -344.85564046, 'Si-229': -344.81108268, 'Si-230': -298.83222646, 'Si-231': -298.96501782, 'Si-232': -295.20943762, 'Si-233': -291.86293882, 'Si-234': -344.74080048, 'Si-235': -344.74080047, 'Si-236': -344.74080046, 'Si-237': -341.22165747, 'Si-238': -341.22165734, 'Si-239': -341.22165747}\n",
            "Si-215 -297.62773938 -300.2811584472656 63\n",
            "Si-216 -295.77170067 -296.49090576171875 63\n",
            "Si-217 -291.28958206 -299.065673828125 63\n",
            "Si-218 -296.24088456 -295.0887451171875 63\n",
            "Si-219 -294.41361742 -296.1150817871094 63\n",
            "Si-220 -334.75283939 -334.7198486328125 63\n",
            "Si-221 -334.69215136 -334.9636535644531 63\n",
            "Si-222 -184.71808052 -190.67774963378906 36\n",
            "Si-223 -121.41180043 -126.26878356933594 24\n",
            "Si-224 -338.93899696 -335.3374328613281 64\n",
            "Si-225 -338.83557056 -335.2520446777344 64\n",
            "Si-226 -335.68901422 -332.79541015625 64\n",
            "Si-227 -333.7064957 -330.9458923339844 64\n",
            "Si-228 -344.85564046 -340.52606201171875 64\n",
            "Si-229 -344.81108268 -340.42626953125 64\n",
            "Si-230 -298.83222646 -305.8009948730469 64\n",
            "Si-231 -298.96501782 -306.7210998535156 64\n",
            "Si-232 -295.20943762 -302.5899658203125 64\n",
            "Si-233 -291.86293882 -299.7530822753906 64\n",
            "Si-234 -344.74080048 -335.03753662109375 64\n",
            "Si-235 -344.74080047 -335.03753662109375 64\n",
            "Si-236 -344.74080046 -335.03753662109375 64\n",
            "Si-237 -341.22165747 -335.36785888671875 64\n",
            "Si-238 -341.22165734 -335.3678894042969 64\n",
            "Si-239 -341.22165747 -335.36785888671875 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entry.energy.values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya4Pak_cSBHU",
        "outputId": "837c8a01-26b0-434d-ce3c-2d55c009ece0"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-341.22165747"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -altr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK1rdjyGRqGD",
        "outputId": "b8f3325a-5cb2-477f-8e7e-ae05dbea301e"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2620\n",
            "-rw-r--r--   1 root root 2542319 Apr 29 01:14 mlearn.json.zip\n",
            "drwxr-xr-x 158 root root   12288 Jul 22 15:37 ..\n",
            "-rw-r--r--   1 root root     668 Jul 22 15:44 AI-MLFF-energy-mlearn_Si-test-mae.csv\n",
            "-rw-r--r--   1 root root   53576 Jul 22 15:44 AI-MLFF-forces-mlearn_Si-test-multimae.csv\n",
            "-rw-r--r--   1 root root     840 Jul 22 15:44 AI-MLFF-energy-mlearn_Si-test-mae.csv.zip\n",
            "drwxr-xr-x   2 root root    4096 Jul 22 15:44 .\n",
            "-rw-r--r--   1 root root   53758 Jul 22 15:44 AI-MLFF-forces-mlearn_Si-test-multimae.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_df = pd.read_csv('AI-MLFF-energy-mlearn_Si-test-mae.csv.zip')"
      ],
      "metadata": {
        "id": "FU4jyfg-Rtjl"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "rGyr7zZGRwKB",
        "outputId": "fdac6a0d-926a-457e-e374-ef3532260bee"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id      target  prediction\n",
              "0   Si-215 -297.627739 -300.281158\n",
              "1   Si-216 -295.771701 -296.490906\n",
              "2   Si-217 -291.289582 -299.065674\n",
              "3   Si-218 -296.240885 -295.088745\n",
              "4   Si-219 -294.413617 -296.115082\n",
              "5   Si-220 -334.752839 -334.719849\n",
              "6   Si-221 -334.692151 -334.963654\n",
              "7   Si-222 -184.718081 -190.677750\n",
              "8   Si-223 -121.411800 -126.268784\n",
              "9   Si-224 -338.938997 -335.337433\n",
              "10  Si-225 -338.835571 -335.252045\n",
              "11  Si-226 -335.689014 -332.795410\n",
              "12  Si-227 -333.706496 -330.945892\n",
              "13  Si-228 -344.855640 -340.526062\n",
              "14  Si-229 -344.811083 -340.426270\n",
              "15  Si-230 -298.832226 -305.800995\n",
              "16  Si-231 -298.965018 -306.721100\n",
              "17  Si-232 -295.209438 -302.589966\n",
              "18  Si-233 -291.862939 -299.753082\n",
              "19  Si-234 -344.740800 -335.037537\n",
              "20  Si-235 -344.740800 -335.037537\n",
              "21  Si-236 -344.740800 -335.037537\n",
              "22  Si-237 -341.221657 -335.367859\n",
              "23  Si-238 -341.221657 -335.367889\n",
              "24  Si-239 -341.221657 -335.367859"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-a1255c17-3297-4be3-aa57-ff1ccee9374b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Si-215</td>\n",
              "      <td>-297.627739</td>\n",
              "      <td>-300.281158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Si-216</td>\n",
              "      <td>-295.771701</td>\n",
              "      <td>-296.490906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Si-217</td>\n",
              "      <td>-291.289582</td>\n",
              "      <td>-299.065674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Si-218</td>\n",
              "      <td>-296.240885</td>\n",
              "      <td>-295.088745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Si-219</td>\n",
              "      <td>-294.413617</td>\n",
              "      <td>-296.115082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Si-220</td>\n",
              "      <td>-334.752839</td>\n",
              "      <td>-334.719849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Si-221</td>\n",
              "      <td>-334.692151</td>\n",
              "      <td>-334.963654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Si-222</td>\n",
              "      <td>-184.718081</td>\n",
              "      <td>-190.677750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Si-223</td>\n",
              "      <td>-121.411800</td>\n",
              "      <td>-126.268784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Si-224</td>\n",
              "      <td>-338.938997</td>\n",
              "      <td>-335.337433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Si-225</td>\n",
              "      <td>-338.835571</td>\n",
              "      <td>-335.252045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Si-226</td>\n",
              "      <td>-335.689014</td>\n",
              "      <td>-332.795410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Si-227</td>\n",
              "      <td>-333.706496</td>\n",
              "      <td>-330.945892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Si-228</td>\n",
              "      <td>-344.855640</td>\n",
              "      <td>-340.526062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Si-229</td>\n",
              "      <td>-344.811083</td>\n",
              "      <td>-340.426270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Si-230</td>\n",
              "      <td>-298.832226</td>\n",
              "      <td>-305.800995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Si-231</td>\n",
              "      <td>-298.965018</td>\n",
              "      <td>-306.721100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Si-232</td>\n",
              "      <td>-295.209438</td>\n",
              "      <td>-302.589966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Si-233</td>\n",
              "      <td>-291.862939</td>\n",
              "      <td>-299.753082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Si-234</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-335.037537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Si-235</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-335.037537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Si-236</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-335.037537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Si-237</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-335.367859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Si-238</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-335.367889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Si-239</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-335.367859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1255c17-3297-4be3-aa57-ff1ccee9374b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-e0984617-bb55-4781-b2d2-1c4206fc3e68\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0984617-bb55-4781-b2d2-1c4206fc3e68')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-e0984617-bb55-4781-b2d2-1c4206fc3e68 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1255c17-3297-4be3-aa57-ff1ccee9374b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1255c17-3297-4be3-aa57-ff1ccee9374b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(en_df['target'],en_df['prediction'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJDW6nbNSMyC",
        "outputId": "bc1ffbfe-4a26-45f6-d19b-a08b783a80c3"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.93375330843126"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_en=[]\n",
        "pred_en=[]\n",
        "actual_forces=[]\n",
        "pred_forces=[]\n",
        "\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    ase_atoms = atoms.ase_converter()\n",
        "    a = AtomicData.from_ase(ase_atoms,5)\n",
        "    data = AtomicData.to_AtomicDataDict(a)\n",
        "    tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "    data = tm(data)\n",
        "    out = model(data)\n",
        "    pen=out['total_energy'].squeeze().cpu().detach().numpy().tolist()\n",
        "    print(j,pen)\n",
        "    num_atoms=atoms.num_atoms\n",
        "    actual_en.append(j/num_atoms)\n",
        "    pred_en.append(pen/num_atoms)\n",
        "    actual_forces.append(np.array(k).flatten())\n",
        "    pf=out['forces'].squeeze().cpu().detach().numpy()\n",
        "    pred_forces.append(pf.flatten())\n",
        "    #break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyOyUFYQLfbr",
        "outputId": "8aa04941-b201-45da-ab56-c579b09e151d"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-297.62773938 -300.2811584472656\n",
            "-295.77170067 -296.49090576171875\n",
            "-291.28958206 -299.065673828125\n",
            "-296.24088456 -295.0887451171875\n",
            "-294.41361742 -296.1150817871094\n",
            "-334.75283939 -334.7198486328125\n",
            "-334.69215136 -334.9636535644531\n",
            "-184.71808052 -190.67774963378906\n",
            "-121.41180043 -126.26878356933594\n",
            "-338.93899696 -335.3374328613281\n",
            "-338.83557056 -335.2520446777344\n",
            "-335.68901422 -332.79541015625\n",
            "-333.7064957 -330.9458923339844\n",
            "-344.85564046 -340.52606201171875\n",
            "-344.81108268 -340.42626953125\n",
            "-298.83222646 -305.8009948730469\n",
            "-298.96501782 -306.7210998535156\n",
            "-295.20943762 -302.5899658203125\n",
            "-291.86293882 -299.7530822753906\n",
            "-344.74080048 -335.03753662109375\n",
            "-344.74080047 -335.03753662109375\n",
            "-344.74080046 -335.03753662109375\n",
            "-341.22165747 -335.36785888671875\n",
            "-341.22165734 -335.3678894042969\n",
            "-341.22165747 -335.36785888671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(actual_en,pred_en,'.')\n",
        "plt.xlabel('DFT energy(eV/atom)')\n",
        "plt.ylabel('FF energy(eV/atom)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "zedmdoAcNmHU",
        "outputId": "303a434d-d840-4ea3-8ff7-9f2a92f4b45d"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'FF energy(eV/atom)')"
            ]
          },
          "metadata": {},
          "execution_count": 172
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(actual_en,pred_en)#energy error MAE per atom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgJemMF5N3Wd",
        "outputId": "7cf28d57-d116-4531-ff20-4adc0fbfc86e"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08518824692146651"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_forces = np.concatenate(actual_forces)\n",
        "pred_forces = np.concatenate(pred_forces)"
      ],
      "metadata": {
        "id": "ms8fXomtOVsP"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(actual_forces,pred_forces,'.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "R8DdStiMOlM0",
        "outputId": "5f31587d-432a-42aa-89f3-3df7ed673c46"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7c98a123a1d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 147
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5WklEQVR4nO3de3SU9b3v8c8zUWKCZAwENEggIVWj5SKXmEIogroUZK8WYVFarbtyKN7wVt1HEntBd7tIvPRsl3fFfdienqo53UC1VnR7REVEJYIX0BMsl5gYIhDBCYY0wcxz/khnzMw8M5lJ8uR5JvN+rZUuM/NMnl9mUebD7/f9fX+GaZqmAAAAHOBxegAAACB1EUQAAIBjCCIAAMAxBBEAAOAYgggAAHAMQQQAADiGIAIAABxDEAEAAI45wekBxOL3+7V//34NGTJEhmE4PRwAABAH0zR19OhRjRw5Uh5P7DkPVweR/fv3Ky8vz+lhAACAHqivr9eoUaNiXuPqIDJkyBBJnb9IVlaWw6MBAADxaG5uVl5eXvBzPBZXB5HAckxWVhZBBACAJBNPWQXFqgAAwDEEEQAA4BiCCAAAcAxBBAAAOIYgAgAAHEMQAQAAjiGIAAAAxxBEAACAYwgiAADAMQQRAADgGIIIAAApqtHXqi17mtToa3VsDLYHkYaGBv30pz/VsGHDlJGRofHjx+u9996z+7YAACCGquo6lVZu1OWr31Vp5UZVVdc5Mg5bg8iRI0dUWlqqE088URs2bNAnn3yi3//+98rOzrbztgAAIIZGX6vK1+2Q3+z83m9Kd6zb6cjMiK2n7959993Ky8vTmjVrgo8VFBTYeUsAANCNfU0twRAS0GGaqm06plxvRr+OxdYZkeeff15Tp07VokWLNGLECE2aNEmrV6+285YAAKAbBTmD5TFCH0szDOXnZPb7WGwNInv37tWjjz6qM844Qy+//LKuu+463XTTTXrqqacsr29ra1Nzc3PIFwAA6Fu53gxVLBivNKMzjaQZhlYtGNfvsyGSZJimaXZ/Wc8MGjRIU6dO1ZYtW4KP3XTTTaqurtbbb78dcf2dd96pu+66K+Jxn8+nrKwsu4YJAEBKavS1qrbpmPJzMvs0hDQ3N8vr9cb1+W3rjEhubq7OOeeckMfOPvts1dVZV+aWl5fL5/MFv+rr6+0cHgAAKS3Xm6FphcMcmQkJsLVYtbS0VLt27Qp57NNPP9WYMWMsr09PT1d6erqdQwIAAC5i64zIL37xC73zzjtatWqVdu/eraefflpPPPGEli9fbudtAQBAkrA1iBQXF2v9+vV65plnNG7cOP32t7/V/fffryuuuMLO2wIAgCRha7FqbyVS7AIAANzBNcWqAAAAsRBEAACAYwgiAAAkATeclGsHW7fvAgCA3quqrgseUucxpIoF47W4eLTTw+oTzIgAAOBibjop1w4EEQAAXCzWSbkDAUEEAAAXc9NJuXYgiAAA4GJuOinXDhSrAgDgcouLR2vmmcNtOSnXaQQRAACSQK43Y0AFkACWZgAAgGMIIgAAwDEEEQAA4BiCCAAAcAxBBAAAOIYgAgAAHEMQAQAAjiGIAAAAxxBEAACAYwgiAADAMQQRAADgGIIIAABwDEEEAAA4hiACAAAcQxABAKSsRl+rtuxpUqOv1emhpKwTnB4AAABOqKquU/m6HfKbkseQKhaM1+Li0U4PK+UwIwIASDmNvtZgCJEkvyndsW4nMyMOIIgAAFLOvqaWYAgJ6DBN1TYdc2ZAKYwgAgBIKY2+Vh1uaZcR9niaYSg/J9ORMaUyakQAACmja12Ioc4vU50hZNWCccr1Zjg8wtRDEAEApITwuhBTnUWqD/54kqbkZxNCHMLSDAAgJVjVhfhNadjJ6YQQBxFEAAApoSBnsDxhhSHUhTiPIAIASAm53gxVLBivNKMzjVAX4g7UiAAAUsbi4tGaeeZw1TYdU35OJiHEBQgiAICk0uhr1b6mFhXkDO5RkMj1ZhBAXIQgAgBIGj1py97b4AJ7EUQAAEkhWlv2mWcOjxowOE/G/ShWBQAkhUTbsnOeTHIgiAAAkkKi2285TyY5EEQAAEkh0e230YJL5iCPtuxpYmbEJQzTNM3uL3NGc3OzvF6vfD6fsrKynB4OAMAFGn2tcW+/raqu0x3rdqrDNJVmGJo/aaTWv99AzYjNEvn8JogAAAa0QHDJHOTRZY9sCVmuSTMMbS6bzW6aPpbI53e/Lc1UVlbKMAzdcsst/XVLAACU683QtMJhamnvoGbEhfoliFRXV+vxxx/XhAkT+uN2AABE4KwZd7I9iHz99de64oortHr1amVnZ9t9OwAALHHWjDvZ3tBs+fLlmjdvni666CL97ne/i3ltW1ub2tragt83NzfbPTwAQBLpbZdUzppxH1uDyLPPPqvt27eruro6rusrKip011132TkkAECS6qsuqZw14y62Lc3U19fr5ptv1h//+EeddNJJcb2mvLxcPp8v+FVfX2/X8AAASYQuqQOXbTMi27Zt08GDBzV58uTgYx0dHdq0aZMeeughtbW1KS0tLeQ16enpSk9Pt2tIAIAkFatLKrMbyc22IHLhhRdqx44dIY8tWbJERUVFWrFiRUQIAQAgmsCOl/AeIOx4SX62BZEhQ4Zo3LhxIY8NHjxYw4YNi3gcAJC64ilADex46dollR0vA4Ptu2YAAIgmkQJUdrwMTLR4BwA4otHXqtLKjbRcH4Bc2eIdAICuYhWgInUQRAAAjqDlOiSCCADAIbRch0SxKgDAJvHshqEAFQQRAECfS2Q3DC3XUxtLMwCAmBp9rdqyp8mynbrVc33Zjj3WvTEwMCMCAIgq1sxGtOfeqz3cJ+3Y++qQO7gbQQQAYCnazMbMM4dLkuVzX7Ue190baiJ+lseQMgfFPwkf694s4wwsLM0AACzF6vMR7bnKDTURj0udQeKyR7aoqrqu1/fGwEIQAQBYitXnw+o5j6RYvboTqRWJp8cI9SMDA0EEAGApVp8Pq+dWzC2KCA/h4p3V6K7HSFV1nUorN+ry1e+qtHJj3DMtcB/OmgEAxNToa43a5yP8uarquuAJuR6jc4ak64dMomfJWN2bM2rcL5HPb4pVAQAxddfnw+wSNcIblG369FAwmPSkc6rVvWPVjxBEkg9BBACQkEDH1B0NPt39j+LU8O21gXBiR+fUQP1I+IwIZ9QkJ4IIACBuXXt7dBW+fTc8nPTlTEWgfqQ3My1wD2pEAABxsarNCGcYoTtn7KzdiFW7AmdRIwIA6HNWtRldeaR+rd3gjJqBge27AIC4WPX2CIi2fTfNMJQ5yGPZ74M+IJCYEQEAxMmqNuP2OWfp9FMyJEOaMiZbp2SeGPL8/EkjddkjWyJqRjhHBgHUiAAAEtK1NmPTp4ciAkVgl0zmIE8whASkGYbWXT/N8nH6gAwciXx+szQDAEhIrjdD0wqHSbI++E6SphUOU0t7h2XNSHXtEc6RQRBLMwAwgDX6WvVe7WEZhqEpY7L7dMahu8Zi0fp9FOdn0wcEQQQRABggAo3GCnIGB9utl63dEdJi/ervF2jJjALLVu1dXxvtsa6sgoZHCgaKaP0+JuZl0wcEQdSIAMAAEF78uWJukSpfrJHVX/DhxaFWhaOS4iomDQ87hqTKhaHXRuv3QR+QgSuRz2+CCAAkOatGY+EzFeECxaEHm/+u+Q9vCQksHnUeVBfPYXUcQAcrFKsCQAqxqtWIFUKkzlqONW/tiwghkuSXIh6LVkwaq04EiAdBBACSnFWjsTTDUPncIkXpPybDkJ7YtM9y6cZK19qPeO5N4SniRRABgCQXKApNMzoTQaD485rzC7Wl/AI99JNJmn/uyJDAkOii/M9nRha4xro3yzKIF7tmAGAAWFw8OthIrGvxZ643Qy3th/T8h/u7Xa6RpH+5+Ez9j1c+jag3WVJaYHl9o69VeUMzte76aTrW7k+o8LS7XTlIDQQRABggAh/m+5pagt9/WH9EZet2xD0DkpVxglbMLdI9G3Z1u7XWardNoNFZd2jxjgB2zQBAkrLqG9L1w/2ySadr3faGuOtAAgxJZXOLNGHUKVFnOHqzW4adNgNfIp/fzIgAgIvEWq7o+lz4GS8r5hbp7g01Ie3W125viHmvNMPQdbPG6uHX9oSEFVPS3Rtq9Fb5BVGDQXe7ZWItuXTXkRWphSACAC4Ra7mi63OGQrfX+k1FbV4Wzc0XfEc/LhmtfU0teui1PRHP+6WYwSBa+/aPPv9KVzz5Tswll2ivZadNamLXDAC4QKOv1fIAuUZfa8RzVoEj0eWXBzbu1vMf7rfcfitF364bYLVb5va5Z+nul2osf4fuXstOm9TFjAgAuECs5QpTZlw7XqTO/iDxVP6ZkiperJHMzlmLrgWthqSKheO7DQbhO3USWXKJtssHqYcgAgAu0N1yRXct2wMMU/rt/O/q13/+OK5ZkkAtyJayC/Tq/zugg81tuvDsEZqYlx3zdV3rVbrulElkySXXm0EAAUszAOAG0ZYrDjb/XS98tF/XzyoMeW7h5NMtu6b6Jf39uF9lc4vi+gs+UAuy6dND+s1zH+uBjbt12SNbVFVdF/U1VdV1Kq3cqMtXv6vSyo3Ba1lyQU+wfRcAXKTribT3vbwrZOdLSX62/nl6viaP6Zyt+L+fHNCvn/vY8ucEdtJ8ebRdT27eG3U2Jc0wtO76abrskS1xbaeNZ+stp+qC7bsAkKQCyxUf1h+J2H77bu0Rba09ogWTT9f69xssd9AE+E3png27tLlstpbMyNeat/bpyU375O9yTWDGoqW9I+7ajnjqQFhyQSIIIgDgQltrD1s+biq0P0isKe0O09TDG3dr0dRRuuPSc7SktEC1TceUOcgT0o690dcad20HW2/R16gRAQAX+urY8T75Of/73Tr98OEtuu3/fKBcb4amFQ7TxLzsYIHplj1NkhR3bQd1IOhrzIgAgMs0+lr1iEWTsd5Yu71B544+RRedfaplO/iKBeO1uWx2XLUdbL1FX7J1RqSiokLFxcUaMmSIRowYofnz52vXrl123hIAkt62z45EXXKx2ikTr1//+WOVVm7U42/ssWyeJknTCofFFSwCsyuEEPSWrUHkjTfe0PLly/XOO+/olVde0fHjx3XxxRerpaXFztsCQFJp9LXqhY/26y8fNqjR16pomxkvLBrRuySif7SD73ImTUDXc2KA/mTr0sxLL70U8v1//Md/aMSIEdq2bZtmzpxp560BIClUVdepbO2OkBmQ8rlFlrthNu46GNE11SNpxaVFavn7N3pg4+647mkVcwwjdkt3wC79Wqzq8/kkSUOHDrV8vq2tTc3NzSFfADBQBc6QCQ8GlRtqVHbptw3JPJK+f0aOZev2H5fk6QcTR/Z6piThw2qAPtJvxap+v1+33HKLSktLNW7cOMtrKioqdNddd/XXkADAEYH26HsOfW3ZaMyUVNPYrNU/m6LMQScqc5BHP3x4i+XPeubdej27tT7us2iiMSVt/+yI5k2g5gP9q986q1533XXasGGDNm/erFGjRlle09bWpra2tuD3zc3NysvLo7MqgAGj626VeMw8I0cXnTNCv3nukz65f7QGaIHnKheO1+Li0X1yL6SuRDqr9svSzA033KAXXnhBr732WtQQIknp6enKysoK+QKAgeLD+iMqSyCESNKmvzX1WQhJMwyVXVoU7AESzlTn7plGX2vUn9Hoa9WWPU0xrwESYevSjGmauvHGG7V+/Xq9/vrrKigosPN2AOCoRl+r3qs9LMMwNGVMdsjWVqui1N4KxIl4fqbHkFYtGKfFxaP1g4kjVdt0TH872BwRcqK1dpdk2XuE2RP0lq1BZPny5Xr66af13HPPaciQIfriiy8kSV6vVxkZrEMCGDisgsbd/1jmiFaU2lOGpHnjc7VsZoHe2XtYFRtqun3N+uuna2JedrA+pSBnsPJzMnXn85/E1a498DuE9x6ZeeZweomgV2xdmnn00Ufl8/k0a9Ys5ebmBr+qqqrsvC0A2MZqaaLR12o521G2dkfwg7+3xaRSZwA5d5RXpqQXdjTqske26LMv4+vLdKzdr6rqOpVWbtTlq99VaeVGbfr0UNzt2mMddgf0hu1LMwAwUDz+xh5VbqiRqdCliX1NLZazHaakdds/V9PX7X1yf1PSB5/7gt/7TenprfXdvs5jSJmDPJYzGpvLZsfV2p3D7mAXDr0DgDg8vmmPKv4RQqRvP8gbfa0qyBkc9XX3vvyp1rxV2y9jjObnMwpUf6Q16oxGPO3aOewOduHQOwDoRqOvVZUWdRgdpqlttUd630ysDxmS/rx8uuoPt+rtvV/qma11euLNfTIUuXU30RkNDruDHQgiANCNfU0tll1NJemmZ9/vk/qPvmKqsx5kSn52yNhMdQaRwPJKT2c0cr0ZBBD0KYIIAHTDqj4iwE0hRPp2lsOquNSU9OCPJ2nYyenMaMA1qBEBgG7kejO0dIb7+yB1neUIhKfw56fkZ3dbDwL0J4IIAMThnybkOj2EoNLCYcH/TjMMlc8t0jPLvqfNZbODDcYoLkWyYGkGALoR6CjqFlv2filJMgzp9rln6ZqZhZbXUVyKZMCMCICU1t3ZKeEdRd0gUDhrmtI9G3bFPPclnq25gJOYEQGQErq2Ng98KMdzdkpfdUW1S6yzYYBkQBABMOCFB46lMwr0vbFDQ9qyRzs7pSBncET/DTehuymSHUszAAY0q8PaVr+5T0uf2hYRLoINylzKY0gLJ58eswC1u6UmwG2YEQEwoCW6tHLDM+/ro8+/0pIZBcr1ZmjbZ0ccnQ3xGNKKOUWaMOqUYMHpv1xylmUBajxLTYDbEEQADAhWNSCStKPBF+NV1p54c5+eeHOfLh1/ml7c8UVfDjNuhqRlMwu0pLQgov7Dqrup1cyP1VIT4DYEEQBJL9pMQKOvVXdbnBETL6dCiCQ9dPkkzZswMu7rrWZ+KGRFMiCIAEhqVjMB5Wt3aHD6CcHvk40hafKY7IReY9WGnkJWJAOKVQG4SqLFllYzAX5JNzz9vm58+n03HYwbvx4Mmk6qSFbMiABwja5LLIaksrlFuuZ8666hAbEOpAs/cdYwFPUUXTcxTfVoSYVOqkhGzIgAcIXwJRZTUsWGGj2+aU/M14XPBIQzJT3w40l6Ztn3dNcPzunbQdukN0sqdFJFsiGIAHCFaNts795Q0+0yzeLi0dpcNlu//eF3LZ8/0tqu/JxM7fg88R00/SUQo1hSQaphaQaAK0TrYOqPc5ki15uhwhEnWz736z9/rJXPfezawtXLz8vTjReewZIKUhIzIgBcIdebobK5RRGPJ7JMEagXseLWECJJz26tlySWVJCSmBEB4BrXnF8oGZ3LMX6z+2WKQBOzwYPSVHf4mAzD0HWzCvXo63tcHTzC+dWz4lRgICCIAHCVa2YW6gcTR3a7TFFVXRdyaF0yo98HUhlBBIDjwtuzW7UwD78+GUNIoG37sMHpuuelXeowTRmSFhePcnpogGMIIgAclchBbYHAsr3O2YPoesJjSOuvn66Jef/omGpIlS/WyJT09NZ6PbO1XpULOaQOqYcgAsAx0Q5qyzl5kPY2tei8/KHBD+6ugSVZeNRZ/xGodQn8LoEzcLr+KqY6W9NzSB1SDUEEgGOiHdS29Kltwe8XTj5d/zxtjMrW7UiKrqgBsbbkRuuZQtEqUhFBBIBjYrVnD1i7vUFrtzf036D6gCHpxgvPiFrrEu339kgUrSLl0EcEgGPC27Mn5QF1YQxJlQvHx5zVCPzeXbvSG5IqunkdMBAxIwKgXzX6WrXtsyMyTVNT84dqcfFofXXsuCrDaiaS1UOXT9K8CSO7vS5wQN32z47INKUp+dmEEKQkggiAPhe+HTcgvPeHIans0qIBE0IMSZPHZMd9fa43Q/MmED6Q2ggiAPpUtO24gR0y4TtFKl6scWqofa7s0iJmNYAEUSMCoM9E244bmCFJpq23iTAklc8t0jUzC50eCpB0mBEB0GeibcetbToW1w6ZZPTreWfr0gm5zIQAPcSMCIA+Y3X6beAcleBOEWeGZptvTJMQAvQCQQRAnwnfjht+eu7WfYcHRFFqV/ds2KVGX6vTwwCSFkszAOISbSdMuMC21PCOoh/WH0m6xmTxCCw9MSsC9AxBBEC3Ej2YLtAnJNAltNHXqv/zXn1/DrnfBJaeAPQMQQRATFY7YcrX7VDRaUNCDnHb19SiHZ/7QnqCGJIWTD5d699vSJoi1cXFo1RV/XnMawx1bj0OX3oCkDiCCICYrHbC+E1p/sNbVLlwvCRFPRXXlJJuOeby80bLe9KJeuLNfTGve/jySZo8hm6oQG8RRADEFG3bbeDYeg2gLbkl+dmamJetEVkn6cnN+6L+XqakoYPTCSFAH2DXDICYAjthwrflSp3H1g+UECJJ9/9kkqTI3T/hqAsB+k6/BJGHH35Y+fn5Oumkk1RSUqKtW7f2x20B9JHFxaO1/vrpET1APJJlQElG5XND27MvLh6tzWWz9cyy76n80qKoW5IB9I7tSzNVVVW69dZb9dhjj6mkpET333+/LrnkEu3atUsjRoyw+/YA+sjEvGxVLhyvO9btVIdpBj+QJQUfS2YTRp0S8ViuN0O53gxNKxymH0wcGbElGUDvGaZp798eJSUlKi4u1kMPPSRJ8vv9ysvL04033qiysrKYr21ubpbX65XP51NWVpadwwQQp0Zfa/ADWZLeqz2sr1qPa2jmIP31o0a9uPMLh0eYOI8hvVV2AQED6COJfH7bOiPS3t6ubdu2qby8PPiYx+PRRRddpLfffjvi+ra2NrW1tQW/b25utnN4AHogMEtQVV2nsrU7kr5TqqHOviiEEMAZtgaRpqYmdXR06NRTTw15/NRTT1VNTeTR3xUVFbrrrrvsHBKAHuraWVVS0oeQBZNGavKYbF149qmEEMBBrtq+W15erltvvTX4fXNzs/Ly8hwcEYBGX6vWbN6n1W/uk6nOZYylMwqSOoSkGYb++5wiAgjgArYGkZycHKWlpenAgQMhjx84cECnnXZaxPXp6elKT0+3c0gAEtC1tXuA35RWd9Psy83Y9QK4i63bdwcNGqQpU6bo1VdfDT7m9/v16quvatq0aXbeGkAvhbd2T3bzzx2pZ5Z9T5vLZkc9JwdA/7N9aebWW2/Vz372M02dOlXnnXee7r//frW0tGjJkiV23xpADzX6WvXCR/sHTAjxSFoxl6UYwI1sDyKLFy/WoUOH9Jvf/EZffPGFzj33XL300ksRBawA3MFqOSaZsRQDuJvtfUR6gz4iQP9q9LWqtHLjgAghhqQHfzJJU/I5mA7ob67pIwIguVidtJuMDEOqXDBe/zRxpNNDAdANggiQYrr2AwnMFAQeGzwozfKk3WTz5+una2JettPDABAHggiQQrrWf3iMzo6ikkIem/Pd05KyTbvUWZRasXA8IQRIItSIAANYeDfU8PqPwMG5Xf8SMMK+TxYeQ1rPTAjgCtSIAIiY/fj5jIKIJRerwJGMIUTqXE461u53ehgAEmRrQzMAzghvRuY3pSff3CePEft1ycQI+13SDCN4IjCA5EEQAQYgq90vfkk/nzF2QISR2WcNV+WC8Ur7RxqhVwiQvFiaAQaggpzBEbtfPJIm5Hm1+p+naOlT2xwbW1+4emahphUO08wzh6u26ZjyczIJIUCSIogAA1CuN0MVC8brjnU71WGawQLUG55+3+mh9ZrHUHAJJtebQQABkhxBBBigFheP1swzh2tb7RHd9Oz7Sd8bJIAzY4CBhRoRIEk0+lq1ZU+TGn2tcb8m15uhoScPGhAhxCOpfG6RrplZ6PRQAPQhZkSAJGDViGzmmcOjdkjt+tjgQWkyDMm9HYO699sfflcXnXMqMyHAAEQQAVzuw/ojKlu7I9jfw29KZet2SGZn3Ue0Dqkr5hSp6es2/fvmfUkdQiSp/Rs/IQQYoAgigItVVdepbN2OiCZjXYOF31RIz5DAYxUbavpljP1haj7dUoGBihoRwKUCTcnimc0YCDUg0SycfDpt24EBjBkRwKWsmpJJ1ufDDBSLp4zS988arowTPaptOqap+dmEEGCAY0YEcKlAU7KuPIb05+XTVblwvJK5Qeq/XHym5eN/2v65bnj6fS37X9t08kknEEKAFEAQAVwq0JSsaxvzigWdR9wvLh6tf/3hdx0eYc+kGYYWThmlu8PClCGFnI1Tvm5HQluVASQnlmYAFws0JattOqbMQR61tHeo0deqXG+GJozyOj28hHmk4JkwXX+3L1vaIrq++k3pwVf/plULJjgzWAD9giACuFyuN0ObPj0U0Uckb2hynTTrMaT1108PWW4JtGhv9LVGnI0jSU9vrdeYnME0MQMGMJZmAJcL7J7pumxxx7qdGjwoLalO0l0xpyhqzUeuN0NLZxRYPnf3hhqWaIABjCACuJzV7pkO09TnR1pVsWB80oSRCaNOifn8f4sSRPymVNt0zIYRAXADggjgcla7Z6TOk3S37jvsqh4il5xzqh76yaSI8aYZRvDE3GhyvRkqn1sU8Xg8rwWQvAgigIsFzo5ZMaco4sPdlLR2e4Mj44rm5U8OqKX9m4jdPoEC1e5cc36hyi/99ndN5LUAkpNhmu49haK5uVler1c+n09ZWVlODwfoV4+/sUeVG2qC58n8YGKu/vxBo9PD6laaYWhz2WxJnUsq+TmZCQeJRl9rj18LwHmJfH6zawZwoTuf26n/ePuz4Pd+U0kRQqTO+pXapmOaVjisxyEisJsGwMBHEAEc1uhr1Xu1h2UYhqaMydZv//KJXtz5hdPD6pZHkt/icWo6ACSCIAL0k0C9R0HO4OC/9quq61S2NvJ0XbcL9AQ51u7XR59/pXte2qUO06SmA0DCCCJAP6iqrotoSDbzzOGuDyEeQ7puVqEefm1P8DFD0tIZBRqRdZJyvRmaVjhMPzh3JDUdAHqEXTOAzaI1JHuv9rCrQ4jUOdbj35ghO3ZMSavf3KfSyo2qqq6TpGAgIYQASBRBBLBZtIZkfzv4tTMDSoAh6cnNey17lQQCFV1PAfQGQQSwWbSGZA+8urv/B5Ogn5TkxWyYFtghAwA9RRABbNLoa9WWPU2SFNLgK1lcOu403XjBGYo1bHbIAOgtgghgg6rqOpVWbtTlq99VaeVGfXXsuO7/8USVFg51emhxe/njA5KkMou261JnISs7ZAD0FrtmgD5mVZxasaHG2UH1QGDZ5ZqZhZLZeQquX53/evn5zAItKS0ghADoNYII0MesilPd7rnl03XZI1tCxt112eWa8wvZogvAFizNAH0sWnFqLFPHnCKnKkhqK+dpYl52twfVsUUXgB049A6wgVs7phrq3Alz4wVnWAYKDpsD0Bc49A5wWNFpQ1wTQjySVswt0oRRp3QbMDhsDkB/I4gAfayquk5l63bYeg/jH/9jmp3/HS30/Hre2bp0Qi7hAoBrUSMC9EKgV0igu2hgx4ydC56B4BG4R7RbeSRCCADXY0YE6KGuB9kZRme/jfGne23fMRPPjzckVSwcTwgB4Hq2zYjU1tZq6dKlKigoUEZGhgoLC7Vy5Uq1t7fbdUvAFuGzHoHHuvYKMU2p4sUavfW3ppidSPuDIenPy6drcfFoZwcCAHGwbUakpqZGfr9fjz/+uL7zne9o586dWrZsmVpaWnTffffZdVukuEZfq/Y1taggZ3CfzAZ0nfXwGJ2t2hcXj47aK+TR1/do+exCPfTanl7fOxGB5ZrAttuJedn9en8A6CnbgsicOXM0Z86c4Pdjx47Vrl279OijjxJEYItooaGnrDqk3rFup2aeOVwFOYNlGIqoBfFLKjrNeqvaRWcP1//9f4d6PJ5oDElP/myKMgedyLZbAEmnX4tVfT6fhg6NftZGW1ubmpubQ76AeEQLDb05ot5q1qPrabPn5Uf+WU4zDEXrTGZHCJE6Z0KW/a9tqjvcQggBkHT6LYjs3r1bDz74oK655pqo11RUVMjr9Qa/8vLy+mt4SHLdhYaesOqQmmYY+ujzrzS9YqPe3Xc44jW3zzlLedm9DwOGpPIoh81Z6YvgBQBOSDiIlJWVyTCMmF81NaEHfDU0NGjOnDlatGiRli1bFvVnl5eXy+fzBb/q6+sT/42QkqKFht4cUZ/rzYhoe3773LN090s1UXeujMrOUEt7R4/vKUmXnzdaW8ov0DXnF+ruhePjbv3e2+AFAE5IuEbktttu01VXXRXzmrFjxwb/e//+/Zo9e7amT5+uJ554Iubr0tPTlZ6enuiQgGBouGPdTnWYpuVZKT2xuHi0Zp45PNj2vLsD7Uzz21DU0228z26t040Xfifk/s+8W6cHNu6O+breBi8AcELCQWT48OEaPnx4XNc2NDRo9uzZmjJlitasWSOPh/5psE94aOireonwtuexQsbhY22SFBKKEuWXtGZzre6Yd3bw/j8pGa2HXtsdcl/DkAyz8/q+Cl4A0N9sO/SuoaFBs2bN0pgxY/TUU08pLS0t+Nxpp50W18/g0Du4UVV1ncrX7pA/yvOBHTtfHTuuyg3Rl3Fi8RjSW2UXhASLquq6iBkfO4IXAPSWKw69e+WVV7R7927t3r1bo0aNCnnOxQf+At1aXDxaOScP0tKntlk+7zelsrU7Orf39vAeflOqbToWEi6izfgQQAAkM9vWSq666iqZpmn5BSSbrt1Vq6rr9PMoISTAlPXyza/nna0bZhd2e79o9R653gxNKxxG+AAwYHDWDFJed91YwxulmWbPZzqm5mdrYl62TFN6+HXr7qvUewBIJQQRpLTuurFaNUrrjWPtnZUl7R3WFSa/nnc2J+YCSClsY0HKiqcba3fbdRP10edfqdHXqn/fvC/iOY9ECAGQcggiSFnddWNt9LXqy6/bIhqlWYn3xN27N9Ro22dHLMPNj0vytK+phe6oAFIKSzNIWVaNxwJFol2XbAx9e7ptNMtmjNXqzXsjDsEL51fnDwq/ryHpmXfr9fS79X1yYB8AJAtmRJCyNn16KCQ4GIa0asE4SQpZsuluZSbNMDRvwmlxV7DmDQ1tHR+YcQm8nHNjAKQSZkSQkgL1IV2zg2FKM88cbrlkY6ozqITPeHjUGV7qDh+LeyfNsXZ/SE+Qpq//rhuf+SDkmsASEfUiAAY6ZkSQkqzChl+dTcSiHaBXNrcoZBbj6u+P1VvlF0iSbn72g4h7eKSIA+u69gcJ9ASZmj+0zw/sA4BkwYwIUlKs+pBoB+gtLh6tH0wcGdLZNHznTYBHUsXC8ZLU7UF8dh3YBwDJgCCClNTdh3/XpZPMQR61tHeo0dcacQBetO29xfnZwWLTeM6DsevAPgBwO4IIUlassBGwseaA/n3zvmDDs6UzCvTfZhQErynIGWxZO/Ju7RF9WH9EE/OyI8JLNPFeBwADCUEEKS3Xm6FNnx4K2apbNrdIp2SeqLK1ocWsflNa/eY+PfnmPlUu7Nxem+vN0KXjTtNfd3wR8bPfq+0MIgCA6AgiSGnhNR6mpIoNNTFfY6pze2/moDRNzR+qq2eOtQwiU/MJIQDQHYIIUlLgoLvDLe09auHuN6Ubn/kg2Hxs4eTTtXZ7Q/D5hZNPZzYEAOJgmGZ3vSCd09zcLK/XK5/Pp6ysLKeHgyTW9YTd8KWY3v4fIM0wtLlstg42/13v1R4JnrALAKkqkc9vZkSQlLoGi+4KPMNP2DXNb8NHrBASCCkeSTPOyNHmvzXJ6szcQPOxaYXDCCAAkCCCCJJOeLCIdS6L1Qm7Vi4/b7Sera4LzpJI33ZTXTG3SNfMLFSjr1XbPzuiG55+PyTAeAwpcxC9AQGgJ/jbE0nFKljEOpclWp+PcNmDTwxuwTXVZcbElO7ZsCu4rXfehJFaMPn0kNf6TemyR7aoqrquZ78UAKQwggiSilWwCCyNWLFq127lkdf2RF2m6frzG32tWv9+Q8Q1HFQHAD1DEEFSiXYOTLRzWQIdVINnxCjy/BePEbtWpOvPjzXDEisQAQCsEUSQdJbOKAiGkXjOZVlcPFqby2brmWXf01vlF6hy4bfBJM0wtGJuUdRZE48h3T73rJBOqtGu5aA6AEgcxapIGl2LVA1JV88s0JLSgoTbp1ud63JKxokh585c/N1T9dLOL+Q3pbs31OiUjBODnVS7nlETwEF1ANAz9BFBUmj0taq0cmPEabmby2bH/eHf3ZbfRl9r8NyZyx7ZEvNeXa891u7noDoA6II+IkgKifQCiVWkGk8AiGfLb2DWZMuepm7vxQF1ANA3CCJwRCK9QKRvazPCZyniqcmItuV35pnDLcNEb+4FAEgMxarod1bBoHztjphbX8N3vyRSk5Holt/e3AsAkBhmRNDvrIKBX9KazbW6Y97ZUV9nVWQaj57McPT0XgCAxDAjgn5XkDM4opeHJD25eW+3DcFyvRmaVjgsoWDQ0xmOntwLAJAYZkTQ73K9GVr2/QI98ea+kMf9puIqPk2kyDWAGQ4AcCeCCByxZEaBnty8L+GC0HiKXKMFFXa6AID7sDQDR/RkuSSeA++qqutUWrlRl69+V6WVGzmIDgBcjhkROKbrcknmII9a2juCocJqRqO7XiKJbtMFADiPIAJH5XoztOnTQyGt26XOQ+jCl152fO6LeH1gOafR16oXPtrfq6ZnAID+RxCBo8JnMbrmiK4zGpJUuaEm4vW3zz0rJMiEoxEZALgbQQSOslpu6Sowo9H09d9ldVnmiWkxQwiNyADA3QgicJRVs7GuAjMaX7a0WT7f9HWb5WtvvvA7+vF5owkhAOBy7JqBo8J3z3RlGArOaEwZkx3RBM0wpAuKRshj0R3tgVd3a9Onh+wZNACgzxBE4LjFxaO17vppkUHDVLA+JNebocqF44Ohw2NIlQvGa2JetioWjI8II6Yit/YCANyHpRm4Qkt7R0QNiF+hnVajdUddXDxamYPSdOMzH4S8nh0zAOB+BBG4QuD8ma5hxGrHS7TuqFPzhyZ8sB0AwHkszcBxjb5W/c/NoefOGFJCO156erAdAMBZzIjAUVXVdSpbuyNiWcYwvq0PiRcH2wFA8umXGZG2tjade+65MgxDH3zwQX/cEkkg0MzMaudu4CTeROV6MzStcBghBACSRL8Ekdtvv10jR47sj1shicRqZkZ9BwCkBtuDyIYNG/Rf//Vfuu++++y+FZJMoJlZOI+RWH0IACB52RpEDhw4oGXLlukPf/iDMjP51y1ChReYeiRdPbNAb5VdEDzoDgAwsNlWrGqapq666ipde+21mjp1qmpra7t9TVtbm9ravm3l3dzcbNfw4BIUmAJAakt4RqSsrEyGYcT8qqmp0YMPPqijR4+qvLw87p9dUVEhr9cb/MrLy0t0eEhCFJgCQOoyTNOMcfZppEOHDunLL7+Mec3YsWP1ox/9SH/5y19kdDlDpKOjQ2lpabriiiv01FNPRbzOakYkLy9PPp9PWVlZiQwTAAA4pLm5WV6vN67P74SDSLzq6upCllb279+vSy65RP/5n/+pkpISjRo1qtufkcgvgsQ1+lq1r6lFBTmDu52NSORaAEBqS+Tz27YakdGjQ4sNTz75ZElSYWFhXCEE9qqqrlP5uh3ym527VCoWjI9aIJrItQAAJIIW7yko0Egs0MPDb0Y/qTaRawEASFS/tXjPz8+XTatASJBVI7FoJ9Umci0AAIliRiQFWTUSi9bJ1OpajyFlDuKPDgCg9/g0SUGJnFQbfq3UuTxz2SNbVFVd129jBgAMTLbtmukL7JqxV6OvNe5GYh/WH9H8h7eEHFCXZhjaXDabJRoAQAhX7JqB++V6M+IOES3tHRGn5FIrAgDoLZZmEJdE6koAAIgXQQRxSaSuBACAeLE0g7g0+lqVNzRT666fpmPtfg6oAwD0CYIIumXVWXVa4TCnhwUAGABYmkFMdFYFANiJIIKYYnVWBQCgtwgiiIndMgAAOxFEEBO7ZQAAdqJYFd1aXDxaM88cHncXVgAA4kUQQVwS6cIKAEC8WJoBAACOIYgAAADHEEQAAIBjCCIAAMAxBBEAAOAYgggAAHAMQQQAADiGIAIAABxDEAEAAI4hiAAAAMcQRAAAgGMIIgAAwDEEEQAA4BiCCAAAcAxBBAAAOIYgAgAAHEMQAQAAjiGIAAAAxxBEAACAYwgiAADAMQQRAADgGIIIAABwDEEEAAA4hiACAAAcQxABAACOIYgAAADHEEQAAIBjCCIAAMAxBBEAAOAYW4PIX//6V5WUlCgjI0PZ2dmaP3++nbcDAABJ5gS7fvDatWu1bNkyrVq1ShdccIG++eYb7dy5067bAQCAJGRLEPnmm2908803695779XSpUuDj59zzjl23A4AACQpW5Zmtm/froaGBnk8Hk2aNEm5ubmaO3dutzMibW1tam5uDvkCAAADly1BZO/evZKkO++8U7/61a/0wgsvKDs7W7NmzdLhw4ejvq6iokJerzf4lZeXZ8fwAACASyQURMrKymQYRsyvmpoa+f1+SdIvf/lLLVy4UFOmTNGaNWtkGIb+9Kc/Rf355eXl8vl8wa/6+vre/XYAAMDVEqoRue2223TVVVfFvGbs2LFqbGyUFFoTkp6errFjx6quri7qa9PT05Wenp7IkAAAQBJLKIgMHz5cw4cP7/a6KVOmKD09Xbt27dKMGTMkScePH1dtba3GjBnTs5ECAIABx5ZdM1lZWbr22mu1cuVK5eXlacyYMbr33nslSYsWLbLjlgAAIAnZ1kfk3nvv1QknnKArr7xSra2tKikp0caNG5WdnW3XLQEAQJIxTNM0nR5ENM3NzfJ6vfL5fMrKynJ6OAAAIA6JfH5z1gwAAHAMQQQAADiGIAIAABxDEAEAAI5J2SDS6GvVlj1NavS1Oj0UAABSlm3bd92sqrpO5et2yG9KHkOqWDBei4tHOz0sAABSTsrNiDT6WoMhRJL8pnTHup3MjAAA4ICUCyL7mlqCISSgwzRV23TMmQEBAJDCUi6IFOQMlscIfSzNMJSfk+nMgAAASGEpF0RyvRmqWDBeaUZnGkkzDK1aME653gyHRwYAQOpJyWLVxcWjNfPM4aptOqb8nExCCAAADknJICJ1zowQQAAAcFbKLc0AAAD3IIgAAADHEEQAAIBjCCIAAMAxBBEAAOAYgggAAHAMQQQAADiGIAIAABxDEAEAAI4hiAAAAMcQRAAAgGNcfdaMaZqSpObmZodHAgAA4hX43A58jsfi6iBy9OhRSVJeXp7DIwEAAIk6evSovF5vzGsMM5644hC/36/9+/dryJAhMgzD6eF0q7m5WXl5eaqvr1dWVpbTw3EN3pfoeG+s8b5Y432xxvtizcn3xTRNHT16VCNHjpTHE7sKxNUzIh6PR6NGjXJ6GAnLysri/wwWeF+i472xxvtijffFGu+LNafel+5mQgIoVgUAAI4hiAAAAMcQRPpQenq6Vq5cqfT0dKeH4iq8L9Hx3ljjfbHG+2KN98Vasrwvri5WBQAAAxszIgAAwDEEEQAA4BiCCAAAcAxBBAAAOIYgYqO//vWvKikpUUZGhrKzszV//nynh+QqbW1tOvfcc2UYhj744AOnh+Oo2tpaLV26VAUFBcrIyFBhYaFWrlyp9vZ2p4fW7x5++GHl5+frpJNOUklJibZu3er0kBxXUVGh4uJiDRkyRCNGjND8+fO1a9cup4flKpWVlTIMQ7fccovTQ3GFhoYG/fSnP9WwYcOUkZGh8ePH67333nN6WJYIIjZZu3atrrzySi1ZskQffvih3nrrLV1++eVOD8tVbr/9do0cOdLpYbhCTU2N/H6/Hn/8cX388cf6t3/7Nz322GO64447nB5av6qqqtKtt96qlStXavv27Zo4caIuueQSHTx40OmhOeqNN97Q8uXL9c477+iVV17R8ePHdfHFF6ulpcXpoblCdXW1Hn/8cU2YMMHpobjCkSNHVFpaqhNPPFEbNmzQJ598ot///vfKzs52emjWTPS548ePm6effrr55JNPOj0U13rxxRfNoqIi8+OPPzYlme+//77TQ3Kde+65xywoKHB6GP3qvPPOM5cvXx78vqOjwxw5cqRZUVHh4Kjc5+DBg6Yk84033nB6KI47evSoecYZZ5ivvPKKef7555s333yz00Ny3IoVK8wZM2Y4PYy4MSNig+3bt6uhoUEej0eTJk1Sbm6u5s6dq507dzo9NFc4cOCAli1bpj/84Q/KzMx0ejiu5fP5NHToUKeH0W/a29u1bds2XXTRRcHHPB6PLrroIr399tsOjsx9fD6fJKXUn49oli9frnnz5oX8uUl1zz//vKZOnapFixZpxIgRmjRpklavXu30sKIiiNhg7969kqQ777xTv/rVr/TCCy8oOztbs2bN0uHDhx0enbNM09RVV12la6+9VlOnTnV6OK61e/duPfjgg7rmmmucHkq/aWpqUkdHh0499dSQx0899VR98cUXDo3Kffx+v2655RaVlpZq3LhxTg/HUc8++6y2b9+uiooKp4fiKnv37tWjjz6qM844Qy+//LKuu+463XTTTXrqqaecHpolgkgCysrKZBhGzK/AWr8k/fKXv9TChQs1ZcoUrVmzRoZh6E9/+pPDv4U94n1vHnzwQR09elTl5eVOD7lfxPu+dNXQ0KA5c+Zo0aJFWrZsmUMjh1stX75cO3fu1LPPPuv0UBxVX1+vm2++WX/84x910kknOT0cV/H7/Zo8ebJWrVqlSZMm6eqrr9ayZcv02GOPOT00Syc4PYBkctttt+mqq66Kec3YsWPV2NgoSTrnnHOCj6enp2vs2LGqq6uzc4iOife92bhxo95+++2Isw+mTp2qK664wrWJvafifV8C9u/fr9mzZ2v69Ol64oknbB6du+Tk5CgtLU0HDhwIefzAgQM67bTTHBqVu9xwww164YUXtGnTJo0aNcrp4Thq27ZtOnjwoCZPnhx8rKOjQ5s2bdJDDz2ktrY2paWlOThC5+Tm5oZ8/kjS2WefrbVr1zo0otgIIgkYPny4hg8f3u11U6ZMUXp6unbt2qUZM2ZIko4fP67a2lqNGTPG7mE6It735oEHHtDvfve74Pf79+/XJZdcoqqqKpWUlNg5REfE+75InTMhs2fPDs6geTypNWE5aNAgTZkyRa+++mpwq7vf79err76qG264wdnBOcw0Td14441av369Xn/9dRUUFDg9JMddeOGF2rFjR8hjS5YsUVFRkVasWJGyIUSSSktLI7Z3f/rpp679/CGI2CArK0vXXnutVq5cqby8PI0ZM0b33nuvJGnRokUOj85Zo0ePDvn+5JNPliQVFham9L/wGhoaNGvWLI0ZM0b33XefDh06FHwulWYDbr31Vv3sZz/T1KlTdd555+n+++9XS0uLlixZ4vTQHLV8+XI9/fTTeu655zRkyJBgzYzX61VGRobDo3PGkCFDImpkBg8erGHDhqV87cwvfvELTZ8+XatWrdKPfvQjbd26VU888YRrZ1kJIja59957dcIJJ+jKK69Ua2urSkpKtHHjRvfu44ajXnnlFe3evVu7d++OCGRmCh2QvXjxYh06dEi/+c1v9MUXX+jcc8/VSy+9FFHAmmoeffRRSdKsWbNCHl+zZk23S39IPcXFxVq/fr3Ky8v1r//6ryooKND999+vK664wumhWTLMVPpbDgAAuEpqLUIDAABXIYgAAADHEEQAAIBjCCIAAMAxBBEAAOAYgggAAHAMQQQAADiGIAIAABxDEAEAAI4hiAAAAMcQRAAAgGMIIgAAwDH/H3ZJKnlJlo0xAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_error(actual_forces,pred_forces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NICx2BStO1Az",
        "outputId": "8c23e4fb-9a11-4c0c-fcc6-2119acaf1b8a"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15627579211683"
            ]
          },
          "metadata": {},
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvbS6NaC7U0w",
        "outputId": "d23b14a8-d8e9-44f3-d41d-90ea6b84b736"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['edge_index', 'pos', 'cell', 'edge_cell_shift', 'pbc', 'atom_types', 'node_attrs', 'node_features', 'edge_vectors', 'edge_lengths', 'edge_embedding', 'edge_attrs', 'edge_features', 'edge_energy', 'atomic_energy', 'batch', 'total_energy', 'forces'])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out['total_energy'].squeeze().cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4QLSATpJmIz",
        "outputId": "a2686d54-6828-4229-cd67-3e700672096e"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.741132736206055"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out['forces'].squeeze().cpu().detach().numpy() #.tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFIOTBuOJqWu",
        "outputId": "a36b738e-427b-4df2-fa59-ad4c12d1f9a3"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.8503770e-05,  9.6802987e-06,  7.3572533e-06],\n",
              "       [-1.8491430e-05, -9.6678314e-06, -7.3700189e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7lR1ljg9Fum",
        "outputId": "c2e39bd2-a2dd-48d9-9564-78c6fff7da64"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37352"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ_0eoNe9j8d",
        "outputId": "9903f5e1-e35a-4553-9ce7-7782ea6dccfa"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_jit_bailout_depth': 2, '_jit_fusion_strategy': [('DYNAMIC', 3)], 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'allow_tf32': False, 'append': True, 'ase_args': {'format': 'extxyz'}, 'avg_num_neighbors': 27.008066177368164, 'batch_size': 1, 'chemical_symbol_to_type': {'Si': 0}, 'code_commits': {}, 'dataloader_num_workers': 0, 'dataset': 'ase', 'dataset_extra_fixed_fields': {'r_max': 5.0}, 'dataset_file_name': './Si_data/sitraj.xyz', 'dataset_seed': 123456, 'dataset_statistics_stride': 1, 'default_dtype': 'float32', 'device': 'cuda', 'e3nn_version': '0.5.1', 'early_stopping': None, 'early_stopping_kwargs': None, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_patiences': {'validation_loss': 100}, 'edge_eng_mlp_initialization': 'uniform', 'edge_eng_mlp_latent_dimensions': [32], 'edge_eng_mlp_nonlinearity': None, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'embed_initial_edge': True, 'end_of_batch_callbacks': [], 'end_of_epoch_callbacks': [], 'end_of_train_callbacks': [], 'env_embed_mlp_initialization': 'uniform', 'env_embed_mlp_latent_dimensions': [], 'env_embed_mlp_nonlinearity': None, 'env_embed_multiplicity': 8, 'equivariance_test': 1, 'exclude_keys': [], 'final_callbacks': [], 'grad_anomaly_mode': False, 'init_callbacks': [], 'irreps_edge_sh': '1x0e+1x1o', 'l_max': 1, 'latent_mlp_initialization': 'uniform', 'latent_mlp_latent_dimensions': [128], 'latent_mlp_nonlinearity': 'silu', 'latent_resnet': True, 'learning_rate': 0.002, 'log_batch_freq': 10, 'log_epoch_freq': 1, 'loss_coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}, 'lr_scheduler_factor': 0.5, 'lr_scheduler_kwargs': {'cooldown': 0, 'eps': 1e-08, 'factor': 0.5, 'min_lr': 0, 'mode': 'min', 'patience': 50, 'threshold': 0.0001, 'threshold_mode': 'rel', 'verbose': False}, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 50, 'max_epochs': 100, 'max_gradient_norm': inf, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'metrics_key': 'validation_loss', 'model_builders': ['allegro.model.Allegro', 'PerSpeciesRescale', 'ForceOutput', 'RescaleEnergyEtc'], 'model_debug_mode': False, 'n_train': 50, 'n_val': 10, 'nequip_version': '0.5.6', 'nonscalars_include_parity': True, 'num_layers': 1, 'num_types': 1, 'optimizer_kwargs': {'amsgrad': False, 'betas': (0.9, 0.999), 'capturable': False, 'eps': 1e-08, 'foreach': None, 'maximize': False, 'weight_decay': 0.0}, 'optimizer_name': 'Adam', 'optimizer_params': {'amsgrad': False, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0}, 'parity': 'o3_full', 'r_max': 5.0, 'report_init_validation': True, 'root': 'results/silicon-tutorial', 'run_id': 'bh3o27yv', 'run_name': 'si', 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'seed': 123456, 'shuffle': True, 'torch_version': '1.12.1+cu102', 'train_idcs': tensor([28045, 23299, 27958, 15848, 28572, 16227, 26309, 20556,  8247, 11326,\n",
              "        10229, 18453, 21268, 21200, 27260,  8500, 29063,  3917, 34709, 31459,\n",
              "        33334, 24133,  8430, 25716,  3258, 12116, 16166, 13173, 30094,  1264,\n",
              "        16616,  5969, 11209, 11514,  1183,   935, 29944, 24324, 23657, 34168,\n",
              "        32145, 25833,  6923, 29876, 31671, 13592, 18533, 31474,  1024, 26994]), 'train_on_keys': ['forces', 'total_energy'], 'train_val_split': 'random', 'two_body_latent_mlp_initialization': 'uniform', 'two_body_latent_mlp_latent_dimensions': [32, 64, 128], 'two_body_latent_mlp_nonlinearity': 'silu', 'type_names': ['Si'], 'use_ema': True, 'val_idcs': tensor([24842, 20434, 17763, 17511, 17148,  2219,  1916,  1688,  9305, 32557]), 'validation_batch_size': 5, 'var_num_neighbors': 1.911603569984436, 'verbose': 'info', 'wandb': True, 'wandb_project': 'allegro-tutorial'}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqfWeB-TIJXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}