{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwwBmEA/S7zEcuW4KQFNMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knc6/jarvis-tools-notebooks/blob/master/jarvis-tools-notebooks/Train_MLFF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install wandb\n",
        "!pip install wandb\n",
        "\n",
        "# install nequip\n",
        "!pip install nequip==0.5.5 torch==1.11\n",
        "\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "\n",
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJl9OzZZKWh_",
        "outputId": "8a3a6c4b-fcbb-481b-bd1d-cb84e8967b98"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.28.1-py2.py3-none-any.whl (214 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.7/214.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=7e3076af0c06c4fbf0ceae693911d668c6209cc392a477a03f4f78043f9b12d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.32 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.28.1 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n",
            "Collecting nequip==0.5.5\n",
            "  Downloading nequip-0.5.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.11\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (1.22.4)\n",
            "Collecting ase (from nequip==0.5.5)\n",
            "  Downloading ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (4.65.0)\n",
            "Collecting e3nn<0.6.0,>=0.3.5 (from nequip==0.5.5)\n",
            "  Downloading e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip==0.5.5) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip==0.5.5)\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip==0.5.5)\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting scikit-learn<=1.0.1 (from nequip==0.5.5)\n",
            "  Downloading scikit-learn-1.0.1.tar.gz (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 28 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.97 KiB | 23.99 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nequip>=0.5.3 (from mir-allegro==0.2.0)\n",
            "  Downloading nequip-0.5.6-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.22.4)\n",
            "Collecting ase (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.65.0)\n",
            "Collecting torch!=1.9.0,<1.13,>=1.10.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting e3nn<0.6.0,>=0.4.4 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27439 sha256=9dd4197683a0bfd82447a42968dd5201fad9cd171b5a8428ff447fe458025647\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x82r3yrm/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: torch-runstats, torch, torch-ema, opt-einsum-fx, e3nn, ase, nequip, mir-allegro\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ase-3.22.1 e3nn-0.5.1 mir-allegro-0.2.0 nequip-0.5.6 opt-einsum-fx-0.1.4 torch-1.12.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruL2JdaB5hbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r1_3LmhI8y3",
        "outputId": "e2a6db8d-c350-4f3b-b657-db4f1719a0c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 13:35:25--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ni/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/test.json [following]\n",
            "--2023-07-22 13:35:26--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 611126 (597K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 596.80K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-07-22 13:35:26 (87.9 MB/s) - ‘test.json’ saved [611126/611126]\n",
            "\n",
            "--2023-07-22 13:35:26--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Cu/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/test.json [following]\n",
            "--2023-07-22 13:35:26--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 608400 (594K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 594.14K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:27 (55.0 MB/s) - ‘test.json’ saved [608400/608400]\n",
            "\n",
            "--2023-07-22 13:35:27--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Mo/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/test.json [following]\n",
            "--2023-07-22 13:35:27--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 250629 (245K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 244.75K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-22 13:35:27 (72.7 MB/s) - ‘test.json’ saved [250629/250629]\n",
            "\n",
            "--2023-07-22 13:35:27--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ge/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/test.json [following]\n",
            "--2023-07-22 13:35:28--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 311848 (305K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 304.54K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-22 13:35:28 (114 MB/s) - ‘test.json’ saved [311848/311848]\n",
            "\n",
            "--2023-07-22 13:35:28--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Si/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/test.json [following]\n",
            "--2023-07-22 13:35:28--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 309803 (303K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 302.54K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2023-07-22 13:35:29 (63.5 MB/s) - ‘test.json’ saved [309803/309803]\n",
            "\n",
            "--2023-07-22 13:35:29--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Li/test.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/test.json [following]\n",
            "--2023-07-22 13:35:29--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 279684 (273K) [text/plain]\n",
            "Saving to: ‘test.json’\n",
            "\n",
            "test.json           100%[===================>] 273.13K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-22 13:35:29 (86.8 MB/s) - ‘test.json’ saved [279684/279684]\n",
            "\n",
            "allegro       Ge_test.json  Mo_test.json  sample_data\n",
            "Cu_test.json  Li_test.json  Ni_test.json  Si_test.json\n"
          ]
        }
      ],
      "source": [
        "# get data from mlearn\n",
        "#!rm *.json\n",
        "!for m in Ni Cu Mo Ge Si Li; do wget https://github.com/materialsvirtuallab/mlearn/raw/master/data/${m}/test.json; mv test.json ${m}_test.json; done;\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data from mlearn\n",
        "#!rm *.json\n",
        "!for m in Ni Cu Mo Ge Si Li; do wget https://github.com/materialsvirtuallab/mlearn/raw/master/data/${m}/training.json; mv training.json ${m}_training.json; done;\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soGwt8gCJUx4",
        "outputId": "c1ee3535-8b74-41ea-91f9-382de6c1b4c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-22 13:35:30--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ni/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/training.json [following]\n",
            "--2023-07-22 13:35:30--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ni/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5289402 (5.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   5.04M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-07-22 13:35:31 (184 MB/s) - ‘training.json’ saved [5289402/5289402]\n",
            "\n",
            "--2023-07-22 13:35:31--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Cu/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/training.json [following]\n",
            "--2023-07-22 13:35:31--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Cu/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5246466 (5.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   5.00M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-22 13:35:32 (326 MB/s) - ‘training.json’ saved [5246466/5246466]\n",
            "\n",
            "--2023-07-22 13:35:32--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Mo/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/training.json [following]\n",
            "--2023-07-22 13:35:32--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Mo/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2127325 (2.0M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.03M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:33 (182 MB/s) - ‘training.json’ saved [2127325/2127325]\n",
            "\n",
            "--2023-07-22 13:35:33--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Ge/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/training.json [following]\n",
            "--2023-07-22 13:35:33--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Ge/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2796663 (2.7M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.67M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:34 (219 MB/s) - ‘training.json’ saved [2796663/2796663]\n",
            "\n",
            "--2023-07-22 13:35:34--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Si/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/training.json [following]\n",
            "--2023-07-22 13:35:34--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Si/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2669946 (2.5M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.55M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:35 (171 MB/s) - ‘training.json’ saved [2669946/2669946]\n",
            "\n",
            "--2023-07-22 13:35:35--  https://github.com/materialsvirtuallab/mlearn/raw/master/data/Li/training.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/training.json [following]\n",
            "--2023-07-22 13:35:35--  https://raw.githubusercontent.com/materialsvirtuallab/mlearn/master/data/Li/training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2446522 (2.3M) [text/plain]\n",
            "Saving to: ‘training.json’\n",
            "\n",
            "training.json       100%[===================>]   2.33M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-22 13:35:36 (185 MB/s) - ‘training.json’ saved [2446522/2446522]\n",
            "\n",
            "allegro\t\t  Ge_training.json  Mo_training.json  Si_test.json\n",
            "Cu_test.json\t  Li_test.json\t    Ni_test.json      Si_training.json\n",
            "Cu_training.json  Li_training.json  Ni_training.json\n",
            "Ge_test.json\t  Mo_test.json\t    sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymatgen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O3hJ4WVJDyf",
        "outputId": "9266397a-fd59-4aef-a45e-81405c210708"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymatgen\n",
            "  Downloading pymatgen-2023.7.20-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.7.1)\n",
            "Collecting monty>=3.0.2 (from pymatgen)\n",
            "  Downloading monty-2023.5.8-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mp-api>=0.27.3 (from pymatgen)\n",
            "  Downloading mp_api-0.33.3-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.1)\n",
            "Requirement already satisfied: numpy>=1.20.1 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.22.4)\n",
            "Requirement already satisfied: palettable>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (3.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.5.3)\n",
            "Requirement already satisfied: plotly>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (5.13.1)\n",
            "Collecting pybtex (from pymatgen)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.10.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pymatgen) (2.27.1)\n",
            "Collecting ruamel.yaml>=0.17.0 (from pymatgen)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.10.1)\n",
            "Collecting spglib>=2.0.2 (from pymatgen)\n",
            "  Downloading spglib-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (515 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.11.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from pymatgen) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pymatgen) (4.65.0)\n",
            "Collecting uncertainties>=3.1.4 (from pymatgen)\n",
            "  Downloading uncertainties-3.1.7-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.4/98.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pymatgen) (1.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.5->pymatgen) (2.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (67.7.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from mp-api>=0.27.3->pymatgen) (4.7.1)\n",
            "Collecting emmet-core>=0.54.0 (from mp-api>=0.27.3->pymatgen)\n",
            "  Downloading emmet_core-0.63.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.5.0->pymatgen) (8.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pymatgen) (3.4)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.0->pymatgen)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from uncertainties>=3.1.4->pymatgen) (0.18.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pymatgen) (2022.7.1)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.10/dist-packages (from pybtex->pymatgen) (6.0.1)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex->pymatgen)\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pybtex->pymatgen) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->pymatgen) (1.3.0)\n",
            "Installing collected packages: uncertainties, spglib, ruamel.yaml.clib, monty, latexcodec, ruamel.yaml, pybtex, emmet-core, mp-api, pymatgen\n",
            "Successfully installed emmet-core-0.63.0 latexcodec-2.0.1 monty-2023.5.8 mp-api-0.33.3 pybtex-0.24.0 pymatgen-2023.7.20 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 spglib-2.0.2 uncertainties-3.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FqDN7vTJ8ds",
        "outputId": "ffa1c5e5-28c9-4ebe-a124-085e9e59ef66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'allegro' already exists and is not an empty directory.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nequip>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from mir-allegro==0.2.0) (0.5.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.22.4)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (3.22.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.9.0,<1.13,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: e3nn<0.6.0,>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.5.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: torch-runstats>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: torch-ema>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.10.1)\n",
            "Requirement already satisfied: opt-einsum-fx>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (0.1.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27439 sha256=60dd7bf49ae0a56b9bd1d8099e07d5c17c47d4807275e4df424c63bc088e3b4e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fd3p20ar/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: mir-allegro\n",
            "  Attempting uninstall: mir-allegro\n",
            "    Found existing installation: mir-allegro 0.2.0\n",
            "    Uninstalling mir-allegro-0.2.0:\n",
            "      Successfully uninstalled mir-allegro-0.2.0\n",
            "Successfully installed mir-allegro-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jarvis-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_vtPWpNY7Tl",
        "outputId": "451cbf04-9271-40b5-9635-8c3b1e8f16b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jarvis-tools\n",
            "  Downloading jarvis_tools-2023.5.26-py2.py3-none-any.whl (974 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/974.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/974.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (3.7.1)\n",
            "Requirement already satisfied: spglib>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (2.0.2)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (2.27.1)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (0.12.0)\n",
            "Collecting xmltodict>=0.11.0 (from jarvis-tools)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools) (4.65.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (4.41.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->jarvis-tools) (1.16.0)\n",
            "Installing collected packages: xmltodict, jarvis-tools\n",
            "Successfully installed jarvis-tools-2023.5.26 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from jarvis.core.atoms import pmg_to_atoms\n",
        "from jarvis.db.jsonutils import dumpjson\n",
        "from jarvis.db.jsonutils import loadjson\n",
        "from monty.serialization import loadfn, MontyEncoder, MontyDecoder\n",
        "from ase.stress import voigt_6_to_full_3x3_stress\n",
        "# Ref: https://github.com/materialsvirtuallab/mlearn\n",
        "data = loadfn(\n",
        "    \"Si_training.json\",\n",
        "    cls=MontyDecoder,\n",
        ")\n",
        "train_structures = [d[\"structure\"] for d in data]\n",
        "train_energies = [d[\"outputs\"][\"energy\"] for d in data]\n",
        "train_forces = [d[\"outputs\"][\"forces\"] for d in data]\n",
        "train_stresses = [d[\"outputs\"][\"virial_stress\"] for d in data]\n",
        "\n",
        "\n",
        "data = loadfn(\n",
        "    \"Si_test.json\",\n",
        "    cls=MontyDecoder,\n",
        ")\n",
        "test_structures = [d[\"structure\"] for d in data]\n",
        "test_energies = [d[\"outputs\"][\"energy\"] for d in data]\n",
        "test_forces = [d[\"outputs\"][\"forces\"] for d in data]\n",
        "test_stresses = [d[\"outputs\"][\"virial_stress\"] for d in data]\n",
        "\n"
      ],
      "metadata": {
        "id": "uQcYsDbgJIiC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "os.makedirs('Si_data')"
      ],
      "metadata": {
        "id": "huyOqxYmY5G7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"Si_data/sitraj.xyz\", \"w\")\n",
        "mem = []\n",
        "count = 0\n",
        "line = \"\"\n",
        "for i, j, k, l in zip(\n",
        "    train_structures, train_energies, train_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    #print(atoms)\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    #info[\"stresses\"] = l\n",
        "    #info[\"stresses\"] = voigt_6_to_full_3x3_stress(l)\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(l).flatten()))\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "for i, j, k, l in zip(\n",
        "    test_structures, test_energies, test_forces, train_stresses\n",
        "):\n",
        "    info = {}\n",
        "    count += 1\n",
        "    info[\"jid\"] = str(count)\n",
        "    atoms = pmg_to_atoms(i)\n",
        "    info[\"atoms\"] = atoms.to_dict()\n",
        "    info[\"total_energy\"] = j / atoms.num_atoms\n",
        "    info[\"forces\"] = k\n",
        "    #info[\"stresses\"] = l\n",
        "    #info[\"stresses\"] = voigt_6_to_full_3x3_stress(l)\n",
        "    info[\"stresses\"] = voigt_6_to_full_3x3_stress(l).tolist()\n",
        "    mem.append(info)\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(j)\n",
        "        + ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(l).flatten()))\n",
        "        + \" \".join(map(str, np.array(info[\"stresses\"]).flatten()))\n",
        "        + '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(j)\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, k):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "f.close()\n",
        "dumpjson(data=mem, filename=\"Si_data/id_prop.json\")\n"
      ],
      "metadata": {
        "id": "e3Vk8wEOZOdg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!rm -rf ./results\n",
        "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiIbyRqIZtIW",
        "outputId": "6d2f3951-55a1-42f0-e42b-86f616f2d907"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230722_133612-bh3o27yv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msi\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-26092402475183654/allegro-tutorial?apiKey=5e4262c24ac1ed30e3b7b22a99893c835272cdb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-26092402475183654/allegro-tutorial/runs/bh3o27yv?apiKey=5e4262c24ac1ed30e3b7b22a99893c835272cdb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Do NOT share these links with anyone. They can be used to claim your runs.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[2139211, 1], batch=[2139211], cell=[34980, 3, 3], edge_cell_shift=[57841530, 3], edge_index=[2, 57841530], forces=[2139211, 3], free_energy=[34980], pbc=[34980, 3], pos=[2139211, 3], ptr=[34981], stress=[34980, 3, 3], total_energy=[34980, 1])\n",
            "    processed data size: ~1629.18 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(34980)...\n",
            "Replace string dataset_forces_rms to 0.9137988090515137\n",
            "Replace string dataset_per_atom_total_energy_mean to -5.076940059661865\n",
            "Atomic outputs are scaled by: [Si: 0.913799], shifted by [Si: -5.076940].\n",
            "Replace string dataset_forces_rms to 0.9137988090515137\n",
            "Initially outputs are globally scaled by: 0.9137988090515137, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:276: UserWarning: operator() profile_node %884 : int[] = prim::profile_ivalue(%882)\n",
            " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:108.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Equivariance test passed; equivariance errors:\n",
            "   Errors are in real units, where relevant.\n",
            "   Please note that the large scale of the typical\n",
            "   shifts to the (atomic) energy can cause\n",
            "   catastrophic cancellation and give incorrectly\n",
            "   the equivariance error as zero for those fields.\n",
            "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field free_energy          -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field stress               -> max error=0.000e+00\n",
            "   node permutation equivariance of field forces                     -> max error=1.974e-07\n",
            "   edge & node permutation invariance for field pbc                  -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field total_energy         -> max error=6.104e-05\n",
            "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_embedding             -> max error=2.831e-07\n",
            "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_features              -> max error=1.192e-07\n",
            "   edge permutation equivariance of field edge_energy                -> max error=1.043e-07\n",
            "   node permutation equivariance of field atomic_energy              -> max error=4.768e-07\n",
            "   node permutation equivariance of field batch                      -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=4.678e-07\n",
            "   (parity_k=0, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=7.477e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=2.147e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=4.686e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=6.557e-07\n",
            "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=forces              )     -> max error=1.570e-06\n",
            "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=9.413e-07\n",
            "   (parity_k=0, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=7.423e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=3.855e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=6.437e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=7.451e-07\n",
            "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=forces              )     -> max error=2.282e-06\n",
            "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=4.721e-07\n",
            "   (parity_k=1, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=7.423e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.839e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=4.590e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=5.066e-07\n",
            "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=forces              )     -> max error=1.730e-06\n",
            "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=9.385e-07\n",
            "   (parity_k=1, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=8.649e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=2.132e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=4.590e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=forces              )     -> max error=1.300e-06\n",
            "Number of weights: 37352\n",
            "Number of trainable weights: 37352\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2         1.09        0.836        0.251         0.54        0.835         26.2        0.414\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.410    0.002         0.82        0.212         1.03        0.539        0.828         22.9         0.36\n",
            "Wall time: 4.4110590350001075\n",
            "! Best model        0    1.032\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10        0.383        0.265        0.118         0.37        0.471         20.1        0.314\n",
            "      1    20        0.275        0.202       0.0727        0.329        0.411         15.5        0.246\n",
            "      1    30        0.337        0.274       0.0631        0.381        0.478         14.7        0.229\n",
            "      1    40        0.189         0.11       0.0798        0.239        0.302         16.3        0.258\n",
            "      1    50        0.496        0.419       0.0774        0.481        0.591           16        0.254\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2        0.366        0.274        0.092        0.327        0.478         17.4        0.274\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   16.587    0.002        0.429        0.133        0.561        0.399        0.605         18.7        0.297\n",
            "! Validation          1   16.587    0.002        0.263       0.0874        0.351        0.314        0.469         16.7        0.262\n",
            "Wall time: 16.587881341000184\n",
            "! Best model        1    0.351\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10       0.0777       0.0703      0.00735        0.194        0.242         4.94       0.0784\n",
            "      2    20        0.759        0.522        0.237         0.47         0.66         28.5        0.445\n",
            "      2    30        0.365        0.322       0.0425         0.41        0.519         11.9        0.188\n",
            "      2    40        0.818        0.621        0.197        0.544         0.72         25.9        0.405\n",
            "      2    50        0.331        0.145        0.186        0.274        0.348         25.2        0.394\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2        0.276        0.213       0.0625        0.289        0.422           14         0.22\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   17.911    0.002        0.223       0.0815        0.305        0.292        0.436         14.8        0.234\n",
            "! Validation          2   17.911    0.002          0.2       0.0632        0.264        0.276        0.409         14.2        0.223\n",
            "Wall time: 17.912243662000037\n",
            "! Best model        2    0.264\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10       0.0842       0.0627       0.0215        0.183        0.229         8.43        0.134\n",
            "      3    20        0.159       0.0583          0.1        0.174        0.221         18.2        0.289\n",
            "      3    30         0.19        0.187      0.00277        0.312        0.396         3.08       0.0481\n",
            "      3    40       0.0748       0.0525       0.0224        0.167        0.209         8.61        0.137\n",
            "      3    50        0.319        0.221       0.0988        0.248        0.429         18.4        0.287\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2        0.234        0.168        0.065        0.261        0.375         14.4        0.226\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   19.215    0.002        0.179       0.0707         0.25        0.268        0.391         13.3        0.212\n",
            "! Validation          3   19.215    0.002        0.158       0.0613        0.219        0.249        0.363         13.7        0.216\n",
            "Wall time: 19.215972834000013\n",
            "! Best model        3    0.219\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10       0.0661       0.0512       0.0149        0.164        0.207         7.03        0.112\n",
            "      4    20         0.37        0.212        0.158        0.328        0.421         22.9        0.364\n",
            "      4    30        0.253         0.25      0.00283        0.353        0.457         3.06       0.0486\n",
            "      4    40        0.227        0.168       0.0593        0.298        0.375         14.2        0.223\n",
            "      4    50        0.125        0.119      0.00565        0.245        0.315          4.4       0.0687\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2         0.19        0.133       0.0565        0.235        0.334         13.4        0.211\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   20.529    0.002        0.145       0.0494        0.194        0.245        0.351         10.8        0.173\n",
            "! Validation          4   20.529    0.002        0.131       0.0537        0.184        0.227         0.33         12.9        0.202\n",
            "Wall time: 20.530231636000053\n",
            "! Best model        4    0.184\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0644       0.0466       0.0178        0.158        0.197         7.69        0.122\n",
            "      5    20       0.0228       0.0115       0.0114       0.0373       0.0979         1.17       0.0975\n",
            "      5    30        0.258        0.129         0.13        0.261        0.328         20.7        0.329\n",
            "      5    40        0.233        0.168       0.0653        0.298        0.374         14.7        0.234\n",
            "      5    50        0.171     6.53e-11        0.171     4.53e-06     7.38e-06         24.2        0.377\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2        0.159        0.123       0.0361        0.225        0.321         10.7        0.169\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   21.817    0.002        0.126       0.0426        0.169        0.232        0.328         9.94        0.158\n",
            "! Validation          5   21.817    0.002        0.119       0.0359        0.155        0.218        0.315         10.7        0.167\n",
            "Wall time: 21.818174574000068\n",
            "! Best model        5    0.155\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10       0.0373       0.0312      0.00611        0.133        0.161         4.57       0.0714\n",
            "      6    20        0.151         0.11       0.0404        0.245        0.304         11.8        0.184\n",
            "      6    30        0.297        0.197          0.1        0.333        0.405         18.5        0.289\n",
            "      6    40       0.0658       0.0532       0.0126        0.172        0.211         6.47        0.103\n",
            "      6    50       0.0921       0.0771        0.015        0.147        0.254         7.15        0.112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2         0.15        0.119       0.0314        0.221        0.315         9.89        0.156\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   23.111    0.002        0.114       0.0305        0.145        0.222        0.311         8.88         0.14\n",
            "! Validation          6   23.111    0.002        0.116       0.0308        0.147        0.215        0.312         9.59         0.15\n",
            "Wall time: 23.112080949000074\n",
            "! Best model        6    0.147\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10       0.0119      0.00907      0.00287       0.0368        0.087        0.587       0.0489\n",
            "      7    20         0.23        0.188       0.0414        0.325        0.396         11.9        0.186\n",
            "      7    30       0.0414         0.04      0.00138        0.144        0.183         2.14       0.0339\n",
            "      7    40        0.263        0.244       0.0186        0.361        0.451         7.98        0.125\n",
            "      7    50        0.113        0.082       0.0309        0.211        0.262         10.3        0.161\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2        0.151         0.13       0.0213        0.229        0.329         7.99        0.126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   24.622    0.002        0.119       0.0197        0.139        0.225        0.318         6.53        0.105\n",
            "! Validation          7   24.622    0.002        0.123       0.0213        0.144        0.219         0.32          7.8        0.122\n",
            "Wall time: 24.62356043700015\n",
            "! Best model        7    0.144\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10        0.169        0.144       0.0241        0.275        0.347         8.94        0.142\n",
            "      8    20       0.0409       0.0374      0.00345        0.142        0.177         3.38       0.0537\n",
            "      8    30        0.259        0.187       0.0717        0.311        0.396         15.7        0.245\n",
            "      8    40       0.0587       0.0516      0.00712        0.162        0.207         4.86       0.0771\n",
            "      8    50       0.0501       0.0433      0.00673         0.15         0.19         4.72        0.075\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2        0.142         0.12       0.0222        0.221        0.317         8.14        0.128\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   26.383    0.002        0.109       0.0229        0.132        0.218        0.304          7.3        0.116\n",
            "! Validation          8   26.383    0.002        0.116       0.0222        0.138        0.214        0.311         7.93        0.124\n",
            "Wall time: 26.385614086000032\n",
            "! Best model        8    0.138\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10        0.152          0.1       0.0517        0.167        0.289         13.3        0.208\n",
            "      9    20        0.138        0.113       0.0259        0.244        0.307         9.26        0.147\n",
            "      9    30        0.121        0.115       0.0062         0.25        0.309         4.61        0.072\n",
            "      9    40      0.00977      0.00757      0.00221       0.0328       0.0795        0.515       0.0429\n",
            "      9    50       0.0504       0.0304         0.02        0.124        0.159         8.26        0.129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2        0.136        0.117       0.0192        0.218        0.312         7.56        0.119\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   28.320    0.002        0.104        0.022        0.126        0.214        0.297         7.31        0.116\n",
            "! Validation          9   28.320    0.002        0.113       0.0197        0.133        0.211        0.307          7.5        0.118\n",
            "Wall time: 28.321278563000078\n",
            "! Best model        9    0.133\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0417       0.0408     0.000837        0.146        0.185         1.67       0.0264\n",
            "     10    20       0.0658       0.0338       0.0321        0.134        0.168         10.3        0.164\n",
            "     10    30        0.159        0.159     0.000403        0.296        0.364         1.17       0.0183\n",
            "     10    40        0.211        0.168       0.0431          0.3        0.374         12.1         0.19\n",
            "     10    50        0.138        0.137     0.000344        0.266        0.338         1.07       0.0169\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2        0.133        0.115       0.0181        0.217         0.31         7.22        0.114\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   30.259    0.002        0.104       0.0218        0.126        0.214        0.298         6.58        0.107\n",
            "! Validation         10   30.259    0.002        0.111       0.0186         0.13        0.209        0.305         7.13        0.112\n",
            "Wall time: 30.261285066000028\n",
            "! Best model       10    0.130\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10        0.113        0.101       0.0113        0.232        0.291          6.2       0.0969\n",
            "     11    20        0.274        0.163         0.11         0.29        0.369         19.4        0.303\n",
            "     11    30        0.132        0.132     3.01e-05        0.258        0.332        0.316      0.00501\n",
            "     11    40        0.278        0.197       0.0813        0.318        0.406         16.7         0.26\n",
            "     11    50        0.313        0.218       0.0944        0.336        0.427           18        0.281\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2        0.128        0.111       0.0177        0.214        0.304         7.13        0.112\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   31.725    0.002       0.0986       0.0216         0.12        0.209         0.29         6.79        0.108\n",
            "! Validation         11   31.725    0.002        0.108       0.0184        0.126        0.207          0.3         7.09        0.111\n",
            "Wall time: 31.72636620900016\n",
            "! Best model       11    0.126\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10       0.0613       0.0605     0.000861        0.179        0.225         1.69       0.0268\n",
            "     12    20       0.0675       0.0266       0.0409         0.12        0.149         11.8        0.185\n",
            "     12    30         0.23        0.178       0.0526        0.312        0.385         13.4         0.21\n",
            "     12    40       0.0315        0.031     0.000483        0.125        0.161         1.26       0.0201\n",
            "     12    50        0.063       0.0334       0.0296        0.133        0.167         10.1        0.157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2        0.126        0.111       0.0146        0.214        0.305         6.36          0.1\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   33.021    0.002       0.0971       0.0162        0.113        0.206        0.287         6.14       0.0987\n",
            "! Validation         12   33.021    0.002        0.107       0.0156        0.123        0.206        0.299         6.43        0.101\n",
            "Wall time: 33.021972776999974\n",
            "! Best model       12    0.123\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10        0.176        0.123       0.0528        0.255         0.32         13.2         0.21\n",
            "     13    20       0.0518       0.0418      0.00999        0.145        0.187         5.75       0.0913\n",
            "     13    30        0.161         0.15       0.0109        0.281        0.354         6.01       0.0953\n",
            "     13    40         0.15        0.148      0.00159        0.274        0.352         2.29       0.0364\n",
            "     13    50        0.234        0.207       0.0277        0.325        0.415         9.73        0.152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2        0.125        0.112       0.0133        0.215        0.305         5.94       0.0934\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   34.307    0.002        0.107       0.0153        0.122        0.215        0.301         5.83       0.0943\n",
            "! Validation         13   34.307    0.002        0.107       0.0144        0.121        0.205        0.299         6.02       0.0943\n",
            "Wall time: 34.30741511700012\n",
            "! Best model       13    0.121\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10          0.2        0.185       0.0154        0.316        0.393         7.13        0.113\n",
            "     14    20        0.129        0.112       0.0173        0.245        0.306         7.68         0.12\n",
            "     14    30       0.0857       0.0817        0.004          0.2        0.261          3.7       0.0578\n",
            "     14    40        0.202        0.178       0.0239        0.304        0.385         8.89        0.141\n",
            "     14    50        0.101       0.0952      0.00591        0.225        0.282          4.5       0.0703\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2         0.12        0.107        0.013        0.211        0.299         5.83       0.0917\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   35.596    0.002       0.0974       0.0203        0.118        0.207        0.287          6.8        0.108\n",
            "! Validation         14   35.596    0.002        0.104       0.0142        0.118        0.202        0.295         5.97       0.0935\n",
            "Wall time: 35.59654124200006\n",
            "! Best model       14    0.118\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.104       0.0825       0.0211        0.202        0.262          8.5        0.133\n",
            "     15    20       0.0774       0.0457       0.0317        0.154        0.195         10.3        0.163\n",
            "     15    30        0.163        0.155      0.00743        0.284         0.36         4.96       0.0788\n",
            "     15    40        0.121        0.117      0.00412        0.246        0.312         3.75       0.0586\n",
            "     15    50        0.143        0.138      0.00515        0.282        0.339         4.13       0.0656\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2        0.119        0.105       0.0134        0.209        0.296         5.84       0.0918\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   36.892    0.002       0.0905       0.0191         0.11        0.202        0.277         6.32        0.102\n",
            "! Validation         15   36.892    0.002        0.103       0.0147        0.117        0.201        0.293         5.92       0.0928\n",
            "Wall time: 36.89333569700011\n",
            "! Best model       15    0.117\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10        0.116       0.0722       0.0436        0.142        0.246         12.2        0.191\n",
            "     16    20       0.0469       0.0467     0.000247        0.114        0.197        0.918       0.0143\n",
            "     16    30       0.0429       0.0328       0.0101        0.129        0.165         5.79       0.0919\n",
            "     16    40        0.151        0.147      0.00412        0.276         0.35          3.7       0.0587\n",
            "     16    50        0.216        0.183       0.0326        0.318        0.391         10.6        0.165\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2        0.117        0.105       0.0119        0.209        0.296          5.4       0.0848\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   38.194    0.002       0.0927       0.0124        0.105        0.202         0.28         5.23       0.0847\n",
            "! Validation         16   38.194    0.002        0.102       0.0134        0.115        0.201        0.292         5.63       0.0882\n",
            "Wall time: 38.19511750300012\n",
            "! Best model       16    0.115\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10        0.129        0.128      0.00129        0.264        0.327         2.06       0.0328\n",
            "     17    20       0.0435       0.0412      0.00228        0.146        0.185         2.75       0.0437\n",
            "     17    30       0.0346       0.0328      0.00183        0.132        0.165         2.46        0.039\n",
            "     17    40        0.239         0.19       0.0489        0.317        0.398         12.9        0.202\n",
            "     17    50        0.152        0.124       0.0278        0.256        0.322         9.76        0.152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2        0.113        0.101        0.012        0.205        0.291         5.43       0.0854\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   39.491    0.002       0.0934       0.0177        0.111        0.202        0.282         6.49        0.104\n",
            "! Validation         17   39.491    0.002       0.0992       0.0136        0.113        0.198        0.288          5.7       0.0894\n",
            "Wall time: 39.49201717699998\n",
            "! Best model       17    0.113\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10        0.121         0.12     8.69e-05        0.254        0.317        0.545      0.00852\n",
            "     18    20        0.015        0.014      0.00105       0.0428        0.108        0.355       0.0296\n",
            "     18    30       0.0541       0.0367       0.0174        0.143        0.175         7.59        0.121\n",
            "     18    40        0.193        0.137       0.0563        0.269        0.338         13.9        0.217\n",
            "     18    50       0.0968       0.0731       0.0238        0.197        0.247         9.02        0.141\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2        0.112        0.101       0.0117        0.205         0.29         5.28        0.083\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   40.812    0.002       0.0893       0.0163        0.106        0.198        0.275         6.11       0.0973\n",
            "! Validation         18   40.812    0.002       0.0985       0.0134        0.112        0.197        0.287         5.63       0.0882\n",
            "Wall time: 40.81330694799999\n",
            "! Best model       18    0.112\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10        0.122       0.0938       0.0281        0.222         0.28          9.8        0.153\n",
            "     19    20        0.157        0.115       0.0414         0.25         0.31         11.9        0.186\n",
            "     19    30        0.103        0.103     0.000308        0.169        0.293         1.03        0.016\n",
            "     19    40        0.157         0.14       0.0163        0.275        0.342         7.35        0.117\n",
            "     19    50       0.0303       0.0301     0.000151        0.129        0.159        0.719       0.0112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2         0.11       0.0986       0.0117        0.202        0.287         5.22       0.0819\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   42.555    0.002       0.0869       0.0134          0.1        0.196        0.271         5.55       0.0899\n",
            "! Validation         19   42.555    0.002       0.0966       0.0136         0.11        0.195        0.284         5.64       0.0884\n",
            "Wall time: 42.556312154000125\n",
            "! Best model       19    0.110\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.142        0.132         0.01        0.266        0.332         5.86       0.0916\n",
            "     20    20         0.14        0.138      0.00125        0.278         0.34         2.03       0.0323\n",
            "     20    30       0.0462       0.0353       0.0109        0.134        0.172         6.01       0.0954\n",
            "     20    40       0.0867       0.0866     0.000157        0.209        0.269        0.732       0.0114\n",
            "     20    50        0.197        0.152       0.0441        0.284        0.357         12.3        0.192\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2        0.109       0.0985       0.0106        0.202        0.287         4.84        0.076\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   44.432    0.002        0.089       0.0159        0.105        0.198        0.274         5.62       0.0908\n",
            "! Validation         20   44.432    0.002       0.0963       0.0125        0.109        0.194        0.284         5.39       0.0845\n",
            "Wall time: 44.43392191900011\n",
            "! Best model       20    0.109\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10      0.00958      0.00958     5.25e-06       0.0516       0.0894        0.134      0.00209\n",
            "     21    20        0.171        0.162      0.00896        0.293        0.368         5.53       0.0865\n",
            "     21    30        0.202        0.131       0.0711        0.269        0.331         15.6        0.244\n",
            "     21    40      0.00939     5.37e-11      0.00939     4.17e-06      6.7e-06         5.67       0.0886\n",
            "     21    50        0.133        0.125       0.0083        0.257        0.323         5.24       0.0832\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2        0.108       0.0962       0.0114          0.2        0.283         5.03       0.0791\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   46.366    0.002       0.0819       0.0165       0.0984        0.191        0.263         5.65       0.0891\n",
            "! Validation         21   46.366    0.002       0.0947       0.0134        0.108        0.193        0.281          5.6       0.0877\n",
            "Wall time: 46.36777843300001\n",
            "! Best model       21    0.108\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10        0.189        0.166       0.0231        0.215        0.373         8.88        0.139\n",
            "     22    20        0.163         0.15       0.0132        0.286        0.354          6.6        0.105\n",
            "     22    30       0.0829       0.0775       0.0054        0.203        0.254          4.3       0.0672\n",
            "     22    40       0.0587       0.0559      0.00279        0.172        0.216         3.04       0.0483\n",
            "     22    50        0.241        0.168       0.0727        0.296        0.374         15.8        0.246\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2        0.106       0.0952       0.0109        0.199        0.282         4.82       0.0756\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   48.065    0.002       0.0876       0.0149        0.102        0.195        0.272         5.73       0.0924\n",
            "! Validation         22   48.065    0.002       0.0936        0.013        0.107        0.192         0.28         5.48       0.0858\n",
            "Wall time: 48.06626284700019\n",
            "! Best model       22    0.107\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10       0.0955       0.0943      0.00129        0.225        0.281          2.1       0.0328\n",
            "     23    20        0.161         0.15       0.0117        0.278        0.354         6.24        0.099\n",
            "     23    30        0.155        0.136       0.0192        0.278        0.336         8.09        0.126\n",
            "     23    40        0.164         0.12       0.0437        0.253        0.317           12        0.191\n",
            "     23    50       0.0764       0.0434        0.033        0.149         0.19         10.5        0.166\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2        0.104       0.0933       0.0104        0.197        0.279         4.66       0.0732\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   49.370    0.002       0.0839       0.0163          0.1        0.192        0.266         5.87        0.095\n",
            "! Validation         23   49.370    0.002       0.0922       0.0126        0.105         0.19        0.277          5.4       0.0846\n",
            "Wall time: 49.37044075300014\n",
            "! Best model       23    0.105\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10         0.13        0.121      0.00897        0.253        0.318         5.45       0.0866\n",
            "     24    20       0.0491       0.0207       0.0284        0.106        0.131         9.86        0.154\n",
            "     24    30       0.0763       0.0742      0.00213        0.201        0.249          2.7       0.0422\n",
            "     24    40        0.198        0.129        0.069        0.262        0.328         15.1         0.24\n",
            "     24    50       0.0745       0.0357       0.0388       0.0997        0.173         11.5         0.18\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2        0.103       0.0928      0.00994        0.196        0.278         4.42       0.0694\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   50.668    0.002       0.0816       0.0148       0.0964        0.188        0.262         5.45       0.0877\n",
            "! Validation         24   50.668    0.002       0.0913       0.0122        0.103        0.189        0.276         5.26       0.0823\n",
            "Wall time: 50.66854643900001\n",
            "! Best model       24    0.103\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10        0.203        0.143       0.0597        0.278        0.345         14.3        0.223\n",
            "     25    20         0.11         0.11     5.47e-05        0.251        0.303        0.426      0.00676\n",
            "     25    30       0.0319         0.03      0.00187        0.123        0.158         2.49       0.0395\n",
            "     25    40        0.192        0.166       0.0258        0.298        0.373          9.4        0.147\n",
            "     25    50      0.00611     5.84e-11      0.00611     4.66e-06     6.98e-06         4.57       0.0714\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2        0.102       0.0921      0.00958        0.195        0.277          4.2       0.0659\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   51.952    0.002        0.082       0.0134       0.0954         0.19        0.263         5.19       0.0842\n",
            "! Validation         25   51.952    0.002       0.0904        0.012        0.102        0.188        0.275         5.14       0.0805\n",
            "Wall time: 51.95260084200004\n",
            "! Best model       25    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10        0.118        0.106       0.0122        0.239        0.297         6.35        0.101\n",
            "     26    20       0.0522        0.022       0.0303        0.109        0.135         10.2        0.159\n",
            "     26    30        0.102       0.0896       0.0124        0.158        0.273         6.51        0.102\n",
            "     26    40        0.125        0.115      0.00975        0.251         0.31         5.78       0.0902\n",
            "     26    50        0.134        0.127      0.00698        0.256        0.326         4.81       0.0763\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2          0.1       0.0899       0.0102        0.193        0.274         4.41       0.0691\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   53.227    0.002       0.0802       0.0237        0.104        0.189         0.26         6.82        0.108\n",
            "! Validation         26   53.227    0.002       0.0889       0.0126        0.102        0.186        0.272         5.32       0.0833\n",
            "Wall time: 53.22782578700003\n",
            "! Best model       26    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10        0.229        0.227      0.00192        0.342        0.435         2.56       0.0401\n",
            "     27    20         0.14        0.108       0.0323        0.241          0.3         10.3        0.164\n",
            "     27    30        0.127       0.0906       0.0362         0.22        0.275         11.1        0.174\n",
            "     27    40       0.0362        0.036     0.000178        0.134        0.173        0.768       0.0122\n",
            "     27    50       0.0747       0.0747     3.01e-09        0.202         0.25       0.0032     5.01e-05\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2       0.0995         0.09      0.00952        0.192        0.274         4.18       0.0655\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   54.511    0.002       0.0837       0.0207        0.104        0.189        0.266         6.39        0.105\n",
            "! Validation         27   54.511    0.002       0.0883       0.0119          0.1        0.185        0.271         5.15       0.0807\n",
            "Wall time: 54.51344873300013\n",
            "! Best model       27    0.100\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10        0.067       0.0226       0.0445        0.108        0.137         12.3        0.193\n",
            "     28    20       0.0737       0.0712      0.00243        0.195        0.244         2.88       0.0451\n",
            "     28    30        0.212        0.164       0.0479        0.292         0.37         12.8          0.2\n",
            "     28    40        0.239        0.161       0.0777        0.297        0.367         16.3        0.255\n",
            "     28    50        0.119        0.117      0.00225        0.252        0.313         2.73       0.0434\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2       0.0983       0.0893      0.00901        0.192        0.273         4.01       0.0629\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   55.826    0.002       0.0793       0.0209          0.1        0.186        0.259         6.48        0.106\n",
            "! Validation         28   55.826    0.002       0.0875       0.0113       0.0988        0.184         0.27         5.03       0.0787\n",
            "Wall time: 55.827243898999996\n",
            "! Best model       28    0.099\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10       0.0135     5.86e-11       0.0135     4.64e-06     6.99e-06         6.79        0.106\n",
            "     29    20        0.161        0.127       0.0342         0.26        0.325         10.8        0.169\n",
            "     29    30        0.134        0.134     0.000124        0.264        0.334        0.641       0.0102\n",
            "     29    40        0.103       0.0694       0.0337        0.186        0.241         10.7        0.168\n",
            "     29    50       0.0551        0.051      0.00409        0.119        0.206         3.74       0.0584\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2       0.0981       0.0893      0.00875        0.192        0.273         3.85       0.0604\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   57.110    0.002       0.0805       0.0163       0.0968        0.186        0.261         5.84       0.0948\n",
            "! Validation         29   57.110    0.002       0.0871       0.0111       0.0982        0.184         0.27         4.91       0.0768\n",
            "Wall time: 57.11108945500018\n",
            "! Best model       29    0.098\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10        0.108       0.0893       0.0186         0.22        0.273         7.97        0.125\n",
            "     30    20        0.024       0.0194      0.00461        0.102        0.127         3.97        0.062\n",
            "     30    30       0.0447       0.0139       0.0308       0.0867        0.108         10.3         0.16\n",
            "     30    40        0.102        0.101     0.000203        0.235        0.291        0.833        0.013\n",
            "     30    50       0.0363        0.033      0.00327        0.131        0.166         3.29       0.0523\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2       0.0965       0.0875      0.00906         0.19         0.27         4.06       0.0636\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   58.721    0.002       0.0738       0.0174       0.0913         0.18        0.249         6.18       0.0992\n",
            "! Validation         30   58.721    0.002       0.0856       0.0114        0.097        0.182        0.267         5.06       0.0792\n",
            "Wall time: 58.72161733899998\n",
            "! Best model       30    0.097\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10       0.0199       0.0162      0.00367       0.0944        0.116         3.54       0.0554\n",
            "     31    20       0.0548       0.0183       0.0364       0.0472        0.124         2.09        0.174\n",
            "     31    30         0.16        0.119       0.0407        0.254        0.316         11.8        0.184\n",
            "     31    40        0.108        0.099       0.0088        0.221        0.288          5.4       0.0857\n",
            "     31    50      0.00543     5.88e-11      0.00543      4.7e-06     7.01e-06         4.31       0.0674\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2       0.0945       0.0857      0.00881        0.188        0.268         4.01        0.063\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   60.528    0.002       0.0745       0.0129       0.0874         0.18        0.251         4.89       0.0811\n",
            "! Validation         31   60.528    0.002       0.0841       0.0111       0.0952        0.181        0.265         5.02       0.0786\n",
            "Wall time: 60.528525446\n",
            "! Best model       31    0.095\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10        0.124        0.106       0.0171        0.172        0.298         7.64        0.119\n",
            "     32    20       0.0097     5.46e-11       0.0097     4.47e-06     6.75e-06         5.76         0.09\n",
            "     32    30        0.125        0.109       0.0153        0.246        0.302         7.12        0.113\n",
            "     32    40         0.07       0.0674      0.00262        0.193        0.237         2.99       0.0468\n",
            "     32    50       0.0196       0.0173       0.0023       0.0946         0.12          2.8       0.0438\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2       0.0928       0.0837      0.00903        0.186        0.264         4.09       0.0641\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   62.464    0.002       0.0725       0.0121       0.0847        0.178        0.247         5.16       0.0834\n",
            "! Validation         32   62.464    0.002       0.0825       0.0114       0.0939        0.179        0.262          5.1       0.0798\n",
            "Wall time: 62.464894721000064\n",
            "! Best model       32    0.094\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10        0.172        0.154       0.0184        0.207        0.358         7.94        0.124\n",
            "     33    20       0.0416       0.0408     0.000748        0.144        0.185         1.57        0.025\n",
            "     33    30        0.124        0.114      0.00973        0.242        0.309         5.68       0.0901\n",
            "     33    40        0.209        0.149       0.0604        0.275        0.353         14.4        0.225\n",
            "     33    50       0.0334        0.033     0.000404        0.131        0.166         1.16       0.0184\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2       0.0919        0.083      0.00894        0.185        0.263            4       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   64.362    0.002       0.0781       0.0165       0.0947        0.182        0.256         5.63       0.0927\n",
            "! Validation         33   64.362    0.002       0.0818       0.0113       0.0931        0.179        0.261         5.04       0.0789\n",
            "Wall time: 64.36281347500017\n",
            "! Best model       33    0.093\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10        0.129        0.117       0.0124        0.252        0.312         6.51        0.102\n",
            "     34    20       0.0526       0.0171       0.0355       0.0964         0.12           11        0.172\n",
            "     34    30        0.127        0.114       0.0131        0.249        0.308         6.58        0.105\n",
            "     34    40         0.17        0.116       0.0538         0.18        0.312         13.6        0.212\n",
            "     34    50       0.0785       0.0785     6.71e-06        0.148        0.256        0.151      0.00237\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2       0.0897        0.081      0.00867        0.183         0.26            4       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   65.644    0.002       0.0728       0.0175       0.0903        0.177        0.247         6.01        0.095\n",
            "! Validation         34   65.644    0.002         0.08        0.011        0.091        0.177        0.258         5.01       0.0784\n",
            "Wall time: 65.64460954000015\n",
            "! Best model       34    0.091\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10         0.12        0.101       0.0184        0.232        0.291          7.8        0.124\n",
            "     35    20      0.00786      5.7e-11      0.00786     4.67e-06      6.9e-06         5.19        0.081\n",
            "     35    30       0.0928       0.0602       0.0326        0.179        0.224         10.6        0.165\n",
            "     35    40        0.117        0.108      0.00984        0.243          0.3          5.8       0.0906\n",
            "     35    50     0.000189     7.52e-11     0.000189     5.07e-06     7.92e-06        0.804       0.0126\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2       0.0882       0.0797      0.00852        0.181        0.258         3.97       0.0623\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   66.928    0.002       0.0706       0.0179       0.0884        0.174        0.244         5.94       0.0966\n",
            "! Validation         35   66.928    0.002       0.0787       0.0108       0.0895        0.175        0.256         4.98       0.0779\n",
            "Wall time: 66.92894804399998\n",
            "! Best model       35    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10        0.225        0.195       0.0304        0.308        0.403         10.2        0.159\n",
            "     36    20       0.0361       0.0357     0.000431        0.131        0.173          1.2        0.019\n",
            "     36    30        0.122        0.105       0.0161        0.238        0.297         7.31        0.116\n",
            "     36    40       0.0257       0.0253     0.000448        0.112        0.145         1.22       0.0193\n",
            "     36    50        0.122        0.114       0.0078        0.243        0.309         5.08       0.0807\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2       0.0874       0.0795      0.00796        0.181        0.258         3.72       0.0583\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   68.212    0.002        0.073        0.012       0.0849        0.177        0.248            5       0.0816\n",
            "! Validation         36   68.212    0.002       0.0781       0.0102       0.0883        0.175        0.255         4.79        0.075\n",
            "Wall time: 68.21317980700019\n",
            "! Best model       36    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10       0.0442       0.0437     0.000519        0.151        0.191         1.33       0.0208\n",
            "     37    20       0.0498       0.0381       0.0117        0.138        0.178         6.32       0.0988\n",
            "     37    30       0.0281       0.0273     0.000792        0.117        0.151         1.62       0.0257\n",
            "     37    40       0.0277       0.0249      0.00278        0.111        0.144         3.04       0.0482\n",
            "     37    50       0.0806       0.0599       0.0207         0.18        0.224         8.42        0.132\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2        0.086       0.0786       0.0074         0.18        0.256         3.53       0.0553\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   69.496    0.002       0.0718       0.0108       0.0826        0.174        0.245         4.61        0.074\n",
            "! Validation         37   69.496    0.002       0.0771      0.00955       0.0867        0.174        0.254         4.64       0.0727\n",
            "Wall time: 69.49695629200005\n",
            "! Best model       37    0.087\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10       0.0481       0.0481     3.06e-05        0.156          0.2        0.324      0.00506\n",
            "     38    20        0.119        0.113      0.00519         0.25        0.308         4.21       0.0658\n",
            "     38    30       0.0567        0.045       0.0117        0.112        0.194         6.33       0.0989\n",
            "     38    40        0.115        0.109       0.0057        0.241        0.302         4.35        0.069\n",
            "     38    50       0.0256       0.0254     0.000155         0.11        0.146        0.717       0.0114\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2       0.0837       0.0758      0.00784        0.177        0.252         3.72       0.0584\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   70.790    0.002       0.0661       0.0139         0.08        0.169        0.236         5.31       0.0858\n",
            "! Validation         38   70.790    0.002        0.075       0.0101        0.085        0.172         0.25          4.8       0.0751\n",
            "Wall time: 70.79053929700012\n",
            "! Best model       38    0.085\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10       0.0708       0.0706     0.000185        0.196        0.243        0.795       0.0124\n",
            "     39    20       0.0442       0.0295       0.0147        0.127        0.157         6.97        0.111\n",
            "     39    30        0.193        0.153       0.0392        0.274        0.358         11.6        0.181\n",
            "     39    40       0.0391       0.0304      0.00868        0.122        0.159         5.36       0.0851\n",
            "     39    50         0.12        0.108        0.012        0.241          0.3          6.3          0.1\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2       0.0819       0.0746      0.00731        0.175         0.25         3.54       0.0554\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   72.087    0.002       0.0677       0.0129       0.0806         0.17        0.238         5.42        0.087\n",
            "! Validation         39   72.087    0.002       0.0738      0.00949       0.0833        0.171        0.248         4.67        0.073\n",
            "Wall time: 72.087811999\n",
            "! Best model       39    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10       0.0194       0.0151      0.00429       0.0896        0.112         3.83       0.0598\n",
            "     40    20       0.0306       0.0255      0.00502        0.111        0.146         4.08       0.0648\n",
            "     40    30       0.0813       0.0813        4e-05        0.203        0.261        0.364      0.00578\n",
            "     40    40        0.103       0.0996       0.0039        0.166        0.288         3.65       0.0571\n",
            "     40    50         0.14        0.123       0.0173        0.263         0.32          7.7         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2       0.0805       0.0731      0.00735        0.174        0.247         3.53       0.0553\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   73.379    0.002       0.0667       0.0131       0.0798        0.169        0.237         4.88       0.0805\n",
            "! Validation         40   73.379    0.002       0.0724      0.00957        0.082        0.169        0.246         4.66       0.0728\n",
            "Wall time: 73.37965994500018\n",
            "! Best model       40    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10       0.0295       0.0291     0.000388        0.124        0.156         1.13        0.018\n",
            "     41    20       0.0824       0.0677       0.0146        0.192        0.238         7.07        0.111\n",
            "     41    30       0.0955       0.0938      0.00163         0.22         0.28         2.33       0.0369\n",
            "     41    40       0.0584       0.0163       0.0421        0.092        0.117           12        0.187\n",
            "     41    50        0.181        0.117       0.0645        0.254        0.312         14.8        0.232\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2       0.0799       0.0728      0.00708        0.173        0.247         3.42       0.0536\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   74.819    0.002       0.0672       0.0128         0.08         0.17        0.237          5.1       0.0819\n",
            "! Validation         41   74.819    0.002        0.072      0.00926       0.0812        0.169        0.245         4.56       0.0714\n",
            "Wall time: 74.82038251000017\n",
            "! Best model       41    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10        0.123        0.118      0.00508         0.25        0.314          4.1       0.0651\n",
            "     42    20       0.0931       0.0781        0.015        0.199        0.255         7.04        0.112\n",
            "     42    30        0.028       0.0142       0.0139       0.0859        0.109         6.89        0.108\n",
            "     42    40     3.37e-05     7.42e-11     3.37e-05     5.31e-06     7.87e-06        0.339       0.0053\n",
            "     42    50        0.128        0.121      0.00654        0.184        0.318         4.73       0.0739\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2       0.0779       0.0713      0.00655        0.172        0.244         3.25        0.051\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   76.572    0.002       0.0681      0.00945       0.0776        0.169        0.239         4.69       0.0771\n",
            "! Validation         42   76.572    0.002       0.0704      0.00867       0.0791        0.167        0.242         4.43       0.0693\n",
            "Wall time: 76.57404710500009\n",
            "! Best model       42    0.079\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10        0.114        0.111      0.00356        0.236        0.304         3.44       0.0545\n",
            "     43    20        0.059       0.0565      0.00249        0.176        0.217         2.92       0.0456\n",
            "     43    30       0.0872       0.0871     0.000131        0.212         0.27        0.659       0.0105\n",
            "     43    40        0.193        0.116       0.0773        0.241        0.311         16.3        0.254\n",
            "     43    50        0.142        0.114       0.0282        0.178        0.309         9.82        0.153\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2       0.0768       0.0704      0.00637        0.171        0.243         3.24       0.0507\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   78.574    0.002       0.0683       0.0136       0.0819        0.169         0.24         5.12       0.0832\n",
            "! Validation         43   78.574    0.002       0.0698      0.00841       0.0782        0.166        0.241         4.39       0.0687\n",
            "Wall time: 78.57575307700017\n",
            "! Best model       43    0.078\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10       0.0162       0.0141      0.00211       0.0876        0.109         2.68       0.0419\n",
            "     44    20        0.029       0.0234      0.00553        0.108         0.14         4.28        0.068\n",
            "     44    30       0.0396       0.0389     0.000698        0.139         0.18         1.54       0.0241\n",
            "     44    40        0.127        0.122      0.00487        0.252         0.32         4.08       0.0638\n",
            "     44    50       0.0359       0.0348      0.00112        0.132         0.17         1.93       0.0306\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2        0.077       0.0709      0.00612        0.171        0.243         3.08       0.0482\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   80.501    0.002       0.0658      0.00955       0.0753        0.167        0.235         4.39       0.0723\n",
            "! Validation         44   80.501    0.002       0.0699      0.00817       0.0781        0.167        0.242         4.27       0.0668\n",
            "Wall time: 80.50148832100012\n",
            "! Best model       44    0.078\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10        0.018       0.0109      0.00712       0.0754       0.0954         4.94       0.0771\n",
            "     45    20        0.118         0.11      0.00819        0.175        0.303         5.29       0.0827\n",
            "     45    30       0.0667       0.0648       0.0019        0.187        0.233         2.55       0.0399\n",
            "     45    40       0.0435       0.0325        0.011        0.134        0.165         6.03       0.0957\n",
            "     45    50        0.129        0.123       0.0055        0.249        0.321         4.27       0.0678\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2       0.0763       0.0702      0.00608        0.171        0.242         3.07       0.0481\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   82.031    0.002       0.0657       0.0127       0.0784        0.166        0.234         4.89       0.0797\n",
            "! Validation         45   82.031    0.002       0.0693      0.00815       0.0774        0.166         0.24         4.27       0.0667\n",
            "Wall time: 82.03215943999999\n",
            "! Best model       45    0.077\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10          0.1       0.0995      0.00057        0.232        0.288         1.37       0.0218\n",
            "     46    20       0.0139       0.0138     0.000104       0.0864        0.107        0.596      0.00931\n",
            "     46    30       0.0463       0.0343        0.012         0.13        0.169         6.29       0.0999\n",
            "     46    40       0.0834       0.0694        0.014        0.139        0.241         6.91        0.108\n",
            "     46    50       0.0868       0.0775      0.00932        0.205        0.254         5.65       0.0882\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2       0.0751       0.0694      0.00571         0.17        0.241         2.93       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   83.322    0.002       0.0666       0.0111       0.0777        0.167        0.236         4.45       0.0738\n",
            "! Validation         46   83.322    0.002       0.0686      0.00767       0.0763        0.166        0.239         4.14       0.0648\n",
            "Wall time: 83.32287125100015\n",
            "! Best model       46    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10        0.113        0.095       0.0181        0.224        0.282         7.74        0.123\n",
            "     47    20        0.274        0.182        0.092        0.302         0.39         17.7        0.277\n",
            "     47    30       0.0363       0.0347      0.00159        0.141         0.17         2.29       0.0364\n",
            "     47    40        0.112       0.0954       0.0164        0.226        0.282         7.37        0.117\n",
            "     47    50        0.153        0.128       0.0248        0.189        0.328         9.22        0.144\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2       0.0736       0.0674       0.0062        0.167        0.237         3.29       0.0516\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   84.592    0.002       0.0667       0.0149       0.0816        0.167        0.237         5.86       0.0949\n",
            "! Validation         47   84.592    0.002       0.0675      0.00812       0.0756        0.164        0.237         4.38       0.0685\n",
            "Wall time: 84.59280485199997\n",
            "! Best model       47    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10       0.0901       0.0898      0.00027        0.214        0.274        0.946        0.015\n",
            "     48    20         0.13        0.108       0.0216        0.163        0.301         4.84        0.134\n",
            "     48    30       0.0306       0.0227      0.00786        0.106        0.138          5.1        0.081\n",
            "     48    40       0.0518       0.0517     3.56e-05        0.163        0.208        0.343      0.00545\n",
            "     48    50       0.0824       0.0807      0.00169        0.206         0.26          2.4       0.0376\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2       0.0738       0.0681      0.00572        0.168        0.238         3.11       0.0487\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   85.916    0.002       0.0646       0.0114        0.076        0.165        0.233         5.06       0.0826\n",
            "! Validation         48   85.916    0.002        0.068      0.00752       0.0755        0.165        0.238         4.19       0.0656\n",
            "Wall time: 85.9168381290001\n",
            "! Best model       48    0.076\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10       0.0305        0.011       0.0195       0.0756       0.0957         8.16        0.128\n",
            "     49    20        0.123        0.119      0.00417        0.249        0.315         3.72        0.059\n",
            "     49    30        0.157        0.107       0.0499        0.238        0.299         13.1        0.204\n",
            "     49    40        0.135        0.124       0.0109        0.186        0.321         6.11       0.0955\n",
            "     49    50       0.0662       0.0564      0.00974        0.125        0.217         5.77       0.0902\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2       0.0732       0.0677      0.00548        0.167        0.238         3.01       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   87.200    0.002       0.0656       0.0137       0.0793        0.165        0.234         5.57       0.0906\n",
            "! Validation         49   87.200    0.002       0.0673      0.00727       0.0746        0.164        0.237          4.1       0.0641\n",
            "Wall time: 87.20047465800008\n",
            "! Best model       49    0.075\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10        0.117        0.117     6.55e-05        0.249        0.313        0.466       0.0074\n",
            "     50    20       0.0366       0.0329      0.00365        0.134        0.166         3.48       0.0552\n",
            "     50    30        0.103       0.0984      0.00427        0.231        0.287         3.76       0.0597\n",
            "     50    40       0.0159       0.0115      0.00442       0.0779        0.098         3.89       0.0608\n",
            "     50    50       0.0329       0.0225       0.0103        0.106        0.137         5.85       0.0929\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2       0.0737       0.0684      0.00526        0.168        0.239         2.82       0.0442\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   88.491    0.002        0.066       0.0095       0.0755        0.166        0.235         4.23       0.0698\n",
            "! Validation         50   88.491    0.002       0.0677      0.00707       0.0748        0.165        0.238         3.98       0.0622\n",
            "Wall time: 88.49148783500004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10        0.078       0.0747      0.00336        0.192         0.25         3.34        0.053\n",
            "     51    20        0.138          0.1       0.0373        0.229        0.289         11.3        0.177\n",
            "     51    30       0.0934       0.0885      0.00492        0.217        0.272         4.04       0.0641\n",
            "     51    40       0.0314       0.0309     0.000458         0.12        0.161         1.23       0.0196\n",
            "     51    50       0.0121       0.0112      0.00089       0.0763       0.0968         1.74       0.0273\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2       0.0725       0.0673      0.00513        0.167        0.237         2.82       0.0443\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   89.767    0.002       0.0648       0.0139       0.0787        0.165        0.233         5.47       0.0887\n",
            "! Validation         51   89.767    0.002        0.067       0.0069       0.0739        0.164        0.237         3.97       0.0622\n",
            "Wall time: 89.76769117399999\n",
            "! Best model       51    0.074\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10        0.145        0.125       0.0206        0.247        0.323         8.26        0.131\n",
            "     52    20        0.103       0.0733       0.0296        0.197        0.247          9.9        0.157\n",
            "     52    30         0.13       0.0958       0.0345        0.225        0.283         10.9         0.17\n",
            "     52    40         0.22        0.151       0.0687        0.287        0.355         15.3         0.24\n",
            "     52    50        0.093        0.077        0.016        0.199        0.254         7.28        0.116\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2       0.0715       0.0655        0.006        0.165        0.234         3.36       0.0526\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   91.048    0.002       0.0644       0.0234       0.0878        0.165        0.233          7.2        0.119\n",
            "! Validation         52   91.048    0.002       0.0658      0.00782       0.0736        0.162        0.234         4.36       0.0682\n",
            "Wall time: 91.04904389300009\n",
            "! Best model       52    0.074\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10       0.0486        0.017       0.0316        0.047        0.119         1.95        0.162\n",
            "     53    20       0.0568       0.0542      0.00262        0.171        0.213         2.95       0.0468\n",
            "     53    30       0.0892       0.0856      0.00359        0.211        0.267         3.45       0.0547\n",
            "     53    40        0.218        0.161       0.0574        0.296        0.366           14        0.219\n",
            "     53    50        0.119        0.102       0.0172         0.23        0.292         7.68         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2       0.0715       0.0657      0.00585        0.164        0.234         3.31       0.0519\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   92.733    0.002       0.0652       0.0142       0.0794        0.165        0.234         5.69       0.0924\n",
            "! Validation         53   92.733    0.002       0.0661      0.00757       0.0737        0.162        0.235         4.29       0.0671\n",
            "Wall time: 92.73529282599998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10       0.0907       0.0907     1.29e-05         0.22        0.275         0.21      0.00328\n",
            "     54    20       0.0939       0.0829        0.011        0.152        0.263         6.14       0.0959\n",
            "     54    30       0.0283       0.0276     0.000695        0.108        0.152         1.52       0.0241\n",
            "     54    40        0.141        0.109       0.0321        0.239        0.301         10.5        0.164\n",
            "     54    50       0.0512       0.0317       0.0195        0.122        0.163         8.04        0.128\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2        0.072       0.0665       0.0055        0.165        0.236         3.12       0.0489\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   94.609    0.002       0.0641       0.0105       0.0747        0.163        0.232         4.68       0.0768\n",
            "! Validation         54   94.609    0.002       0.0665       0.0072       0.0737        0.163        0.236         4.14       0.0648\n",
            "Wall time: 94.61142167700018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10        0.132       0.0855        0.047        0.215        0.267         12.5        0.198\n",
            "     55    20        0.118       0.0931       0.0249        0.226        0.279         9.09        0.144\n",
            "     55    30        0.129         0.12      0.00872        0.174        0.317         3.07       0.0854\n",
            "     55    40        0.167        0.152       0.0154        0.271        0.356         7.25        0.113\n",
            "     55    50       0.0136       0.0113      0.00232        0.077       0.0971         2.81        0.044\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2       0.0711       0.0654      0.00571        0.164        0.234         3.23       0.0506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   96.576    0.002       0.0622       0.0121       0.0743        0.162        0.228          5.2       0.0845\n",
            "! Validation         55   96.576    0.002       0.0656       0.0075       0.0731        0.162        0.234         4.25       0.0665\n",
            "Wall time: 96.57761163999999\n",
            "! Best model       55    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10         0.11        0.103      0.00684        0.233        0.293         4.76       0.0756\n",
            "     56    20       0.0327       0.0306      0.00205         0.12         0.16         2.61       0.0414\n",
            "     56    30        0.144        0.136      0.00803        0.186        0.337         2.95       0.0819\n",
            "     56    40       0.0279       0.0273     0.000596        0.115        0.151         1.41       0.0223\n",
            "     56    50       0.0294       0.0294     4.34e-05        0.116        0.157        0.379      0.00602\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2       0.0707       0.0651      0.00562        0.164        0.233         3.15       0.0493\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   98.412    0.002       0.0638       0.0112        0.075        0.164        0.231         4.78       0.0788\n",
            "! Validation         56   98.412    0.002       0.0652      0.00745       0.0727        0.162        0.233         4.21       0.0658\n",
            "Wall time: 98.41263649400003\n",
            "! Best model       56    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10        0.131        0.103       0.0282         0.24        0.293         9.83        0.154\n",
            "     57    20       0.0348       0.0261      0.00877         0.11        0.148         5.39       0.0856\n",
            "     57    30       0.0315       0.0312     0.000291        0.131        0.161        0.982       0.0156\n",
            "     57    40       0.0335       0.0326     0.000985        0.122        0.165         1.81       0.0287\n",
            "     57    50        0.198        0.147       0.0514         0.28         0.35         13.3        0.207\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2       0.0705       0.0649      0.00566        0.164        0.233         3.15       0.0495\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   99.721    0.002       0.0647      0.00963       0.0744        0.165        0.233         4.53        0.074\n",
            "! Validation         57   99.721    0.002       0.0652      0.00749       0.0727        0.161        0.233         4.21       0.0659\n",
            "Wall time: 99.722359826\n",
            "! Best model       57    0.073\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10       0.0459       0.0341       0.0119         0.13        0.169         6.28       0.0996\n",
            "     58    20        0.119        0.115      0.00384        0.247         0.31         3.57       0.0566\n",
            "     58    30       0.0944       0.0894        0.005        0.222        0.273         4.07       0.0646\n",
            "     58    40       0.0955       0.0876      0.00787         0.21        0.271         5.11       0.0811\n",
            "     58    50        0.138       0.0892       0.0491        0.219        0.273         12.8        0.202\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2       0.0703       0.0647       0.0056        0.163        0.233         3.08       0.0483\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58  101.025    0.002       0.0638       0.0114       0.0752        0.163        0.231         4.74       0.0801\n",
            "! Validation         58  101.025    0.002       0.0647      0.00749       0.0722        0.161        0.232         4.17       0.0653\n",
            "Wall time: 101.02611851400002\n",
            "! Best model       58    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10        0.115         0.11      0.00504        0.249        0.303         4.15       0.0649\n",
            "     59    20       0.0715       0.0695      0.00199         0.19        0.241         2.61       0.0408\n",
            "     59    30        0.113        0.109       0.0039        0.251        0.302         3.65       0.0571\n",
            "     59    40       0.0274       0.0264        0.001        0.108        0.148         1.82       0.0289\n",
            "     59    50       0.0806       0.0792      0.00144        0.209        0.257         2.19       0.0347\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2       0.0698        0.064      0.00586        0.162        0.231         3.27       0.0513\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59  102.315    0.002       0.0627       0.0114       0.0741        0.163        0.229         5.11       0.0829\n",
            "! Validation         59  102.315    0.002       0.0644      0.00774       0.0722         0.16        0.232         4.31       0.0675\n",
            "Wall time: 102.31536403400014\n",
            "! Best model       59    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10       0.0772       0.0745      0.00265          0.2        0.249         3.01        0.047\n",
            "     60    20        0.131        0.107        0.024        0.236        0.299         9.05        0.141\n",
            "     60    30        0.144        0.119       0.0257         0.26        0.315         9.38        0.147\n",
            "     60    40       0.0401        0.038      0.00208        0.129        0.178         2.62       0.0416\n",
            "     60    50       0.0536       0.0241       0.0295        0.109        0.142         9.89        0.157\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2       0.0695       0.0639      0.00561        0.162        0.231          3.2       0.0501\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60  103.608    0.002       0.0617       0.0112       0.0729        0.162        0.228         4.68       0.0777\n",
            "! Validation         60  103.608    0.002       0.0645      0.00741       0.0719         0.16        0.232         4.23       0.0662\n",
            "Wall time: 103.609156343\n",
            "! Best model       60    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10        0.182        0.118       0.0637        0.244        0.314         14.8        0.231\n",
            "     61    20       0.0211       0.0139      0.00719       0.0849        0.108         4.96       0.0775\n",
            "     61    30        0.115        0.109       0.0055        0.242        0.302         4.27       0.0678\n",
            "     61    40        0.177        0.157       0.0197        0.288        0.363          8.2        0.128\n",
            "     61    50       0.0658       0.0148        0.051       0.0874        0.111         13.2        0.206\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2       0.0694       0.0642      0.00517        0.162        0.232            3       0.0471\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61  104.906    0.002       0.0641       0.0161       0.0802        0.166        0.232         6.14        0.101\n",
            "! Validation         61  104.906    0.002       0.0646      0.00695       0.0716        0.161        0.232         4.08       0.0639\n",
            "Wall time: 104.90735393500017\n",
            "! Best model       61    0.072\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10     5.34e-07     8.92e-11     5.34e-07     5.51e-06     8.63e-06       0.0427     0.000668\n",
            "     62    20        0.152        0.134       0.0183        0.273        0.334         7.91        0.124\n",
            "     62    30       0.0562        0.052      0.00418        0.166        0.208         3.72       0.0591\n",
            "     62    40       0.0276       0.0275     8.21e-05         0.11        0.151        0.522      0.00828\n",
            "     62    50       0.0571        0.055       0.0021        0.169        0.214         2.68       0.0419\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2       0.0696       0.0647      0.00493        0.163        0.232         2.83       0.0444\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62  106.213    0.002       0.0655       0.0135        0.079        0.166        0.234         4.97        0.082\n",
            "! Validation         62  106.213    0.002       0.0646      0.00679       0.0713        0.161        0.232         3.99       0.0624\n",
            "Wall time: 106.21407049100003\n",
            "! Best model       62    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10       0.0831       0.0814      0.00169        0.208        0.261         2.36       0.0375\n",
            "     63    20        0.145        0.104       0.0411        0.236        0.294         11.9        0.185\n",
            "     63    30       0.0159       0.0112      0.00465       0.0764       0.0968         3.99       0.0623\n",
            "     63    40        0.084       0.0685       0.0155        0.194        0.239         7.27        0.114\n",
            "     63    50       0.0544       0.0515      0.00295         0.12        0.207         3.17       0.0496\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2       0.0691       0.0635      0.00557        0.162         0.23         3.11       0.0488\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63  107.492    0.002        0.062       0.0114       0.0734        0.161        0.228         4.65       0.0762\n",
            "! Validation         63  107.492    0.002       0.0638      0.00753       0.0713         0.16        0.231         4.21       0.0659\n",
            "Wall time: 107.49281202800012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10       0.0222       0.0219     0.000301        0.103        0.135        0.998       0.0158\n",
            "     64    20       0.0424       0.0264        0.016        0.109        0.148         7.27        0.115\n",
            "     64    30       0.0415       0.0197       0.0218       0.0502        0.128         1.62        0.135\n",
            "     64    40       0.0198       0.0137       0.0061       0.0841        0.107         4.57       0.0714\n",
            "     64    50         0.15        0.142      0.00846        0.277        0.344         5.38       0.0841\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2       0.0689       0.0636      0.00533        0.162         0.23         2.99       0.0469\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64  109.028    0.002       0.0622       0.0101       0.0723        0.162        0.228         4.59       0.0755\n",
            "! Validation         64  109.028    0.002       0.0639      0.00723       0.0711         0.16        0.231         4.11       0.0643\n",
            "Wall time: 109.02958173399998\n",
            "! Best model       64    0.071\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10       0.0973        0.096      0.00129        0.219        0.283         2.07       0.0328\n",
            "     65    20       0.0171       0.0151        0.002         0.09        0.112         2.62       0.0409\n",
            "     65    30       0.0349       0.0254      0.00956       0.0538        0.146         1.07       0.0894\n",
            "     65    40        0.184        0.183     0.000229        0.226        0.391        0.885       0.0138\n",
            "     65    50        0.127        0.123      0.00385        0.265        0.321         3.63       0.0567\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2       0.0679       0.0629      0.00504         0.16        0.229         2.89       0.0453\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65  110.849    0.002       0.0627      0.00977       0.0725        0.161        0.229          4.7       0.0759\n",
            "! Validation         65  110.849    0.002       0.0635      0.00684       0.0703        0.159         0.23         4.01       0.0627\n",
            "Wall time: 110.85093146400004\n",
            "! Best model       65    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10        0.202        0.135       0.0671        0.257        0.336         15.1        0.237\n",
            "     66    20       0.0291       0.0291     6.66e-06        0.109        0.156        0.149      0.00236\n",
            "     66    30        0.133        0.115       0.0173        0.247         0.31         7.69         0.12\n",
            "     66    40        0.102       0.0914       0.0104        0.221        0.276         5.87       0.0931\n",
            "     66    50       0.0701        0.053       0.0171        0.165         0.21         7.53        0.119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2       0.0684       0.0634      0.00499        0.161         0.23          2.9       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  112.792    0.002       0.0647       0.0113        0.076        0.166        0.233         4.96       0.0793\n",
            "! Validation         66  112.792    0.002       0.0641      0.00676       0.0708         0.16        0.231         3.99       0.0625\n",
            "Wall time: 112.79411164300018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10       0.0902       0.0777       0.0125          0.2        0.255         6.43        0.102\n",
            "     67    20       0.0649       0.0398       0.0251        0.141        0.182         9.27        0.145\n",
            "     67    30       0.0186      0.00985      0.00875       0.0524       0.0907         5.47       0.0855\n",
            "     67    40       0.0479       0.0449      0.00307        0.112        0.194         3.24       0.0507\n",
            "     67    50       0.0301       0.0298     0.000364        0.115        0.158          1.1       0.0174\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2       0.0683       0.0634       0.0049        0.161         0.23         2.77       0.0434\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  114.738    0.002       0.0611      0.00938       0.0705        0.161        0.226         4.24       0.0708\n",
            "! Validation         67  114.738    0.002        0.064      0.00667       0.0707         0.16        0.231         3.91       0.0611\n",
            "Wall time: 114.74009094200005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10       0.0568       0.0417       0.0151        0.108        0.187         7.17        0.112\n",
            "     68    20        0.152        0.117       0.0351        0.255        0.312           11        0.171\n",
            "     68    30        0.096       0.0878      0.00821        0.217        0.271         5.22       0.0828\n",
            "     68    40       0.0321       0.0321     1.09e-05        0.134        0.164         0.19      0.00302\n",
            "     68    50        0.134        0.132      0.00237        0.259        0.332         2.85       0.0445\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2       0.0681       0.0632       0.0049         0.16         0.23         2.76       0.0432\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  116.190    0.002       0.0628        0.008       0.0708        0.161        0.229         3.95       0.0651\n",
            "! Validation         68  116.190    0.002       0.0637      0.00671       0.0704        0.159        0.231         3.92       0.0613\n",
            "Wall time: 116.19116897900017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10       0.0325       0.0306      0.00184         0.12         0.16         2.47       0.0392\n",
            "     69    20       0.0567       0.0425       0.0142         0.15        0.188         6.96        0.109\n",
            "     69    30        0.179        0.142       0.0373        0.275        0.344         11.3        0.176\n",
            "     69    40       0.0297       0.0225      0.00717        0.106        0.137         4.87       0.0774\n",
            "     69    50         0.03       0.0258      0.00423        0.109        0.147         3.74       0.0594\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2       0.0674       0.0626      0.00482         0.16        0.229         2.75       0.0432\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  117.520    0.002       0.0603       0.0146        0.075         0.16        0.225         5.83       0.0949\n",
            "! Validation         69  117.520    0.002       0.0632      0.00663       0.0699        0.159         0.23         3.92       0.0613\n",
            "Wall time: 117.52135847400018\n",
            "! Best model       69    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10        0.038       0.0243       0.0137       0.0822        0.142         6.86        0.107\n",
            "     70    20       0.0999       0.0866       0.0133        0.211        0.269         6.65        0.106\n",
            "     70    30       0.0403       0.0381       0.0022        0.139        0.178         2.74       0.0429\n",
            "     70    40        0.162         0.14       0.0224        0.279        0.341         8.75        0.137\n",
            "     70    50       0.0557       0.0422       0.0135         0.15        0.188          6.8        0.106\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2       0.0673       0.0626       0.0047         0.16        0.229         2.73       0.0428\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  118.813    0.002       0.0598       0.0101       0.0699        0.159        0.224         4.72       0.0774\n",
            "! Validation         70  118.813    0.002       0.0632      0.00648       0.0697        0.158         0.23         3.89       0.0608\n",
            "Wall time: 118.81402507899998\n",
            "! Best model       70    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10       0.0546       0.0518      0.00276        0.165        0.208         3.02        0.048\n",
            "     71    20       0.0627       0.0184       0.0444       0.0475        0.124         2.31        0.192\n",
            "     71    30       0.0298       0.0289     0.000911        0.109        0.155         1.74       0.0276\n",
            "     71    40        0.142        0.127       0.0155        0.254        0.325         7.29        0.114\n",
            "     71    50       0.0758       0.0711      0.00472        0.196        0.244         3.96       0.0628\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2       0.0673       0.0626      0.00472         0.16        0.229         2.76       0.0433\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  120.101    0.002       0.0599      0.00967       0.0696        0.159        0.224         4.35       0.0724\n",
            "! Validation         71  120.101    0.002       0.0631      0.00648       0.0696        0.158        0.229          3.9       0.0609\n",
            "Wall time: 120.10289522100015\n",
            "! Best model       71    0.070\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10        0.038       0.0372     0.000816        0.128        0.176         1.64       0.0261\n",
            "     72    20       0.0165       0.0148      0.00168       0.0898        0.111          2.4       0.0375\n",
            "     72    30       0.0612       0.0593       0.0019        0.175        0.223         2.55       0.0398\n",
            "     72    40        0.106        0.102      0.00407        0.226        0.292         3.67       0.0583\n",
            "     72    50       0.0916       0.0757       0.0158          0.2        0.251         7.25        0.115\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2       0.0668       0.0619       0.0049        0.159        0.227         2.85       0.0447\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  121.397    0.002       0.0621      0.00817       0.0703        0.161        0.228         4.07       0.0664\n",
            "! Validation         72  121.397    0.002       0.0626      0.00668       0.0693        0.158        0.229         3.96       0.0619\n",
            "Wall time: 121.397901047\n",
            "! Best model       72    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10        0.103       0.0984      0.00434        0.165        0.287         3.85       0.0602\n",
            "     73    20        0.124        0.111       0.0136        0.252        0.304         6.82        0.107\n",
            "     73    30        0.134        0.109       0.0258        0.236        0.301         9.39        0.147\n",
            "     73    40        0.022       0.0115       0.0105       0.0775       0.0979            6       0.0937\n",
            "     73    50       0.0932       0.0891       0.0041         0.22        0.273         3.69       0.0585\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2       0.0669       0.0625      0.00447         0.16        0.228         2.62       0.0411\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  122.680    0.002       0.0626      0.00772       0.0704        0.163        0.229         4.08       0.0673\n",
            "! Validation         73  122.680    0.002       0.0629      0.00622       0.0691        0.158        0.229         3.79       0.0593\n",
            "Wall time: 122.68093548299998\n",
            "! Best model       73    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10        0.159        0.121       0.0381         0.26        0.318         11.4        0.178\n",
            "     74    20        0.102        0.102     0.000163        0.227        0.291        0.735       0.0117\n",
            "     74    30       0.0351       0.0277      0.00743        0.116        0.152         4.96       0.0787\n",
            "     74    40       0.0135        0.013     0.000529       0.0833        0.104         1.35        0.021\n",
            "     74    50       0.0915       0.0864      0.00511        0.212        0.269         4.18       0.0653\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2       0.0668       0.0623      0.00446        0.159        0.228         2.58       0.0404\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  123.970    0.002       0.0615       0.0157       0.0772        0.161        0.227         5.68       0.0932\n",
            "! Validation         74  123.970    0.002       0.0626      0.00626       0.0689        0.158        0.229         3.77        0.059\n",
            "Wall time: 123.97058949000007\n",
            "! Best model       74    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10     0.000262      7.5e-11     0.000262     5.39e-06     7.91e-06        0.946       0.0148\n",
            "     75    20       0.0641       0.0534       0.0107        0.166        0.211         5.95       0.0944\n",
            "     75    30        0.109        0.107      0.00207        0.236        0.298         2.66       0.0416\n",
            "     75    40        0.115       0.0886       0.0262        0.215        0.272         9.31        0.148\n",
            "     75    50       0.0278       0.0275     0.000255        0.114        0.152        0.919       0.0146\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2        0.067       0.0625      0.00452         0.16        0.228         2.65       0.0416\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  125.337    0.002       0.0628       0.0112       0.0739        0.162        0.229         4.95       0.0799\n",
            "! Validation         75  125.337    0.002       0.0628      0.00631       0.0691        0.158        0.229         3.81       0.0596\n",
            "Wall time: 125.33795144500004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10       0.0351       0.0328      0.00234         0.12        0.165         2.79       0.0442\n",
            "     76    20       0.0617       0.0476       0.0141        0.115        0.199         6.95        0.109\n",
            "     76    30        0.119        0.109         0.01         0.24        0.302         5.86       0.0915\n",
            "     76    40        0.113        0.102       0.0111        0.241        0.292         6.16       0.0963\n",
            "     76    50       0.0738       0.0732     0.000631        0.195        0.247         1.47        0.023\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2       0.0661       0.0612      0.00486        0.158        0.226         2.89       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  127.063    0.002       0.0608      0.00787       0.0686        0.159        0.225         3.87       0.0617\n",
            "! Validation         76  127.063    0.002       0.0623       0.0066       0.0689        0.157        0.228         3.98       0.0622\n",
            "Wall time: 127.06534930200019\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10       0.0266       0.0221       0.0045       0.0511        0.136        0.735       0.0613\n",
            "     77    20        0.182        0.126       0.0552        0.249        0.325         13.7        0.215\n",
            "     77    30       0.0798        0.079     0.000853          0.2        0.257         1.68       0.0267\n",
            "     77    40       0.0952       0.0952      5.1e-05        0.224        0.282        0.418      0.00652\n",
            "     77    50       0.0751       0.0751     8.46e-07        0.202         0.25       0.0529      0.00084\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2       0.0658       0.0606      0.00516        0.157        0.225         3.03       0.0475\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  128.999    0.002       0.0588      0.00914        0.068        0.158        0.222         4.29       0.0691\n",
            "! Validation         77  128.999    0.002       0.0621      0.00689        0.069        0.157        0.228         4.06       0.0636\n",
            "Wall time: 129.00006546200007\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10       0.0826       0.0821     0.000475        0.211        0.262         1.25       0.0199\n",
            "     78    20        0.179        0.151       0.0279        0.271        0.355         9.77        0.153\n",
            "     78    30       0.0302       0.0275      0.00276        0.113        0.151         3.02        0.048\n",
            "     78    40        0.149        0.129       0.0199        0.256        0.329         8.25        0.129\n",
            "     78    50       0.0717         0.02       0.0517        0.047        0.129         2.49        0.208\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2       0.0658       0.0608      0.00496        0.157        0.225         2.91       0.0457\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  130.971    0.002       0.0597      0.00927        0.069        0.159        0.224         4.25       0.0713\n",
            "! Validation         78  130.971    0.002       0.0621      0.00668       0.0688        0.157        0.228         3.98       0.0622\n",
            "Wall time: 130.973061939\n",
            "! Best model       78    0.069\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10       0.0195       0.0073       0.0122       0.0451       0.0781         6.46        0.101\n",
            "     79    20       0.0289       0.0288     0.000114        0.117        0.155        0.615      0.00976\n",
            "     79    30       0.0811       0.0773      0.00378        0.205        0.254         3.54       0.0562\n",
            "     79    40       0.0186       0.0126        0.006       0.0814        0.103         4.53       0.0708\n",
            "     79    50       0.0228       0.0226     0.000199        0.105        0.137        0.811       0.0129\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2       0.0657       0.0609      0.00481        0.157        0.226         2.83       0.0444\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  132.589    0.002       0.0607       0.0102       0.0709         0.16        0.226         4.57       0.0755\n",
            "! Validation         79  132.589    0.002       0.0619      0.00655       0.0684        0.157        0.227         3.91       0.0612\n",
            "Wall time: 132.58959205200017\n",
            "! Best model       79    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10       0.0514       0.0151       0.0363       0.0887        0.112         11.1        0.174\n",
            "     80    20        0.017       0.0143      0.00274       0.0862        0.109         3.06       0.0479\n",
            "     80    30      0.00142     8.44e-11      0.00142      5.4e-06      8.4e-06          2.2       0.0344\n",
            "     80    40       0.0307       0.0137        0.017       0.0844        0.107         7.63        0.119\n",
            "     80    50       0.0372       0.0291       0.0081        0.118        0.156         5.18       0.0822\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2       0.0653       0.0603      0.00508        0.156        0.224            3        0.047\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  133.884    0.002       0.0603       0.0107        0.071         0.16        0.225         4.66       0.0772\n",
            "! Validation         80  133.884    0.002       0.0615      0.00681       0.0683        0.156        0.227         4.03       0.0631\n",
            "Wall time: 133.885460427\n",
            "! Best model       80    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10        0.135        0.123       0.0126        0.259         0.32         6.58        0.103\n",
            "     81    20       0.0923       0.0887       0.0036         0.22        0.272         3.46       0.0549\n",
            "     81    30        0.115        0.113      0.00172        0.239        0.307         2.39       0.0379\n",
            "     81    40        0.102        0.102     0.000147        0.228        0.291        0.698       0.0111\n",
            "     81    50        0.136        0.101       0.0348         0.23        0.291         10.9         0.17\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2       0.0654       0.0606      0.00484        0.157        0.225         2.87       0.0451\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  135.162    0.002       0.0611      0.00928       0.0704         0.16        0.226         4.48       0.0727\n",
            "! Validation         81  135.162    0.002       0.0615      0.00658        0.068        0.156        0.227         3.94       0.0617\n",
            "Wall time: 135.16274150900017\n",
            "! Best model       81    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10        0.132        0.117       0.0142        0.253        0.313         6.97        0.109\n",
            "     82    20       0.0902       0.0899     0.000266        0.214        0.274        0.939       0.0149\n",
            "     82    30       0.0272       0.0266     0.000679        0.109        0.149          1.5       0.0238\n",
            "     82    40       0.0975       0.0974     0.000122        0.224        0.285        0.635       0.0101\n",
            "     82    50       0.0292       0.0271      0.00213       0.0556         0.15        0.506       0.0422\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2       0.0656       0.0611      0.00456        0.157        0.226         2.65       0.0415\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  136.432    0.002       0.0603      0.00598       0.0663         0.16        0.224         3.31       0.0539\n",
            "! Validation         82  136.432    0.002       0.0619      0.00629       0.0682        0.156        0.227         3.78       0.0592\n",
            "Wall time: 136.43317095800012\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10        0.143        0.094        0.049        0.222         0.28         12.9        0.202\n",
            "     83    20       0.0275       0.0129       0.0146       0.0825        0.104         7.08        0.111\n",
            "     83    30       0.0707       0.0663      0.00431        0.187        0.235         3.78         0.06\n",
            "     83    40        0.147        0.123       0.0235        0.262        0.321         8.97         0.14\n",
            "     83    50        0.123        0.108        0.015        0.238        0.301         7.17        0.112\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2       0.0651       0.0604      0.00468        0.157        0.225          2.8       0.0439\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  137.699    0.002       0.0598       0.0131        0.073        0.158        0.224         5.14       0.0849\n",
            "! Validation         83  137.699    0.002       0.0612       0.0064       0.0676        0.156        0.226         3.88       0.0608\n",
            "Wall time: 137.6996589710002\n",
            "! Best model       83    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10        0.052       0.0479      0.00413        0.163          0.2         3.76       0.0587\n",
            "     84    20       0.0353       0.0258      0.00951       0.0531        0.147         1.07       0.0891\n",
            "     84    30        0.143       0.0974       0.0454        0.231        0.285         12.5        0.195\n",
            "     84    40       0.0936       0.0922      0.00147        0.215        0.277         2.21       0.0351\n",
            "     84    50       0.0569       0.0568     0.000118        0.171        0.218        0.635      0.00992\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2       0.0651       0.0605      0.00461        0.157        0.225         2.73       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  139.050    0.002       0.0599      0.00805        0.068        0.161        0.224         4.14       0.0677\n",
            "! Validation         84  139.050    0.002       0.0614      0.00633       0.0677        0.156        0.226         3.84         0.06\n",
            "Wall time: 139.05098671200017\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10       0.0906       0.0905       0.0001        0.218        0.275        0.576      0.00915\n",
            "     85    20        0.129       0.0946       0.0347        0.155        0.281         6.13         0.17\n",
            "     85    30          0.1        0.099      0.00115        0.234        0.288         1.98       0.0309\n",
            "     85    40         0.16          0.1       0.0592        0.232        0.289         14.2        0.222\n",
            "     85    50       0.0237       0.0222      0.00141        0.104        0.136         2.16       0.0343\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2       0.0652       0.0601      0.00518        0.156        0.224         3.05       0.0479\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  140.327    0.002       0.0585       0.0112       0.0697        0.159        0.222         4.29       0.0715\n",
            "! Validation         85  140.327    0.002       0.0613      0.00691       0.0682        0.156        0.226         4.07       0.0636\n",
            "Wall time: 140.32754743100008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10      0.00027     7.78e-11      0.00027     5.53e-06     8.06e-06        0.961        0.015\n",
            "     86    20        0.119        0.108       0.0106        0.241        0.301         6.03       0.0942\n",
            "     86    30       0.0317       0.0317     1.92e-05        0.121        0.163        0.252      0.00401\n",
            "     86    40        0.094        0.094     1.05e-05        0.223         0.28         0.19      0.00296\n",
            "     86    50        0.101       0.0996      0.00162        0.224        0.288         2.32       0.0368\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2       0.0651       0.0601      0.00495        0.156        0.224         2.93       0.0459\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  141.638    0.002       0.0595      0.00806       0.0676        0.158        0.223         4.21       0.0686\n",
            "! Validation         86  141.638    0.002       0.0612      0.00669       0.0679        0.156        0.226         3.98       0.0623\n",
            "Wall time: 141.63882135100016\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10       0.0578       0.0577      0.00012        0.173        0.219         0.64         0.01\n",
            "     87    20       0.0429       0.0259        0.017       0.0523        0.147         1.43        0.119\n",
            "     87    30        0.124        0.117      0.00712        0.246        0.313         4.86       0.0771\n",
            "     87    40        0.132        0.132     8.49e-05        0.268        0.332        0.539      0.00842\n",
            "     87    50        0.106       0.0769       0.0288        0.204        0.253         9.77        0.155\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2       0.0651       0.0604      0.00468        0.157        0.225         2.81       0.0441\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  143.311    0.002       0.0667        0.011       0.0777        0.166        0.236         4.99        0.082\n",
            "! Validation         87  143.311    0.002       0.0612      0.00641       0.0676        0.156        0.226         3.89       0.0609\n",
            "Wall time: 143.31314326899997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10        0.119        0.103       0.0161        0.237        0.294         7.42        0.116\n",
            "     88    20      0.00218     7.18e-11      0.00218     4.87e-06     7.75e-06         2.73       0.0427\n",
            "     88    30       0.0911       0.0911     5.68e-08        0.226        0.276       0.0137     0.000218\n",
            "     88    40       0.0289       0.0285     0.000439        0.116        0.154         1.21       0.0192\n",
            "     88    50       0.0782       0.0767      0.00151        0.197        0.253         2.23       0.0355\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2       0.0649       0.0604      0.00457        0.156        0.225         2.64       0.0413\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  145.175    0.002       0.0615      0.00877       0.0703        0.161        0.227         4.14        0.068\n",
            "! Validation         88  145.175    0.002       0.0612      0.00633       0.0676        0.155        0.226         3.79       0.0593\n",
            "Wall time: 145.1772268510001\n",
            "! Best model       88    0.068\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10       0.0823       0.0822     0.000116        0.151        0.262        0.631      0.00985\n",
            "     89    20       0.0499       0.0366       0.0134        0.134        0.175         6.66        0.106\n",
            "     89    30        0.152         0.12       0.0324         0.25        0.316         10.4        0.165\n",
            "     89    40       0.0923       0.0911      0.00119         0.22        0.276         1.99       0.0315\n",
            "     89    50       0.0268       0.0264      0.00036       0.0544        0.149        0.208       0.0173\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2       0.0645       0.0597      0.00479        0.155        0.223         2.86       0.0449\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  147.158    0.002       0.0612      0.00959       0.0708         0.16        0.226         4.24       0.0686\n",
            "! Validation         89  147.158    0.002       0.0609      0.00648       0.0674        0.155        0.226         3.93       0.0614\n",
            "Wall time: 147.16011486600019\n",
            "! Best model       89    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10       0.0558       0.0497      0.00604        0.118        0.204         4.55        0.071\n",
            "     90    20       0.0231       0.0048       0.0183       0.0366       0.0633         7.92        0.124\n",
            "     90    30       0.0809       0.0788       0.0021        0.201        0.256         2.64       0.0419\n",
            "     90    40        0.112        0.102      0.00999         0.16        0.292         3.29       0.0913\n",
            "     90    50       0.0348       0.0333      0.00153         0.12        0.167         2.25       0.0357\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2       0.0648       0.0597      0.00503        0.156        0.223         2.96       0.0465\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  148.983    0.002       0.0608       0.0111       0.0719        0.161        0.226         4.86       0.0793\n",
            "! Validation         90  148.983    0.002       0.0611      0.00672       0.0678        0.155        0.226         3.99       0.0624\n",
            "Wall time: 148.98355756800015\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10       0.0325       0.0165        0.016       0.0921        0.118          7.4        0.116\n",
            "     91    20        0.156        0.128       0.0278        0.256        0.327         9.75        0.152\n",
            "     91    30        0.114        0.105      0.00935        0.243        0.296         5.66       0.0884\n",
            "     91    40         0.12        0.103       0.0176        0.234        0.293         7.77        0.121\n",
            "     91    50       0.0758       0.0743      0.00141        0.196        0.249          2.2       0.0344\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2       0.0646       0.0597      0.00487        0.155        0.223          2.9       0.0454\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  150.276    0.002       0.0606      0.00785       0.0685         0.16        0.225         3.98       0.0659\n",
            "! Validation         91  150.276    0.002       0.0611      0.00654       0.0676        0.155        0.226         3.93       0.0615\n",
            "Wall time: 150.27730461200008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10       0.0914       0.0913     0.000171        0.217        0.276        0.754        0.012\n",
            "     92    20       0.0951       0.0912      0.00387        0.216        0.276         3.58       0.0568\n",
            "     92    30       0.0251       0.0232      0.00192        0.107        0.139         2.53       0.0401\n",
            "     92    40       0.0152        0.014      0.00117       0.0868        0.108            2       0.0312\n",
            "     92    50        0.093       0.0916       0.0014        0.221        0.277         2.19       0.0342\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2       0.0647       0.0601      0.00457        0.156        0.224         2.74       0.0429\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  151.556    0.002        0.059      0.00934       0.0684        0.158        0.222         4.28       0.0682\n",
            "! Validation         92  151.556    0.002       0.0613      0.00617       0.0675        0.156        0.226          3.8       0.0594\n",
            "Wall time: 151.55635815699998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10        0.109        0.102      0.00681        0.227        0.292         4.75       0.0754\n",
            "     93    20       0.0807       0.0792      0.00147        0.206        0.257         2.24       0.0351\n",
            "     93    30        0.138        0.123       0.0147        0.185        0.321         7.09        0.111\n",
            "     93    40       0.0112     8.15e-11       0.0112     5.24e-06     8.25e-06         6.19       0.0967\n",
            "     93    50        0.119        0.101       0.0181        0.239        0.291         7.86        0.123\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2       0.0644       0.0589      0.00547        0.154        0.222         3.22       0.0506\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  152.849    0.002       0.0607      0.00919       0.0699        0.159        0.225         4.32       0.0717\n",
            "! Validation         93  152.849    0.002       0.0606      0.00711       0.0677        0.155        0.225         4.16       0.0651\n",
            "Wall time: 152.85003989899997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10        0.137        0.104       0.0323        0.242        0.295         10.5        0.164\n",
            "     94    20          0.1       0.0971      0.00292        0.226        0.285         3.11       0.0494\n",
            "     94    30       0.0268       0.0268     4.22e-05        0.114        0.149        0.374      0.00593\n",
            "     94    40       0.0123       0.0046      0.00769       0.0358        0.062         5.13       0.0801\n",
            "     94    50       0.0289       0.0288     8.11e-05        0.117        0.155        0.519      0.00823\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2       0.0643       0.0594      0.00492        0.155        0.223         2.95       0.0463\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  154.133    0.002        0.059      0.00879       0.0677        0.158        0.223         4.22       0.0702\n",
            "! Validation         94  154.133    0.002       0.0608      0.00655       0.0673        0.155        0.225         3.96       0.0619\n",
            "Wall time: 154.13355871099998\n",
            "! Best model       94    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10       0.0271       0.0269     0.000211        0.109         0.15        0.837       0.0133\n",
            "     95    20       0.0394       0.0327      0.00665         0.13        0.165         4.77       0.0745\n",
            "     95    30       0.0285       0.0281     0.000404        0.113        0.153         1.16       0.0184\n",
            "     95    40        0.119       0.0921        0.027         0.22        0.277         9.46         0.15\n",
            "     95    50       0.0863       0.0849      0.00135        0.209        0.266         2.15       0.0336\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2       0.0641       0.0593      0.00487        0.155        0.222         2.92       0.0458\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  155.436    0.002       0.0593      0.00989       0.0692         0.16        0.223         4.64       0.0758\n",
            "! Validation         95  155.436    0.002       0.0607      0.00653       0.0672        0.155        0.225         3.94       0.0617\n",
            "Wall time: 155.43670044500004\n",
            "! Best model       95    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10        0.164        0.155      0.00863        0.276         0.36         5.43       0.0849\n",
            "     96    20       0.0587       0.0583     0.000361        0.179        0.221         1.11       0.0174\n",
            "     96    30        0.132        0.127      0.00553        0.178        0.325         2.45        0.068\n",
            "     96    40       0.0423       0.0344      0.00786        0.133         0.17         5.18        0.081\n",
            "     96    50       0.0263       0.0263     2.98e-05        0.107        0.148        0.315      0.00499\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2       0.0643       0.0592      0.00512        0.155        0.222         2.94       0.0462\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  156.732    0.002       0.0632       0.0104       0.0736        0.162         0.23         4.24       0.0686\n",
            "! Validation         96  156.732    0.002       0.0602      0.00689       0.0671        0.154        0.224         3.98       0.0624\n",
            "Wall time: 156.73344207900004\n",
            "! Best model       96    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10      0.00231     7.89e-11      0.00231        5e-06     8.12e-06         2.81        0.044\n",
            "     97    20        0.174         0.15       0.0237        0.273        0.354         9.01        0.141\n",
            "     97    30       0.0426        0.016       0.0266       0.0925        0.116         9.53        0.149\n",
            "     97    40       0.0441       0.0439     0.000179        0.157        0.192        0.783       0.0122\n",
            "     97    50       0.0388       0.0388     2.12e-06         0.13         0.18       0.0837      0.00133\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2       0.0637       0.0589      0.00484        0.154        0.222         2.88       0.0452\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  158.025    0.002       0.0601      0.00862       0.0687        0.158        0.224         4.14       0.0654\n",
            "! Validation         97  158.025    0.002       0.0601      0.00652       0.0667        0.154        0.224         3.92       0.0613\n",
            "Wall time: 158.026151625\n",
            "! Best model       97    0.067\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10       0.0294        0.028       0.0014        0.113        0.153         2.15       0.0341\n",
            "     98    20        0.144        0.131       0.0125        0.261        0.331         6.54        0.102\n",
            "     98    30       0.0127       0.0117      0.00107       0.0783       0.0987         1.91       0.0299\n",
            "     98    40       0.0366        0.034      0.00263        0.122        0.169         2.95       0.0468\n",
            "     98    50       0.0425       0.0414      0.00108        0.148        0.186         1.92         0.03\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2       0.0643       0.0596      0.00472        0.155        0.223         2.82       0.0441\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  159.579    0.002       0.0595      0.00683       0.0664        0.159        0.224         3.62       0.0602\n",
            "! Validation         98  159.579    0.002       0.0606      0.00639        0.067        0.155        0.225         3.86       0.0604\n",
            "Wall time: 159.57946116800008\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10       0.0322       0.0309      0.00125        0.127        0.161         2.03       0.0323\n",
            "     99    20     0.000172     7.18e-11     0.000172     5.38e-06     7.74e-06        0.767        0.012\n",
            "     99    30       0.0779       0.0578       0.0202        0.173         0.22         8.31         0.13\n",
            "     99    40       0.0695       0.0694     7.68e-05        0.189        0.241        0.512      0.00801\n",
            "     99    50       0.0833       0.0796       0.0037        0.203        0.258          3.5       0.0556\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2       0.0638       0.0593      0.00455        0.155        0.222         2.64       0.0413\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  161.365    0.002       0.0608      0.00859       0.0694         0.16        0.226         4.27       0.0703\n",
            "! Validation         99  161.365    0.002       0.0605       0.0062       0.0667        0.154        0.225         3.74       0.0585\n",
            "Wall time: 161.36679622200018\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10       0.0311       0.0228      0.00829       0.0796        0.138         5.32       0.0832\n",
            "    100    20       0.0501       0.0487      0.00141        0.161        0.202          2.2       0.0343\n",
            "    100    30       0.0893       0.0846      0.00474        0.213        0.266         3.97       0.0629\n",
            "    100    40        0.104       0.0825       0.0213        0.206        0.263         8.53        0.133\n",
            "    100    50       0.0124       0.0123     8.88e-05       0.0798        0.101        0.551      0.00861\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2       0.0635       0.0591      0.00441        0.154        0.222         2.59       0.0406\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  163.309    0.002       0.0619       0.0124       0.0743        0.162        0.228         4.89       0.0806\n",
            "! Validation        100  163.309    0.002       0.0606      0.00597       0.0665        0.154        0.225         3.69       0.0578\n",
            "Wall time: 163.31055069000013\n",
            "! Best model      100    0.067\n",
            "! Stop training: max epochs\n",
            "Wall time: 163.3330268220002\n",
            "Cumulative wall time: 163.3330268220002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae █▆▃▃▃▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae █▆▄▃▃▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae █▄▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse █▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss █▄▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e █▅▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae █▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae █▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse █▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR 0.002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall 163.30856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae 0.08057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae 4.89087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae 0.16246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse 0.22751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss 0.07428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e 0.01237\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f 0.0619\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae 0.05778\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae 3.69469\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae 0.15438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse 0.22491\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss 0.06654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e 0.00597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f 0.06057\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall 163.30856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msi\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/anony-mouse-26092402475183654/allegro-tutorial/runs/bh3o27yv?apiKey=5e4262c24ac1ed30e3b7b22a99893c835272cdb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230722_133612-bh3o27yv/logs\u001b[0m\n",
            "CPU times: user 7.61 s, sys: 905 ms, total: 8.51 s\n",
            "Wall time: 21min 40s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls results/silicon-tutorial/si #/best_model.pth"
      ],
      "metadata": {
        "id": "0KRsXlG65jGH",
        "outputId": "1007ca71-d41f-4f08-cc4d-59a9b9de41df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model.pth\tlog\t\t\t metrics_epoch.csv\n",
            "config.yaml\tmetrics_batch_train.csv  metrics_initialization.csv\n",
            "last_model.pth\tmetrics_batch_val.csv\t trainer.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nequip.utils import Config\n",
        "from nequip.model import model_from_config\n",
        "from nequip.data import AtomicData, ASEDataset\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "config = Config.from_file(\"results/silicon-tutorial/si/config.yaml\")\n",
        "\n",
        "#config[\"train_on_keys\"]=[\"forces\", \"total_energy\"]\n",
        "#config[\"model_builders\"] = [\"EnergyModel\", \"PerSpeciesRescale\", \"ForceOutput\", \"RescaleEnergyEtc\"]\n",
        "model = model_from_config(config, initialize=False)\n",
        "d = torch.load('results/silicon-tutorial/si/best_model.pth',map_location=device)\n",
        "model.load_state_dict(d)\n"
      ],
      "metadata": {
        "id": "mg9svjLM5jI2",
        "outputId": "9b769ad1-485c-4929-a517-795f1e537b2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "a0fb_mry5jLl",
        "outputId": "4fc54546-e10e-4a17-8ae3-eae5f91d1cf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RescaleOutput(\n",
              "  (model): GradientOutput(\n",
              "    (func): SequentialGraphNetwork(\n",
              "      (one_hot): OneHotAtomEncoding()\n",
              "      (radial_basis): RadialBasisEdgeEncoding(\n",
              "        (basis): NormalizedBasis(\n",
              "          (basis): BesselBasis()\n",
              "        )\n",
              "        (cutoff): PolynomialCutoff()\n",
              "      )\n",
              "      (spharm): SphericalHarmonicEdgeAttrs(\n",
              "        (sh): SphericalHarmonics()\n",
              "      )\n",
              "      (allegro): Allegro_Module(\n",
              "        (latents): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (env_embed_mlps): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (tps): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (linears): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (env_linears): ModuleList(\n",
              "          (0): Identity()\n",
              "        )\n",
              "        (_env_weighter): MakeWeightedChannels()\n",
              "        (final_latent): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng): ScalarMLP(\n",
              "        (_module): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng_sum): EdgewiseEnergySum()\n",
              "      (per_species_rescale): PerSpeciesScaleShift()\n",
              "      (total_energy_sum): AtomwiseReduce()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jarvis.core.atoms import Atoms\n",
        "from jarvis.db.figshare import get_jid_data\n",
        "atoms = Atoms.from_dict(get_jid_data(jid='JVASP-1002',dataset='dft_3d')['atoms'])\n",
        "ase_atoms = atoms.ase_converter()\n",
        "a = ASEDataset.from_atoms_list([ase_atoms,ase_atoms],extra_fixed_fields={\"r_max\": 5.0})"
      ],
      "metadata": {
        "id": "rxCCG4xx5jOK",
        "outputId": "ceadd49c-8451-416e-f950-ea8971c654d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 76k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n",
            "Loading the zipfile...\n",
            "Loading completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nequip.data import AtomicData, Collater, dataset_from_config, register_fields, AtomicDataDict\n",
        "from nequip.data.transforms import TypeMapper\n",
        "c = Collater.for_dataset(a, exclude_keys=[])\n",
        "a = AtomicData.from_ase(ase_atoms,5)\n",
        "data = AtomicData.to_AtomicDataDict(a)\n",
        "# tm = TypeMapper(chemical_symbol_to_type={\"Si\": 0})\n",
        "tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "data = tm(data)\n",
        "out = model(data)\n"
      ],
      "metadata": {
        "id": "0B5JNf8r7Ijz"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.keys()"
      ],
      "metadata": {
        "id": "yvbS6NaC7U0w",
        "outputId": "d23b14a8-d8e9-44f3-d41d-90ea6b84b736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['edge_index', 'pos', 'cell', 'edge_cell_shift', 'pbc', 'atom_types', 'node_attrs', 'node_features', 'edge_vectors', 'edge_lengths', 'edge_embedding', 'edge_attrs', 'edge_features', 'edge_energy', 'atomic_energy', 'batch', 'total_energy', 'forces'])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out['total_energy'].squeeze().cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "id": "F4QLSATpJmIz",
        "outputId": "a2686d54-6828-4229-cd67-3e700672096e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-10.741132736206055"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out['forces'].squeeze().cpu().detach().numpy() #.tolist()"
      ],
      "metadata": {
        "id": "kFIOTBuOJqWu",
        "outputId": "a36b738e-427b-4df2-fa59-ad4c12d1f9a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.8503770e-05,  9.6802987e-06,  7.3572533e-06],\n",
              "       [-1.8491430e-05, -9.6678314e-06, -7.3700189e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "params"
      ],
      "metadata": {
        "id": "T7lR1ljg9Fum",
        "outputId": "c2e39bd2-a2dd-48d9-9564-78c6fff7da64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37352"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "mZ_0eoNe9j8d",
        "outputId": "9903f5e1-e35a-4553-9ce7-7782ea6dccfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_jit_bailout_depth': 2, '_jit_fusion_strategy': [('DYNAMIC', 3)], 'BesselBasis_trainable': True, 'PolynomialCutoff_p': 6, 'allow_tf32': False, 'append': True, 'ase_args': {'format': 'extxyz'}, 'avg_num_neighbors': 27.008066177368164, 'batch_size': 1, 'chemical_symbol_to_type': {'Si': 0}, 'code_commits': {}, 'dataloader_num_workers': 0, 'dataset': 'ase', 'dataset_extra_fixed_fields': {'r_max': 5.0}, 'dataset_file_name': './Si_data/sitraj.xyz', 'dataset_seed': 123456, 'dataset_statistics_stride': 1, 'default_dtype': 'float32', 'device': 'cuda', 'e3nn_version': '0.5.1', 'early_stopping': None, 'early_stopping_kwargs': None, 'early_stopping_lower_bounds': {'LR': 1e-05}, 'early_stopping_patiences': {'validation_loss': 100}, 'edge_eng_mlp_initialization': 'uniform', 'edge_eng_mlp_latent_dimensions': [32], 'edge_eng_mlp_nonlinearity': None, 'ema_decay': 0.99, 'ema_use_num_updates': True, 'embed_initial_edge': True, 'end_of_batch_callbacks': [], 'end_of_epoch_callbacks': [], 'end_of_train_callbacks': [], 'env_embed_mlp_initialization': 'uniform', 'env_embed_mlp_latent_dimensions': [], 'env_embed_mlp_nonlinearity': None, 'env_embed_multiplicity': 8, 'equivariance_test': 1, 'exclude_keys': [], 'final_callbacks': [], 'grad_anomaly_mode': False, 'init_callbacks': [], 'irreps_edge_sh': '1x0e+1x1o', 'l_max': 1, 'latent_mlp_initialization': 'uniform', 'latent_mlp_latent_dimensions': [128], 'latent_mlp_nonlinearity': 'silu', 'latent_resnet': True, 'learning_rate': 0.002, 'log_batch_freq': 10, 'log_epoch_freq': 1, 'loss_coeffs': {'forces': 1.0, 'total_energy': [1.0, 'PerAtomMSELoss']}, 'lr_scheduler_factor': 0.5, 'lr_scheduler_kwargs': {'cooldown': 0, 'eps': 1e-08, 'factor': 0.5, 'min_lr': 0, 'mode': 'min', 'patience': 50, 'threshold': 0.0001, 'threshold_mode': 'rel', 'verbose': False}, 'lr_scheduler_name': 'ReduceLROnPlateau', 'lr_scheduler_patience': 50, 'max_epochs': 100, 'max_gradient_norm': inf, 'metrics_components': [['forces', 'mae'], ['forces', 'rmse'], ['total_energy', 'mae'], ['total_energy', 'mae', {'PerAtom': True}]], 'metrics_key': 'validation_loss', 'model_builders': ['allegro.model.Allegro', 'PerSpeciesRescale', 'ForceOutput', 'RescaleEnergyEtc'], 'model_debug_mode': False, 'n_train': 50, 'n_val': 10, 'nequip_version': '0.5.6', 'nonscalars_include_parity': True, 'num_layers': 1, 'num_types': 1, 'optimizer_kwargs': {'amsgrad': False, 'betas': (0.9, 0.999), 'capturable': False, 'eps': 1e-08, 'foreach': None, 'maximize': False, 'weight_decay': 0.0}, 'optimizer_name': 'Adam', 'optimizer_params': {'amsgrad': False, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0}, 'parity': 'o3_full', 'r_max': 5.0, 'report_init_validation': True, 'root': 'results/silicon-tutorial', 'run_id': 'bh3o27yv', 'run_name': 'si', 'save_checkpoint_freq': -1, 'save_ema_checkpoint_freq': -1, 'seed': 123456, 'shuffle': True, 'torch_version': '1.12.1+cu102', 'train_idcs': tensor([28045, 23299, 27958, 15848, 28572, 16227, 26309, 20556,  8247, 11326,\n",
              "        10229, 18453, 21268, 21200, 27260,  8500, 29063,  3917, 34709, 31459,\n",
              "        33334, 24133,  8430, 25716,  3258, 12116, 16166, 13173, 30094,  1264,\n",
              "        16616,  5969, 11209, 11514,  1183,   935, 29944, 24324, 23657, 34168,\n",
              "        32145, 25833,  6923, 29876, 31671, 13592, 18533, 31474,  1024, 26994]), 'train_on_keys': ['forces', 'total_energy'], 'train_val_split': 'random', 'two_body_latent_mlp_initialization': 'uniform', 'two_body_latent_mlp_latent_dimensions': [32, 64, 128], 'two_body_latent_mlp_nonlinearity': 'silu', 'type_names': ['Si'], 'use_ema': True, 'val_idcs': tensor([24842, 20434, 17763, 17511, 17148,  2219,  1916,  1688,  9305, 32557]), 'validation_batch_size': 5, 'var_num_neighbors': 1.911603569984436, 'verbose': 'info', 'wandb': True, 'wandb_project': 'allegro-tutorial'}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqfWeB-TIJXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}