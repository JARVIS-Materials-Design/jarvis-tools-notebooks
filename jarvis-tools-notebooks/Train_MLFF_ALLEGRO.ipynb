{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmnLToGZsmE3BJ3eQk+Z7k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knc6/jarvis-tools-notebooks/blob/master/jarvis-tools-notebooks/Train_MLFF_ALLEGRO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install wandb\n",
        "!pip install -q wandb\n",
        "\n",
        "# install nequip\n",
        "!pip install -q nequip==0.5.5 torch==1.11  jarvis-tools\n",
        "\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJl9OzZZKWh_",
        "outputId": "613f08bb-8182-48eb-d4a0-bfb2b6179f31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.7/975.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://figshare.com/ndownloader/files/40357663 -O mlearn.json.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezhXOfk0jynn",
        "outputId": "8cb68c09-55e3-4179-e9d0-f58008406288"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-07 00:36:52--  https://figshare.com/ndownloader/files/40357663\n",
            "Resolving figshare.com (figshare.com)... 34.241.157.25, 54.155.30.132, 2a05:d018:1f4:d000:fd71:4bc5:e7e5:3f50, ...\n",
            "Connecting to figshare.com (figshare.com)|34.241.157.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240107/eu-west-1/s3/aws4_request&X-Amz-Date=20240107T003652Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=b1e93646f5cacbd732f88f2e442f6b899fd7b6db5a1f04061abef9c446589ef6 [following]\n",
            "--2024-01-07 00:36:52--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240107/eu-west-1/s3/aws4_request&X-Amz-Date=20240107T003652Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=b1e93646f5cacbd732f88f2e442f6b899fd7b6db5a1f04061abef9c446589ef6\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.24.155, 52.218.41.251, 52.218.0.155, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.24.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542319 (2.4M) [application/zip]\n",
            "Saving to: ‘mlearn.json.zip’\n",
            "\n",
            "mlearn.json.zip     100%[===================>]   2.42M  3.56MB/s    in 0.7s    \n",
            "\n",
            "2024-01-07 00:36:54 (3.56 MB/s) - ‘mlearn.json.zip’ saved [2542319/2542319]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json,zipfile\n",
        "mlearn = json.loads(\n",
        "        zipfile.ZipFile(\"mlearn.json.zip\").read(\n",
        "            \"mlearn.json\"\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "RbUdtA3Hj3H9",
        "outputId": "a7083326-7ae9-467b-bea6-b123ff1a2743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'mlearn.json.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5a38a04d8823>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m mlearn = json.loads(\n\u001b[0;32m----> 3\u001b[0;31m         zipfile.ZipFile(\"mlearn.json.zip\").read(\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0;34m\"mlearn.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         )\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mlearn.json.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install allegro\n",
        "!git clone --depth 1 https://github.com/mir-group/allegro.git\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FqDN7vTJ8ds",
        "outputId": "b5e57a48-440b-4df2-b602-f7c3f66974d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 29 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (44/44), 71.97 KiB | 1.28 MiB/s, done.\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nequip>=0.5.3 (from mir-allegro==0.2.0)\n",
            "  Downloading nequip-0.5.6-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.6/145.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (1.23.5)\n",
            "Collecting ase (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached ase-3.22.1-py3-none-any.whl (2.2 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (4.66.1)\n",
            "Collecting torch!=1.9.0,<1.13,>=1.10.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting e3nn<0.6.0,>=0.4.4 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached e3nn-0.5.1-py3-none-any.whl (118 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from nequip>=0.5.3->mir-allegro==0.2.0) (6.0.1)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Using cached torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.11.4)\n",
            "Collecting opt-einsum-fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.9.0,<1.13,>=1.10.0->nequip>=0.5.3->mir-allegro==0.2.0) (4.5.0)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from opt-einsum-fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip>=0.5.3->mir-allegro==0.2.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.1.0->ase->nequip>=0.5.3->mir-allegro==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: mir-allegro\n",
            "  Building wheel for mir-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-allegro: filename=mir_allegro-0.2.0-py3-none-any.whl size=27432 sha256=54302f4d102faf7cadf12e37e444be8223bfbbadfc686ac2c834f5ac981e5198\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0ap8sufz/wheels/b4/da/7a/12e336aa57ba27cca94b3d21b0f02fa0c6c86f7e1f2b7a4195\n",
            "Successfully built mir-allegro\n",
            "Installing collected packages: torch-runstats, torch, torch-ema, opt-einsum-fx, e3nn, ase, nequip, mir-allegro\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ase-3.22.1 e3nn-0.5.1 mir-allegro-0.2.0 nequip-0.5.6 opt-einsum-fx-0.1.4 torch-1.12.1 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists('jarvis_leaderboard'):\n",
        "  !git clone https://github.com/usnistgov/jarvis_leaderboard.git\n",
        "os.chdir('jarvis_leaderboard')\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQJl67kKkJKB",
        "outputId": "ca5da33e-ec20-49aa-c272-45ceb530ee66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'jarvis_leaderboard'...\n",
            "remote: Enumerating objects: 57850, done.\u001b[K\n",
            "remote: Counting objects: 100% (6362/6362), done.\u001b[K\n",
            "remote: Compressing objects: 100% (786/786), done.\u001b[K\n",
            "remote: Total 57850 (delta 3605), reused 5829 (delta 3298), pack-reused 51488\u001b[K\n",
            "Receiving objects: 100% (57850/57850), 380.99 MiB | 28.81 MiB/s, done.\n",
            "Resolving deltas: 100% (30571/30571), done.\n",
            "Updating files: 100% (3622/3622), done.\n",
            "Obtaining file:///content/jarvis_leaderboard\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (1.11.4)\n",
            "Collecting jarvis-tools>=2021.07.19 (from jarvis-leaderboard==2023.12.16)\n",
            "  Using cached jarvis_tools-2023.12.12-py2.py3-none-any.whl (975 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.2.4 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (1.5.3)\n",
            "Collecting rouge>=1.0.1 (from jarvis-leaderboard==2023.12.16)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting mkdocs>=1.5.2 (from jarvis-leaderboard==2023.12.16)\n",
            "  Downloading mkdocs-1.5.3-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs-material>=9.0.5 (from jarvis-leaderboard==2023.12.16)\n",
            "  Downloading mkdocs_material-9.5.3-py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic>=2.3.0 (from jarvis-leaderboard==2023.12.16)\n",
            "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (3.5.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (5.15.0)\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (1.4.0)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from jarvis-leaderboard==2023.12.16) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2023.12.16) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2023.12.16) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2023.12.16) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->jarvis-leaderboard==2023.12.16) (4.66.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (3.7.1)\n",
            "Collecting spglib>=1.14.1 (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading spglib-2.2.0-cp310-cp310-manylinux_2_17_x86_64.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (2.31.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (0.12.0)\n",
            "Collecting xmltodict>=0.11.0 (from jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting ghp-import>=1.0 (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16) (3.1.2)\n",
            "Requirement already satisfied: markupsafe>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16) (2.1.3)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: packaging>=20.5 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16) (23.2)\n",
            "Collecting pathspec>=0.11.1 (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16) (4.1.0)\n",
            "Collecting pyyaml-env-tag>=0.1 (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading pyyaml_env_tag-0.1-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16) (6.0.1)\n",
            "Collecting watchdog>=2.0 (from mkdocs>=1.5.2->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel~=2.10 in /usr/local/lib/python3.10/dist-packages (from mkdocs-material>=9.0.5->jarvis-leaderboard==2023.12.16) (2.14.0)\n",
            "Collecting colorama~=0.4 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting mkdocs-material-extensions~=1.3 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading mkdocs_material_extensions-1.3.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting paginate~=0.5 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading paginate-0.5.6.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygments~=2.16 in /usr/local/lib/python3.10/dist-packages (from mkdocs-material>=9.0.5->jarvis-leaderboard==2023.12.16) (2.16.1)\n",
            "Collecting pymdown-extensions~=10.2 (from mkdocs-material>=9.0.5->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading pymdown_extensions-10.7-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->jarvis-leaderboard==2023.12.16) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.4->jarvis-leaderboard==2023.12.16) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.3.0->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.6 (from pydantic>=2.3.0->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.6.1 (from pydantic>=2.3.0->jarvis-leaderboard==2023.12.16)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge>=1.0.1->jarvis-leaderboard==2023.12.16) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.1->jarvis-leaderboard==2023.12.16) (3.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->jarvis-leaderboard==2023.12.16) (8.2.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->jarvis-tools>=2021.07.19->jarvis-leaderboard==2023.12.16) (2023.11.17)\n",
            "Building wheels for collected packages: paginate\n",
            "  Building wheel for paginate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paginate: filename=paginate-0.5.6-py3-none-any.whl size=12666 sha256=a31f227ff49338ae49e32bd2ac52594e7b1416b882f30d52ca871c68d3e34b41\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/d3/18/0b5bebc873f29bea61fedece1e92cbcbef416839dfe5bd0eef\n",
            "Successfully built paginate\n",
            "Installing collected packages: paginate, xmltodict, watchdog, typing-extensions, spglib, rouge, pyyaml-env-tag, pymdown-extensions, pathspec, mkdocs-material-extensions, mergedeep, colorama, annotated-types, pydantic-core, ghp-import, pydantic, mkdocs, mkdocs-material, jarvis-tools, jarvis-leaderboard\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Running setup.py develop for jarvis-leaderboard\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 colorama-0.4.6 ghp-import-2.1.0 jarvis-leaderboard-2023.12.16 jarvis-tools-2023.12.12 mergedeep-1.3.4 mkdocs-1.5.3 mkdocs-material-9.5.3 mkdocs-material-extensions-1.3.1 paginate-0.5.6 pathspec-0.12.1 pydantic-2.5.3 pydantic-core-2.14.6 pymdown-extensions-10.7 pyyaml-env-tag-0.1 rouge-1.0.1 spglib-2.2.0 typing-extensions-4.9.0 watchdog-3.0.0 xmltodict-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,glob,sys,yaml\n",
        "import zipfile\n",
        "import json\n",
        "import pandas as pd\n",
        "from jarvis.db.figshare import data\n",
        "from jarvis.core.atoms import Atoms\n",
        "import numpy as np\n",
        "from nequip.data import AtomicData, Collater, dataset_from_config, register_fields, AtomicDataDict\n",
        "from nequip.data.transforms import TypeMapper\n",
        "import fileinput\n",
        "import torch\n",
        "os.chdir('/content')\n",
        "#torch.cuda.is_available = lambda : False\n",
        "with open('allegro/configs/tutorial.yaml','r') as f:\n",
        "    txt=f.read()\n",
        "\n",
        "tut = yaml.load(txt, Loader=yaml.Loader)\n",
        "\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n",
        "cmd = \"wandb offline\"\n",
        "os.system(cmd)\n",
        "#mlearn = data(\"mlearn\")\n",
        "elements = [\"Si\"] #[\"Ni\", \"Si\", \"Ge\", \"Mo\", \"Cu\", \"Li\"]\n",
        "\n",
        "# def replaceAll(filename,searchExp,replaceExp):\n",
        "#     with open(filename, \"r\") as file:\n",
        "#          filedata = file.read().splitlines()\n",
        "#     content = []\n",
        "#     for j in filedata:\n",
        "#         if searchExp in j:\n",
        "#            content.append(replaceExp)\n",
        "#         else:\n",
        "#             content.append(j)\n",
        "#     with open(filename, \"w\") as file:\n",
        "#          file.write(\"\\n\".join(content))\n",
        "for element in elements:\n",
        "    # os.chdir('./allegro')\n",
        "    cmd = \"rm -r Si_data\"\n",
        "    os.system(cmd)\n",
        "    folder = \"Si_data\"\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    benchmark_energies = (\n",
        "        \"jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_\"\n",
        "        + element\n",
        "        + \"_energy.json.zip\"\n",
        "    )\n",
        "    temp_energies = benchmark_energies.split(\"/\")[-1].split(\".zip\")[0]\n",
        "    energies = json.loads(\n",
        "        zipfile.ZipFile(benchmark_energies).read(temp_energies)\n",
        "    )\n",
        "    train_ids = list(energies[\"train\"].keys())\n",
        "    test_ids = list(energies[\"test\"].keys())\n",
        "\n",
        "    f = open(\"Si_data/sitraj.xyz\", \"w\")\n",
        "    line = \"\"\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in train_ids:\n",
        "            # print(i)\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            line += str(atoms.num_atoms) + \"\\n\"\n",
        "            line += (\n",
        "                \"Lattice=\"\n",
        "                + '\"'\n",
        "                + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "                + '\"'\n",
        "                + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "                + str(i[\"energy\"])\n",
        "                # + ' stress=\"'\n",
        "                # + \" \".join(map(str, np.array(i[\"stresses\"]).flatten()))\n",
        "                # + '\"'\n",
        "                + \" free_energy=\"\n",
        "                + str(i[\"energy\"])\n",
        "                + ' pbc=\"T T T\"'\n",
        "                + \"\\n\"\n",
        "            )\n",
        "            for m, n, p in zip(\n",
        "                atoms.elements, atoms.cart_coords, i[\"forces\"]\n",
        "            ):\n",
        "                line += (\n",
        "                    str(m)\n",
        "                    + \" \"\n",
        "                    + \" \".join(map(str, n))\n",
        "                    + \" \"\n",
        "                    + \" \".join(map(str, p))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "            # print(line)\n",
        "            f.write(line)\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in test_ids:\n",
        "            # print(i)\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            line += str(atoms.num_atoms) + \"\\n\"\n",
        "            line += (\n",
        "                \"Lattice=\"\n",
        "                + '\"'\n",
        "                + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "                + '\"'\n",
        "                + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "                + str(i[\"energy\"])\n",
        "                # + ' stress=\"'\n",
        "                # + \" \".join(map(str, np.array(i[\"stresses\"]).flatten()))\n",
        "                # + '\"'\n",
        "                + \" free_energy=\"\n",
        "                + str(i[\"energy\"])\n",
        "                + ' pbc=\"T T T\"'\n",
        "                + \"\\n\"\n",
        "            )\n",
        "            for m, n, p in zip(\n",
        "                atoms.elements, atoms.cart_coords, i[\"forces\"]\n",
        "            ):\n",
        "                line += (\n",
        "                    str(m)\n",
        "                    + \" \"\n",
        "                    + \" \".join(map(str, n))\n",
        "                    + \" \"\n",
        "                    + \" \".join(map(str, p))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "            f.write(line)\n",
        "            # print(line)\n",
        "    for i in mlearn:\n",
        "        if i[\"jid\"] in test_ids:\n",
        "            # print(i)\n",
        "            atoms = Atoms.from_dict(i[\"atoms\"])\n",
        "            line += str(atoms.num_atoms) + \"\\n\"\n",
        "            line += (\n",
        "                \"Lattice=\"\n",
        "                + '\"'\n",
        "                + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "                + '\"'\n",
        "                + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "                + str(i[\"energy\"])\n",
        "                # + ' stress=\"'\n",
        "                # + \" \".join(map(str, np.array(i[\"stresses\"]).flatten()))\n",
        "                # + '\"'\n",
        "                + \" free_energy=\"\n",
        "                + str(i[\"energy\"])\n",
        "                + ' pbc=\"T T T\"'\n",
        "                + \"\\n\"\n",
        "            )\n",
        "            for m, n, p in zip(\n",
        "                atoms.elements, atoms.cart_coords, i[\"forces\"]\n",
        "            ):\n",
        "                line += (\n",
        "                    str(m)\n",
        "                    + \" \"\n",
        "                    + \" \".join(map(str, n))\n",
        "                    + \" \"\n",
        "                    + \" \".join(map(str, p))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "            # print(line)\n",
        "            f.write(line)\n",
        "    f.close()\n",
        "    cmd = \"rm -rf ./results\"\n",
        "    os.system(cmd)\n",
        "\n",
        "    yaml_f = 'allegro/configs/tutorial_'+element+'.yaml'\n",
        "\n",
        "    cmd = 'cp allegro/configs/tutorial.yaml allegro/configs/tutorial_'+element+'.yaml'\n",
        "    os.system(cmd)\n",
        "    tmp=\"  \"+element+\": 0\"\n",
        "    #replaceAll(yaml_f,\"Si: 0\",tmp)\n",
        "    tut['chemical_symbol_to_type'] ={element: 0}\n",
        "    tut['n_train'] = len(train_ids)\n",
        "    tut['shuffle'] = False\n",
        "    tut['n_test'] = len(test_ids)\n",
        "    tut['n_val'] = len(test_ids)\n",
        "    tut['batch_size'] = 1\n",
        "    with open(yaml_f, \"w+\") as fp:\n",
        "        yaml.dump(tut,fp)\n",
        "    cmd = \"nequip-train allegro/configs/tutorial_\"+element+\".yaml  --equivariance-test\"\n",
        "    os.system(cmd)\n",
        "    print('FINISHED')\n",
        "    import torch\n",
        "    from nequip.utils import Config\n",
        "    from nequip.model import model_from_config\n",
        "    from nequip.data import AtomicData, ASEDataset\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    config = Config.from_file(\n",
        "        \"results/silicon-tutorial/si/config.yaml\"\n",
        "    )\n",
        "\n",
        "    # config[\"train_on_keys\"]=[\"forces\", \"total_energy\"]\n",
        "    # config[\"model_builders\"] = [\"EnergyModel\", \"PerSpeciesRescale\", \"ForceOutput\", \"RescaleEnergyEtc\"]\n",
        "    model = model_from_config(config, initialize=False)\n",
        "    d = torch.load(\n",
        "        \"results/silicon-tutorial/si/best_model.pth\",\n",
        "        map_location=device,\n",
        "    )\n",
        "    model.load_state_dict(d)\n",
        "\n",
        "    df = pd.DataFrame(mlearn)\n",
        "\n",
        "    def get_allegro_forces(model=[], atoms=[], cutoff=5):\n",
        "        ase_atoms = atoms.ase_converter()\n",
        "        a = AtomicData.from_ase(ase_atoms, cutoff)\n",
        "        data = AtomicData.to_AtomicDataDict(a)\n",
        "        tm = TypeMapper(\n",
        "            chemical_symbol_to_type=config[\"chemical_symbol_to_type\"]\n",
        "        )\n",
        "        data = tm(data)\n",
        "        out = model(data)\n",
        "        pen = (\n",
        "            out[\"total_energy\"]\n",
        "            .squeeze()\n",
        "            .cpu()\n",
        "            .detach()\n",
        "            .numpy()\n",
        "            .tolist()\n",
        "        )\n",
        "        num_atoms = atoms.num_atoms\n",
        "        pf = out[\"forces\"].squeeze().cpu().detach().numpy()\n",
        "        return pen, pf, 0\n",
        "\n",
        "\n",
        "    for i in glob.glob(\"jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/*energy*.zip\"):\n",
        "\n",
        "        if \"mlearn\" in i and element in i:\n",
        "            fname_e = (\n",
        "                \"AI-MLFF-energy-\"\n",
        "                + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "                + \"-test-mae.csv\"\n",
        "            )\n",
        "            fname_f = (\n",
        "                \"AI-MLFF-forces-\"\n",
        "                + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "                + \"-test-multimae.csv\"\n",
        "            )\n",
        "            fname_s = (\n",
        "                \"AI-MLFF-stresses-\"\n",
        "                + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "                + \"-test-multimae.csv\"\n",
        "            )\n",
        "            f_e = open(fname_e, \"w\")\n",
        "            f_f = open(fname_f, \"w\")\n",
        "            # f_s = open(fname_s, \"w\")\n",
        "\n",
        "            f_e.write(\"id,target,prediction\\n\")\n",
        "            f_f.write(\"id,target,prediction\\n\")\n",
        "            # f_s.write(\"id,prediction\\n\")\n",
        "            #\n",
        "            print(i)\n",
        "            dat = json.loads(\n",
        "                zipfile.ZipFile(i).read(\n",
        "                    i.split(\"/\")[-1].split(\".zip\")[0]\n",
        "                )\n",
        "            )\n",
        "            print(dat[\"test\"])\n",
        "            for key, val in dat[\"test\"].items():\n",
        "                entry = df[df[\"jid\"] == key]\n",
        "                atoms = Atoms.from_dict(entry.atoms.values[0])\n",
        "                # print(key,val,df[df['jid']==key],atoms)\n",
        "                # energy,forces=get_alignn_forces(atoms)\n",
        "                energy, forces, stress = get_allegro_forces(\n",
        "                    model=model, atoms=atoms\n",
        "                )\n",
        "                print(key, val, energy, atoms.num_atoms)\n",
        "                line = (\n",
        "                    key\n",
        "                    + \",\"\n",
        "                    + str(entry.energy.values[0])\n",
        "                    + \",\"\n",
        "                    + str(energy)\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f_e.write(line)\n",
        "                line = (\n",
        "                    key\n",
        "                    + \",\"\n",
        "                    + str(\n",
        "                        \";\".join(\n",
        "                            map(\n",
        "                                str,\n",
        "                                np.array(\n",
        "                                    entry.forces.values[0]\n",
        "                                ).flatten(),\n",
        "                            )\n",
        "                        )\n",
        "                    )\n",
        "                    + \",\"\n",
        "                    + str(\n",
        "                        \";\".join(map(str, np.array(forces).flatten()))\n",
        "                    )\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "                f_f.write(line)\n",
        "                # line = (\n",
        "                #     key\n",
        "                #     + \",\"\n",
        "                #     + str(\";\".join(map(str, np.array(stress).flatten())))\n",
        "                #     + \"\\n\"\n",
        "                # )\n",
        "                # f_s.write(line)\n",
        "            f_e.close()\n",
        "            f_f.close()\n",
        "            # f_s.close()\n",
        "            zname = fname_e + \".zip\"\n",
        "            with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "                myzip.write(fname_e)\n",
        "\n",
        "            zname = fname_f + \".zip\"\n",
        "            with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "                myzip.write(fname_f)\n",
        "\n",
        "            # zname = fname_s + \".zip\"\n",
        "            # with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            #     myzip.write(fname_s)\n"
      ],
      "metadata": {
        "id": "cKjZaxI0Vh-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "EWsdNAt3Vh7m",
        "outputId": "e1e8c8bf-e33d-41ad-cb59-88032a96da7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jarvis_leaderboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fv9Nep_1Vh5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u4WOZpuRVh2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element='Si'\n",
        "os.chdir('/content/allegro')\n",
        "folder='/content/Si_data'\n",
        "if not os.path.exists(folder):\n",
        "   os.makedirs(folder)\n",
        "os.chdir(folder)\n",
        "import os,zipfile,json\n",
        "import pandas as pd\n",
        "from jarvis.db.figshare import data\n",
        "from jarvis.core.atoms import Atoms\n",
        "import numpy as np\n",
        "\n",
        "benchmark_energies='/content/jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_'+element+'_energy.json.zip'\n",
        "#benchmark_forces='jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_Si_forces.json.zip'\n",
        "#benchmark_stresses='jarvis_leaderboard/jarvis_leaderboard/benchmarks/AI/MLFF/mlearn_Si_stresses.json.zip'\n",
        "\n",
        "\n",
        "temp_energies=benchmark_energies.split('/')[-1].split('.zip')[0]\n",
        "#temp_forces=benchmark_forces.split('/')[-1].split('.zip')[0]\n",
        "#temp_stresses=benchmark_stresses.split('/')[-1].split('.zip')[0]\n",
        "energies = json.loads(zipfile.ZipFile(benchmark_energies).read(temp_energies))\n",
        "#forces = json.loads(zipfile.ZipFile(benchmark_forces).read(temp_forces))\n",
        "#stresses = json.loads(zipfile.ZipFile(benchmark_stresses).read(temp_stresses))\n",
        "train_ids = list(energies['train'].keys())\n",
        "test_ids = list(energies['test'].keys())\n",
        "f=open('/content/Si_data/sitraj.xyz','w')\n",
        "line = \"\"\n",
        "for i in mlearn:\n",
        " if i['jid'] in train_ids:\n",
        "    # print(i)\n",
        "    atoms=Atoms.from_dict(i['atoms'])\n",
        "    line += str(atoms.num_atoms) + \"\\n\"\n",
        "    line += (\n",
        "        \"Lattice=\"\n",
        "        + '\"'\n",
        "        + \" \".join(map(str, (atoms.lattice_mat).flatten()))\n",
        "        + '\"'\n",
        "        + \" Properties=species:S:1:pos:R:3:forces:R:3 energy=\"\n",
        "        + str(i['energy'])\n",
        "        #+ ' stress=\"'\n",
        "        #+ \" \".join(map(str, np.array(i[\"stresses\"]).flatten()))\n",
        "        #+ '\"'\n",
        "        + \" free_energy=\"\n",
        "        + str(i['energy'])\n",
        "        + ' pbc=\"T T T\"'\n",
        "        + \"\\n\"\n",
        "    )\n",
        "    for m, n, p in zip(atoms.elements, atoms.cart_coords, i['forces']):\n",
        "        line += (\n",
        "            str(m)\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, n))\n",
        "            + \" \"\n",
        "            + \" \".join(map(str, p))\n",
        "            + \"\\n\"\n",
        "        )\n",
        "    # print(line)\n",
        "    f.write(line)\n",
        "f.close()\n",
        "os.chdir('/content/')"
      ],
      "metadata": {
        "id": "eHZ7KI2lk4Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgGyocOwkx7M",
        "outputId": "cc5e252a-56bc-482a-c8c0-c94587b2852c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# fix colab imports\n",
        "import site\n",
        "site.main()\n",
        "\n",
        "# set to allow anonymous WandB\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\"\n"
      ],
      "metadata": {
        "id": "huyOqxYmY5G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb offline"
      ],
      "metadata": {
        "id": "YqyWPpp2yx66",
        "outputId": "c944d800-95ac-4b96-901e-351f704bf72a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!rm -rf ./results\n",
        "!nequip-train allegro/configs/tutorial.yaml --equivariance-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiIbyRqIZtIW",
        "outputId": "a82387e2-bead-458c-f1b0-f3d50f36d0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id yb0moznz.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "Torch device: cuda\n",
            "Processing dataset...\n",
            "Loaded data: Batch(atomic_numbers=[1400512, 1], batch=[1400512], cell=[23005, 3, 3], edge_cell_shift=[37856782, 3], edge_index=[2, 37856782], forces=[1400512, 3], free_energy=[23005], pbc=[23005, 3], pos=[1400512, 3], ptr=[23006], total_energy=[23005, 1])\n",
            "    processed data size: ~1065.52 MB\n",
            "Cached processed data to disk\n",
            "Done!\n",
            "Successfully loaded the data set of type ASEDataset(23005)...\n",
            "Replace string dataset_forces_rms to 0.9179399013519287\n",
            "Replace string dataset_per_atom_total_energy_mean to -5.100671291351318\n",
            "Atomic outputs are scaled by: [Si: 0.917940], shifted by [Si: -5.100671].\n",
            "Replace string dataset_forces_rms to 0.9179399013519287\n",
            "Initially outputs are globally scaled by: 0.9179399013519287, total_energy are globally shifted by None.\n",
            "Successfully built the network...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:276: UserWarning: operator() profile_node %884 : int[] = prim::profile_ivalue(%882)\n",
            " does not have profile information (Triggered internally at  ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:108.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "Equivariance test passed; equivariance errors:\n",
            "   Errors are in real units, where relevant.\n",
            "   Please note that the large scale of the typical\n",
            "   shifts to the (atomic) energy can cause\n",
            "   catastrophic cancellation and give incorrectly\n",
            "   the equivariance error as zero for those fields.\n",
            "   node permutation equivariance of field pos                        -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field cell                 -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field free_energy          -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field total_energy         -> max error=9.155e-05\n",
            "   node permutation equivariance of field forces                     -> max error=2.086e-07\n",
            "   edge permutation equivariance of field edge_cell_shift            -> max error=0.000e+00\n",
            "   edge & node permutation invariance for field pbc                  -> max error=0.000e+00\n",
            "   node permutation equivariance of field atom_types                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_attrs                 -> max error=0.000e+00\n",
            "   node permutation equivariance of field node_features              -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_vectors               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_lengths               -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_embedding             -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_attrs                 -> max error=0.000e+00\n",
            "   edge permutation equivariance of field edge_features              -> max error=5.960e-08\n",
            "   edge permutation equivariance of field edge_energy                -> max error=7.451e-08\n",
            "   node permutation equivariance of field atomic_energy              -> max error=4.768e-07\n",
            "   node permutation equivariance of field batch                      -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=pos                 )     -> max error=4.744e-07\n",
            "   (parity_k=0, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=edge_embedding      )     -> max error=8.420e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_attrs          )     -> max error=2.260e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_features       )     -> max error=4.581e-06\n",
            "   (parity_k=0, did_translate=False, field=edge_energy         )     -> max error=4.023e-07\n",
            "   (parity_k=0, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=False, field=forces              )     -> max error=2.817e-06\n",
            "   (parity_k=0, did_translate=True , field=pos                 )     -> max error=9.273e-07\n",
            "   (parity_k=0, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=edge_embedding      )     -> max error=8.420e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_attrs          )     -> max error=4.348e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_features       )     -> max error=9.187e-06\n",
            "   (parity_k=0, did_translate=True , field=edge_energy         )     -> max error=5.364e-07\n",
            "   (parity_k=0, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=0, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=0, did_translate=True , field=forces              )     -> max error=3.112e-06\n",
            "   (parity_k=1, did_translate=False, field=pos                 )     -> max error=4.603e-07\n",
            "   (parity_k=1, did_translate=False, field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=edge_embedding      )     -> max error=8.420e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_attrs          )     -> max error=2.803e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_features       )     -> max error=5.245e-06\n",
            "   (parity_k=1, did_translate=False, field=edge_energy         )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=False, field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=False, field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=False, field=forces              )     -> max error=2.379e-06\n",
            "   (parity_k=1, did_translate=True , field=pos                 )     -> max error=9.520e-07\n",
            "   (parity_k=1, did_translate=True , field=edge_index          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_attrs          )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=node_features       )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=edge_embedding      )     -> max error=8.604e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_attrs          )     -> max error=1.849e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_features       )     -> max error=4.716e-06\n",
            "   (parity_k=1, did_translate=True , field=edge_energy         )     -> max error=5.662e-07\n",
            "   (parity_k=1, did_translate=True , field=atomic_energy       )     -> max error=4.768e-07\n",
            "   (parity_k=1, did_translate=True , field=total_energy        )     -> max error=0.000e+00\n",
            "   (parity_k=1, did_translate=True , field=forces              )     -> max error=1.379e-06\n",
            "Number of weights: 37352\n",
            "Number of trainable weights: 37352\n",
            "! Starting training ...\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      0     2         1.48         1.22         0.26        0.719         1.01           25         0.39\n",
            "\n",
            "\n",
            "  Initialization     #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Initial Validation          0    4.031    0.002         1.15        0.223         1.37        0.669        0.983         21.8        0.338\n",
            "Wall time: 4.031521104999911\n",
            "! Best model        0    1.373\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1    10         1.08        0.976        0.109        0.676        0.907         19.1        0.303\n",
            "      1    20        0.834        0.828      0.00663        0.665        0.835         4.78       0.0748\n",
            "      1    30        0.126        0.124      0.00232        0.256        0.323         2.83       0.0442\n",
            "      1    40        0.522        0.455       0.0672        0.468        0.619         15.2        0.238\n",
            "      1    50        0.542        0.476       0.0661        0.478        0.633         15.1        0.236\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      1     2        0.476        0.397        0.079        0.412        0.578         16.3        0.256\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1   15.167    0.002         0.42        0.142        0.562        0.412          0.6         17.7        0.282\n",
            "! Validation          1   15.167    0.002        0.368       0.0781        0.446        0.384        0.556         16.8        0.251\n",
            "Wall time: 15.168190613999968\n",
            "! Best model        1    0.446\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2    10        0.194        0.185      0.00888        0.246        0.395         1.38       0.0865\n",
            "      2    20        0.299        0.256       0.0423        0.365        0.465         12.1        0.189\n",
            "      2    30         0.15        0.146      0.00358        0.281        0.351         3.52       0.0549\n",
            "      2    40        0.204     8.29e-11        0.204     5.64e-06     8.36e-06         26.5        0.415\n",
            "      2    50         0.75        0.532        0.219        0.516        0.669         27.5        0.429\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      2     2        0.363        0.304       0.0598        0.367        0.506         13.6        0.213\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               2   16.491    0.002        0.232       0.0712        0.303        0.311        0.442         13.3         0.21\n",
            "! Validation          2   16.491    0.002        0.274       0.0521        0.326        0.338        0.479         12.9        0.197\n",
            "Wall time: 16.492521484999997\n",
            "! Best model        2    0.326\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3    10        0.135        0.115       0.0196        0.252        0.311         8.22        0.128\n",
            "      3    20       0.0496     9.66e-11       0.0496     5.92e-06     9.02e-06         13.1        0.205\n",
            "      3    30      0.00208     1.16e-10      0.00208     6.31e-06     9.88e-06         2.68       0.0419\n",
            "      3    40       0.0983       0.0469       0.0513        0.159        0.199         13.1        0.208\n",
            "      3    50        0.472        0.459       0.0127        0.495        0.622         6.53        0.104\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      3     2        0.296        0.252       0.0432        0.339        0.461         11.8        0.185\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               3   17.782    0.002        0.191       0.0436        0.235        0.284        0.403         9.58        0.152\n",
            "! Validation          3   17.782    0.002        0.225         0.04        0.265         0.31        0.434         11.6        0.177\n",
            "Wall time: 17.78335849899986\n",
            "! Best model        3    0.265\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4    10        0.317        0.257       0.0601        0.367        0.465         14.4        0.225\n",
            "      4    20        0.252        0.184       0.0684        0.313        0.394         15.4         0.24\n",
            "      4    30        0.469        0.231        0.239        0.356        0.441         28.2        0.448\n",
            "      4    40        0.215        0.112        0.103        0.247        0.307         18.9        0.295\n",
            "      4    50       0.0463       0.0454     0.000872        0.153        0.196         1.73       0.0271\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      4     2        0.252        0.194       0.0581        0.301        0.404         12.6        0.197\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               4   19.372    0.002        0.154       0.0569        0.211        0.261         0.36         11.6        0.184\n",
            "! Validation          4   19.372    0.002         0.17       0.0492        0.219        0.274        0.377         11.5        0.178\n",
            "Wall time: 19.3741673479999\n",
            "! Best model        4    0.219\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5    10       0.0635       0.0567      0.00676        0.177        0.219         4.76       0.0755\n",
            "      5    20       0.0451     1.06e-10       0.0451     6.15e-06     9.47e-06         12.5        0.195\n",
            "      5    30        0.116       0.0857       0.0303        0.226        0.269         10.2         0.16\n",
            "      5    40        0.173        0.171      0.00252        0.248        0.379        0.738       0.0461\n",
            "      5    50        0.308        0.214       0.0948        0.323        0.424         17.8        0.283\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      5     2        0.219        0.183       0.0358        0.295        0.392          9.9        0.155\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               5   21.076    0.002         0.14       0.0288        0.168        0.247        0.342          8.4        0.133\n",
            "! Validation          5   21.076    0.002        0.155         0.03        0.185        0.262         0.36         8.98        0.139\n",
            "Wall time: 21.076570393999873\n",
            "! Best model        5    0.185\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6    10        0.201        0.147       0.0534        0.279        0.352         13.6        0.212\n",
            "      6    20        0.259        0.215       0.0441        0.347        0.426         12.1        0.193\n",
            "      6    30        0.241        0.236      0.00471         0.36        0.446         3.97        0.063\n",
            "      6    40        0.055       0.0508      0.00422        0.168        0.207         3.76       0.0596\n",
            "      6    50       0.0606       0.0333       0.0272        0.132        0.168         9.54        0.151\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      6     2         0.21        0.186       0.0238          0.3        0.396         8.05        0.126\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               6   22.482    0.002        0.133       0.0287        0.161        0.239        0.333         8.22        0.131\n",
            "! Validation          6   22.482    0.002        0.153       0.0199        0.173        0.259        0.357         7.23        0.112\n",
            "Wall time: 22.483142544999964\n",
            "! Best model        6    0.173\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7    10        0.292        0.231       0.0613        0.341        0.441         14.3        0.227\n",
            "      7    20        0.228        0.225      0.00255        0.352        0.435         2.97       0.0463\n",
            "      7    30        0.254        0.182       0.0718        0.309        0.392         15.7        0.246\n",
            "      7    40       0.0453       0.0424      0.00294        0.155        0.189         3.13       0.0497\n",
            "      7    50       0.0532        0.047      0.00621        0.159        0.199         4.56       0.0723\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      7     2        0.205        0.185       0.0199          0.3        0.395         7.32        0.115\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               7   23.814    0.002        0.126       0.0143        0.141        0.233        0.325         5.68       0.0902\n",
            "! Validation          7   23.814    0.002        0.149       0.0165        0.166        0.257        0.352          6.5        0.101\n",
            "Wall time: 23.815532931000007\n",
            "! Best model        7    0.166\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8    10         0.14        0.131      0.00893        0.266        0.333         5.55       0.0868\n",
            "      8    20        0.241        0.198       0.0428        0.329        0.409         12.2         0.19\n",
            "      8    30       0.0861       0.0764      0.00964        0.203        0.254         5.77       0.0901\n",
            "      8    40        0.147        0.144      0.00285        0.275        0.349         3.14        0.049\n",
            "      8    50       0.0872       0.0755       0.0117        0.196        0.252         6.35       0.0992\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      8     2          0.2        0.181       0.0183        0.298        0.391         6.88        0.108\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               8   25.131    0.002         0.12       0.0138        0.134        0.229        0.317         5.72       0.0904\n",
            "! Validation          8   25.131    0.002        0.144        0.015        0.159        0.253        0.346         6.01        0.094\n",
            "Wall time: 25.13201430999993\n",
            "! Best model        8    0.159\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9    10       0.0323       0.0321     0.000277        0.128        0.164        0.963       0.0153\n",
            "      9    20      0.00136     8.69e-11      0.00136     5.43e-06     8.56e-06         2.16       0.0338\n",
            "      9    30        0.213          0.2       0.0129        0.335        0.411         6.57        0.104\n",
            "      9    40        0.208        0.202      0.00667        0.328        0.412         4.72       0.0749\n",
            "      9    50       0.0575       0.0424       0.0151         0.15        0.189         7.11        0.113\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "      9     2        0.192        0.175       0.0165        0.293        0.384         6.54        0.102\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               9   26.423    0.002        0.117       0.0117        0.128        0.225        0.313          5.2       0.0838\n",
            "! Validation          9   26.423    0.002        0.139       0.0134        0.153        0.249         0.34         5.71       0.0891\n",
            "Wall time: 26.423702390000017\n",
            "! Best model        9    0.153\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10    10       0.0395       0.0312      0.00824         0.13        0.162         5.33       0.0833\n",
            "     10    20       0.0536       0.0506      0.00306        0.166        0.206          3.2       0.0507\n",
            "     10    30       0.0759       0.0754     0.000532         0.21        0.252         1.35       0.0212\n",
            "     10    40         0.17         0.17     0.000198        0.307        0.378        0.813       0.0129\n",
            "     10    50        0.207        0.179       0.0281        0.314        0.388         9.69        0.154\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     10     2        0.187        0.169       0.0175        0.288        0.378         6.61        0.103\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              10   27.729    0.002        0.111       0.0137        0.124        0.221        0.304         5.72       0.0906\n",
            "! Validation         10   27.729    0.002        0.134        0.014        0.148        0.244        0.334          5.8       0.0901\n",
            "Wall time: 27.729969139999866\n",
            "! Best model       10    0.148\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11    10       0.0402       0.0402     2.16e-05        0.145        0.184        0.269      0.00426\n",
            "     11    20     0.000506     1.02e-10     0.000506     5.87e-06     9.25e-06         1.32       0.0206\n",
            "     11    30       0.0376       0.0357      0.00199        0.136        0.173         2.58        0.041\n",
            "     11    40        0.195        0.167       0.0273        0.303        0.375         9.56        0.152\n",
            "     11    50        0.138        0.137      0.00104        0.265        0.339         1.87       0.0296\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     11     2        0.184        0.169       0.0152        0.287        0.378         6.09       0.0953\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              11   29.025    0.002         0.11      0.00758        0.117        0.218        0.302         3.65       0.0585\n",
            "! Validation         11   29.025    0.002        0.133        0.012        0.145        0.243        0.332         5.38       0.0831\n",
            "Wall time: 29.025825082999972\n",
            "! Best model       11    0.145\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12    10         0.11        0.108      0.00222        0.244        0.302         2.77       0.0432\n",
            "     12    20        0.179        0.178      0.00175        0.317        0.387         2.42       0.0384\n",
            "     12    30         0.26         0.26      6.4e-05         0.37        0.468        0.463      0.00734\n",
            "     12    40        0.141        0.136       0.0044        0.265        0.339          3.9       0.0609\n",
            "     12    50       0.0832       0.0813      0.00188        0.213        0.262         2.54       0.0398\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     12     2        0.178        0.162       0.0154         0.28         0.37          6.2        0.097\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              12   30.332    0.002        0.106       0.0105        0.116        0.216        0.298         4.94       0.0787\n",
            "! Validation         12   30.332    0.002        0.128       0.0122         0.14        0.238        0.326         5.45       0.0843\n",
            "Wall time: 30.333176570999967\n",
            "! Best model       12    0.140\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13    10       0.0495       0.0485     0.000939        0.164        0.202         1.77       0.0281\n",
            "     13    20        0.166        0.162      0.00326        0.302         0.37          3.3       0.0524\n",
            "     13    30        0.164        0.148       0.0153        0.282        0.354         7.26        0.113\n",
            "     13    40       0.0389       0.0355      0.00333        0.141        0.173         3.34        0.053\n",
            "     13    50       0.0179     2.01e-10       0.0179     8.65e-06      1.3e-05         7.86        0.123\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     13     2        0.175        0.159        0.015        0.277        0.367         6.03       0.0944\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              13   31.727    0.002        0.105      0.00973        0.115        0.215        0.297         4.45       0.0718\n",
            "! Validation         13   31.727    0.002        0.126       0.0118        0.137        0.237        0.323         5.33        0.082\n",
            "Wall time: 31.728153736999957\n",
            "! Best model       13    0.137\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14    10        0.109     2.03e-10        0.109     8.72e-06     1.31e-05         19.4        0.303\n",
            "     14    20       0.0363       0.0362     2.86e-05        0.142        0.175        0.309      0.00491\n",
            "     14    30         0.19        0.149       0.0415         0.28        0.354           12        0.187\n",
            "     14    40       0.0301       0.0298     0.000312        0.123        0.158         1.02       0.0162\n",
            "     14    50       0.0948       0.0931      0.00171        0.224         0.28         2.43       0.0379\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     14     2        0.172        0.156       0.0158        0.274        0.362         6.07       0.0949\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              14   33.410    0.002        0.102       0.0108        0.112        0.212        0.291          4.4       0.0712\n",
            "! Validation         14   33.410    0.002        0.123       0.0122        0.135        0.234        0.319         5.38       0.0824\n",
            "Wall time: 33.411701068999946\n",
            "! Best model       14    0.135\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15    10        0.116        0.115      0.00102        0.242        0.311         1.88       0.0294\n",
            "     15    20        0.198        0.198     0.000442        0.331        0.408         1.22       0.0193\n",
            "     15    30        0.156        0.156     0.000953        0.294        0.362         1.81       0.0283\n",
            "     15    40        0.127        0.122       0.0047        0.249         0.32         3.97        0.063\n",
            "     15    50       0.0394       0.0385     0.000846        0.141         0.18         1.68       0.0267\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     15     2        0.168        0.154        0.014        0.272         0.36         5.67       0.0887\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              15   35.045    0.002        0.106      0.00584        0.111        0.212        0.297         3.37       0.0542\n",
            "! Validation         15   35.045    0.002        0.121       0.0108        0.132        0.232        0.317         5.04       0.0768\n",
            "Wall time: 35.04632090399991\n",
            "! Best model       15    0.132\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16    10        0.159         0.14       0.0185        0.279        0.344         7.87        0.125\n",
            "     16    20       0.0297        0.024      0.00572        0.113        0.142         4.44       0.0694\n",
            "     16    30       0.0625       0.0488       0.0137        0.162        0.203         6.76        0.107\n",
            "     16    40        0.123         0.12      0.00287        0.261        0.318          3.1       0.0492\n",
            "     16    50        0.164        0.153       0.0105         0.29         0.36         5.92        0.094\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     16     2        0.163        0.149       0.0146        0.268        0.354          5.8       0.0906\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              16   36.332    0.002        0.101       0.0159        0.117        0.211        0.291         5.63       0.0921\n",
            "! Validation         16   36.332    0.002        0.118       0.0112        0.129         0.23        0.313         5.15       0.0786\n",
            "Wall time: 36.33275084000002\n",
            "! Best model       16    0.129\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17    10        0.138        0.138     0.000365        0.271        0.341         1.12       0.0175\n",
            "     17    20       0.0283       0.0152       0.0131       0.0902        0.113         6.73        0.105\n",
            "     17    30        0.203        0.198      0.00547        0.338        0.408         4.28       0.0679\n",
            "     17    40        0.191        0.176       0.0147        0.301        0.385         7.12        0.111\n",
            "     17    50       0.0223     9.59e-11       0.0223     6.19e-06     8.99e-06         8.78        0.137\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     17     2        0.159        0.146       0.0133        0.266        0.351         5.45       0.0851\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              17   37.638    0.002       0.0979      0.00729        0.105        0.204        0.285         3.65       0.0593\n",
            "! Validation         17   37.638    0.002        0.116       0.0103        0.126        0.228         0.31         4.87       0.0739\n",
            "Wall time: 37.639093441999876\n",
            "! Best model       17    0.126\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18    10        0.141        0.139      0.00225        0.279        0.342         2.79       0.0435\n",
            "     18    20         0.02     1.16e-10         0.02     6.69e-06     9.88e-06          8.3         0.13\n",
            "     18    30        0.258        0.225       0.0329        0.338        0.436         10.6        0.166\n",
            "     18    40      0.00212     9.71e-11      0.00212     5.91e-06     9.05e-06          2.7       0.0422\n",
            "     18    50       0.0406       0.0332      0.00738        0.138        0.167         4.97       0.0789\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     18     2        0.157        0.143       0.0137        0.263        0.347         5.49       0.0859\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              18   38.938    0.002       0.0943      0.00906        0.103        0.204         0.28         4.02       0.0662\n",
            "! Validation         18   38.938    0.002        0.113       0.0105        0.124        0.225        0.307         4.93       0.0746\n",
            "Wall time: 38.93854740100005\n",
            "! Best model       18    0.124\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19    10        0.176        0.157       0.0194        0.287        0.364         8.05        0.128\n",
            "     19    20       0.0475     2.29e-10       0.0475      9.3e-06     1.39e-05         12.8          0.2\n",
            "     19    30        0.138        0.134      0.00411        0.264        0.336         3.77       0.0588\n",
            "     19    40       0.0798       0.0782      0.00159        0.192        0.257         2.34       0.0366\n",
            "     19    50        0.149        0.148     0.000337        0.291        0.354         1.06       0.0168\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     19     2        0.153        0.139       0.0134         0.26        0.343         5.44       0.0851\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              19   40.231    0.002        0.101      0.00941         0.11        0.209        0.291         4.38       0.0694\n",
            "! Validation         19   40.231    0.002         0.11       0.0103        0.121        0.223        0.303         4.89       0.0739\n",
            "Wall time: 40.23273170099992\n",
            "! Best model       19    0.121\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20    10        0.158        0.128       0.0292        0.266        0.329         9.87        0.157\n",
            "     20    20        0.122        0.113      0.00961        0.255        0.308         5.67         0.09\n",
            "     20    30        0.115        0.114     0.000853        0.248         0.31         1.69       0.0268\n",
            "     20    40        0.279        0.264       0.0156        0.374        0.472         7.33        0.115\n",
            "     20    50      0.00205     1.08e-10      0.00205      6.4e-06     9.52e-06         2.66       0.0415\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     20     2        0.149        0.137       0.0124        0.258         0.34         5.16       0.0806\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              20   41.526    0.002       0.0922      0.00893        0.101          0.2        0.277         4.11       0.0673\n",
            "! Validation         20   41.526    0.002        0.109      0.00963        0.118        0.221          0.3         4.68       0.0703\n",
            "Wall time: 41.52653832099986\n",
            "! Best model       20    0.118\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21    10        0.109        0.102      0.00737        0.227        0.293         5.04       0.0788\n",
            "     21    20        0.132         0.13      0.00216        0.268        0.331         2.69       0.0426\n",
            "     21    30        0.137        0.137     0.000468        0.278        0.339         1.27       0.0199\n",
            "     21    40        0.045       0.0443     0.000695        0.155        0.193         1.53       0.0242\n",
            "     21    50        0.219        0.181       0.0383        0.308         0.39         11.3         0.18\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     21     2        0.145        0.133       0.0127        0.255        0.334         5.27       0.0824\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              21   42.822    0.002       0.0876       0.0156        0.103        0.196         0.27         5.58       0.0891\n",
            "! Validation         21   42.822    0.002        0.106      0.00992        0.115        0.219        0.296         4.78        0.072\n",
            "Wall time: 42.82338489699987\n",
            "! Best model       21    0.115\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22    10        0.106        0.102      0.00397        0.231        0.293         3.64       0.0578\n",
            "     22    20        0.059       0.0587     0.000314        0.177        0.222         1.04       0.0163\n",
            "     22    30        0.115        0.115     0.000522        0.252        0.311         1.32        0.021\n",
            "     22    40       0.0806       0.0803     0.000366         0.21         0.26         1.12       0.0176\n",
            "     22    50       0.0353       0.0351     0.000274        0.136        0.172        0.957       0.0152\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     22     2        0.142        0.129       0.0127        0.251         0.33          5.3       0.0828\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              22   44.124    0.002       0.0842       0.0137       0.0979        0.191        0.264         5.15       0.0826\n",
            "! Validation         22   44.124    0.002        0.103      0.00995        0.113        0.216        0.293         4.82       0.0725\n",
            "Wall time: 44.12532459799991\n",
            "! Best model       22    0.113\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23    10        0.158        0.139       0.0188        0.271        0.342         7.92        0.126\n",
            "     23    20       0.0646       0.0623      0.00223        0.186        0.229         2.77       0.0433\n",
            "     23    30        0.148         0.14      0.00749        0.277        0.344         5.01       0.0795\n",
            "     23    40       0.0515       0.0491      0.00247        0.165        0.203         2.92       0.0456\n",
            "     23    50         0.19        0.176       0.0143        0.241        0.385         1.75         0.11\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     23     2        0.139        0.128       0.0114        0.251        0.328         4.98       0.0778\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              23   45.678    0.002        0.086      0.00673       0.0927        0.192        0.267          3.4        0.055\n",
            "! Validation         23   45.678    0.002        0.102      0.00902        0.111        0.215        0.291         4.58       0.0685\n",
            "Wall time: 45.679457735999904\n",
            "! Best model       23    0.111\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24    10        0.114        0.104      0.00984        0.237        0.296         5.83        0.091\n",
            "     24    20       0.0384     1.89e-10       0.0384     8.52e-06     1.26e-05         11.5         0.18\n",
            "     24    30       0.0403       0.0371      0.00323        0.146        0.177         3.34       0.0522\n",
            "     24    40       0.0302        0.029      0.00116        0.125        0.156         1.97       0.0313\n",
            "     24    50       0.0609       0.0609     5.13e-05        0.177        0.227        0.421      0.00658\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     24     2        0.135        0.124       0.0113        0.247        0.323         4.96       0.0776\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              24   47.377    0.002       0.0811      0.00873       0.0898        0.186        0.259         3.89       0.0627\n",
            "! Validation         24   47.377    0.002       0.0988      0.00893        0.108        0.212        0.287         4.59       0.0686\n",
            "Wall time: 47.377829868999925\n",
            "! Best model       24    0.108\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25    10         0.11        0.106       0.0032        0.242          0.3         3.32       0.0519\n",
            "     25    20       0.0331         0.03       0.0031        0.125        0.159         3.22       0.0511\n",
            "     25    30        0.218        0.212      0.00634        0.337        0.423          4.6       0.0731\n",
            "     25    40       0.0467       0.0424      0.00431        0.157        0.189         3.86       0.0603\n",
            "     25    50       0.0482       0.0382         0.01        0.141        0.179         5.88       0.0919\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     25     2        0.133        0.123       0.0101        0.246        0.321         4.62       0.0722\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              25   48.790    0.002       0.0891      0.00961       0.0987        0.195        0.273         4.62       0.0743\n",
            "! Validation         25   48.790    0.002       0.0977      0.00808        0.106         0.21        0.285         4.32       0.0642\n",
            "Wall time: 48.792053085999896\n",
            "! Best model       25    0.106\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26    10        0.125       0.0835       0.0418         0.21        0.265         11.8        0.188\n",
            "     26    20        0.132        0.129      0.00288        0.262         0.33          3.1       0.0493\n",
            "     26    30        0.162        0.131       0.0311        0.269        0.332         10.2        0.162\n",
            "     26    40        0.067        0.065      0.00205        0.188        0.234         2.66       0.0416\n",
            "     26    50       0.0297        0.027      0.00277        0.121        0.151         3.04       0.0483\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     26     2        0.131         0.12       0.0102        0.244        0.319         4.64       0.0725\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              26   50.089    0.002       0.0815       0.0108       0.0924        0.186         0.26         4.97       0.0804\n",
            "! Validation         26   50.089    0.002        0.096      0.00816        0.104        0.209        0.283         4.36       0.0649\n",
            "Wall time: 50.08997559599993\n",
            "! Best model       26    0.104\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27    10        0.204        0.127       0.0767        0.257        0.328         16.3        0.254\n",
            "     27    20       0.0284       0.0274      0.00102        0.111        0.152         1.85       0.0294\n",
            "     27    30      0.00099     1.16e-10      0.00099     6.57e-06     9.88e-06         1.85       0.0289\n",
            "     27    40        0.164        0.146       0.0175        0.283        0.351         7.65        0.121\n",
            "     27    50       0.0457       0.0427      0.00296        0.152         0.19         3.15       0.0499\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     27     2        0.128        0.118         0.01        0.242        0.315         4.72       0.0737\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              27   51.372    0.002       0.0802       0.0127       0.0929        0.184        0.258         5.11       0.0811\n",
            "! Validation         27   51.372    0.002       0.0942      0.00807        0.102        0.207         0.28         4.43        0.066\n",
            "Wall time: 51.37284833700005\n",
            "! Best model       27    0.102\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28    10        0.118        0.116      0.00282        0.236        0.312         3.07       0.0487\n",
            "     28    20       0.0269       0.0263     0.000568         0.11        0.149         1.38       0.0219\n",
            "     28    30        0.179        0.151       0.0284        0.282        0.357         9.91        0.155\n",
            "     28    40        0.226        0.151       0.0746        0.286        0.357           16        0.251\n",
            "     28    50        0.115        0.111      0.00426        0.247        0.306         3.77       0.0599\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     28     2        0.126        0.117      0.00972        0.241        0.314         4.64       0.0725\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              28   52.654    0.002       0.0805      0.00829       0.0888        0.183        0.257         3.72       0.0602\n",
            "! Validation         28   52.654    0.002       0.0931      0.00786        0.101        0.206        0.278         4.38       0.0652\n",
            "Wall time: 52.654826904999936\n",
            "! Best model       28    0.101\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29    10       0.0329       0.0281      0.00486        0.122        0.154         4.03        0.064\n",
            "     29    20          0.1       0.0968       0.0036        0.228        0.286         3.52       0.0551\n",
            "     29    30        0.115        0.108      0.00704        0.235        0.301         4.85        0.077\n",
            "     29    40       0.0668       0.0511       0.0157         0.17        0.208         7.35        0.115\n",
            "     29    50       0.0356       0.0343      0.00132        0.131         0.17          2.1       0.0333\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     29     2        0.124        0.115      0.00933         0.24        0.311          4.6       0.0719\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              29   53.933    0.002       0.0766       0.0126       0.0892         0.18        0.251          5.5       0.0872\n",
            "! Validation         29   53.933    0.002       0.0917      0.00763       0.0994        0.204        0.276         4.38       0.0653\n",
            "Wall time: 53.93389500399985\n",
            "! Best model       29    0.099\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30    10        0.129        0.111       0.0176        0.247        0.306          7.8        0.122\n",
            "     30    20       0.0399       0.0393     0.000622        0.149        0.182         1.46       0.0229\n",
            "     30    30        0.151        0.121       0.0297        0.259        0.319         9.97        0.158\n",
            "     30    40        0.168        0.109       0.0595        0.242        0.303         14.1        0.224\n",
            "     30    50        0.103        0.101      0.00211        0.237        0.292          2.7       0.0421\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     30     2        0.122        0.112      0.00967        0.237        0.308         4.82       0.0754\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              30   55.238    0.002       0.0801       0.0147       0.0949        0.185        0.259         5.04       0.0829\n",
            "! Validation         30   55.238    0.002       0.0898      0.00792       0.0978        0.203        0.273         4.56       0.0684\n",
            "Wall time: 55.239278071999934\n",
            "! Best model       30    0.098\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31    10       0.0924       0.0564       0.0359        0.171        0.218         11.1        0.174\n",
            "     31    20     2.74e-05     1.09e-10     2.74e-05     6.37e-06     9.56e-06        0.307       0.0048\n",
            "     31    30       0.0362        0.033      0.00317        0.131        0.167         3.26       0.0517\n",
            "     31    40        0.013     1.32e-10        0.013      7.4e-06     1.06e-05         6.69        0.105\n",
            "     31    50        0.033       0.0293       0.0037         0.12        0.157         3.52       0.0558\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     31     2        0.121         0.11       0.0109        0.235        0.305         5.09       0.0795\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              31   56.511    0.002       0.0759        0.012       0.0879        0.178        0.251         4.88       0.0793\n",
            "! Validation         31   56.511    0.002       0.0886      0.00888       0.0975        0.201        0.272         4.81       0.0724\n",
            "Wall time: 56.511434448000045\n",
            "! Best model       31    0.097\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32    10        0.234        0.233     0.000569        0.256        0.443          1.4       0.0219\n",
            "     32    20       0.0951       0.0902      0.00494        0.224        0.276         4.07       0.0645\n",
            "     32    30       0.0378       0.0373     0.000526        0.136        0.177         1.33        0.021\n",
            "     32    40        0.101       0.0888       0.0125         0.21        0.273         6.45        0.102\n",
            "     32    50       0.0344       0.0299      0.00444        0.122        0.159         3.85       0.0612\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     32     2        0.121         0.11       0.0102        0.235        0.305          4.9       0.0767\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              32   57.810    0.002       0.0742       0.0103       0.0844        0.176        0.247          4.7        0.074\n",
            "! Validation         32   57.810    0.002       0.0882      0.00834       0.0966          0.2        0.271         4.69       0.0704\n",
            "Wall time: 57.81072647499991\n",
            "! Best model       32    0.097\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33    10       0.0394       0.0337       0.0057         0.13        0.169         4.44       0.0693\n",
            "     33    20       0.0884       0.0809      0.00743         0.21        0.261         4.99       0.0791\n",
            "     33    30        0.165        0.117       0.0479        0.255        0.314         12.9        0.201\n",
            "     33    40        0.137        0.137       0.0002         0.27        0.339        0.818        0.013\n",
            "     33    50       0.0284       0.0268      0.00156        0.115         0.15         2.28       0.0362\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     33     2        0.119         0.11      0.00898        0.235        0.305         4.61       0.0721\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              33   59.494    0.002       0.0721      0.00929       0.0814        0.174        0.243         4.18        0.068\n",
            "! Validation         33   59.494    0.002       0.0879      0.00748       0.0953          0.2         0.27         4.47       0.0669\n",
            "Wall time: 59.49543003300005\n",
            "! Best model       33    0.095\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34    10       0.0682       0.0631      0.00517        0.181        0.231         4.22        0.066\n",
            "     34    20        0.112         0.11       0.0026        0.244        0.304         2.99       0.0468\n",
            "     34    30        0.029       0.0285     0.000424        0.121        0.155         1.19       0.0189\n",
            "     34    40       0.0989       0.0904      0.00846        0.224        0.276          5.4       0.0844\n",
            "     34    50        0.122        0.122      0.00048        0.262         0.32         1.29       0.0201\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     34     2        0.118        0.109      0.00911        0.234        0.303         4.59       0.0718\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              34   61.178    0.002       0.0758      0.00646       0.0823        0.178         0.25         3.57       0.0579\n",
            "! Validation         34   61.178    0.002       0.0871      0.00761       0.0947        0.199        0.269         4.48       0.0669\n",
            "Wall time: 61.179046869999866\n",
            "! Best model       34    0.095\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35    10        0.106        0.105     0.000682        0.231        0.298         1.53        0.024\n",
            "     35    20       0.0686       0.0674      0.00121         0.18        0.238         2.05        0.032\n",
            "     35    30        0.219        0.198       0.0212        0.334        0.408         8.56        0.134\n",
            "     35    40       0.0451       0.0449     0.000252        0.156        0.194        0.917       0.0146\n",
            "     35    50       0.0457        0.035       0.0107        0.129        0.172         5.98       0.0949\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     35     2        0.116        0.108        0.008        0.233        0.302         4.39       0.0686\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              35   62.484    0.002       0.0788       0.0142        0.093        0.181        0.255         4.53        0.074\n",
            "! Validation         35   62.484    0.002        0.087      0.00677       0.0938        0.198        0.269         4.29        0.064\n",
            "Wall time: 62.48506359699991\n",
            "! Best model       35    0.094\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36    10       0.0953       0.0928      0.00249        0.223         0.28         2.93       0.0458\n",
            "     36    20       0.0359     1.22e-10       0.0359     6.98e-06     1.01e-05         11.1        0.174\n",
            "     36    30        0.135        0.134      0.00176        0.271        0.336         2.42       0.0385\n",
            "     36    40        0.109       0.0975       0.0112        0.219        0.287         6.12       0.0971\n",
            "     36    50        0.105        0.104     0.000145         0.23        0.297        0.696        0.011\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     36     2        0.117        0.109      0.00809        0.234        0.303         4.33       0.0677\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              36   63.775    0.002       0.0745       0.0119       0.0864        0.177        0.247         4.98       0.0788\n",
            "! Validation         36   63.775    0.002       0.0873      0.00686       0.0942        0.199         0.27         4.26       0.0634\n",
            "Wall time: 63.77697210999986\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37    10        0.219        0.213      0.00613        0.324        0.423          4.6       0.0719\n",
            "     37    20        0.114       0.0897       0.0242         0.22        0.275         9.01        0.143\n",
            "     37    30     0.000914     1.49e-10     0.000914     7.51e-06     1.12e-05         1.78       0.0278\n",
            "     37    40        0.193        0.193      0.00032        0.319        0.403         1.03       0.0164\n",
            "     37    50       0.0246       0.0239     0.000634        0.112        0.142         1.46       0.0231\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     37     2        0.116         0.11      0.00649        0.234        0.304         3.94       0.0616\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              37   65.091    0.002       0.0765        0.007       0.0835        0.177        0.251         3.73       0.0597\n",
            "! Validation         37   65.091    0.002        0.088       0.0057       0.0937        0.199        0.271         3.92       0.0581\n",
            "Wall time: 65.09182466799984\n",
            "! Best model       37    0.094\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38    10        0.156        0.145       0.0119        0.202        0.349          6.4          0.1\n",
            "     38    20       0.0687       0.0687     1.72e-05        0.192        0.241        0.244      0.00381\n",
            "     38    30       0.0419       0.0369      0.00505        0.144        0.176         4.17       0.0652\n",
            "     38    40       0.0424       0.0341      0.00826        0.119         0.17         5.25       0.0834\n",
            "     38    50       0.0688        0.052       0.0168        0.166        0.209         7.63        0.119\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     38     2        0.116        0.109      0.00692        0.233        0.303         4.04       0.0631\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              38   66.398    0.002       0.0696      0.00791       0.0775        0.172        0.239         4.13       0.0664\n",
            "! Validation         38   66.398    0.002       0.0874        0.006       0.0933        0.198         0.27         4.01       0.0594\n",
            "Wall time: 66.39882499600003\n",
            "! Best model       38    0.093\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39    10        0.079       0.0789     0.000117        0.204        0.258        0.627      0.00995\n",
            "     39    20        0.164        0.146       0.0177        0.278        0.351         7.82        0.122\n",
            "     39    30       0.0915       0.0905     0.000993        0.222        0.276         1.85       0.0289\n",
            "     39    40       0.0561     1.35e-10       0.0561     7.58e-06     1.07e-05         13.9        0.217\n",
            "     39    50       0.0319       0.0308      0.00105        0.119        0.161         1.88       0.0298\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     39     2        0.115        0.107      0.00759        0.231        0.301         4.22       0.0659\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              39   67.707    0.002        0.068      0.00937       0.0773        0.168        0.236         4.37       0.0714\n",
            "! Validation         39   67.707    0.002       0.0862      0.00649       0.0927        0.197        0.268         4.17       0.0618\n",
            "Wall time: 67.70781662099989\n",
            "! Best model       39    0.093\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40    10       0.0191       0.0114      0.00772       0.0784       0.0979         5.16       0.0806\n",
            "     40    20       0.0343       0.0313      0.00292        0.121        0.163         3.13       0.0496\n",
            "     40    30       0.0875       0.0846       0.0029         0.21        0.267         3.16       0.0494\n",
            "     40    40       0.0322       0.0317     0.000451        0.121        0.163         1.23       0.0195\n",
            "     40    50       0.0543       0.0535     0.000806        0.169        0.212         1.67       0.0261\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     40     2        0.114        0.107      0.00698        0.231        0.301         4.02       0.0628\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              40   69.007    0.002       0.0684        0.012       0.0804        0.169        0.237         4.46       0.0726\n",
            "! Validation         40   69.007    0.002       0.0862      0.00606       0.0922        0.197        0.268         4.03       0.0596\n",
            "Wall time: 69.00853494399985\n",
            "! Best model       40    0.092\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41    10       0.0295       0.0281      0.00139        0.126        0.154         2.19       0.0343\n",
            "     41    20       0.0453       0.0452     9.58e-05        0.156        0.195        0.575      0.00899\n",
            "     41    30       0.0225       0.0225     8.54e-05        0.107        0.138        0.534      0.00848\n",
            "     41    40       0.0363       0.0362     0.000155        0.138        0.175         0.72       0.0114\n",
            "     41    50       0.0936       0.0928     0.000807        0.223         0.28         1.64       0.0261\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     41     2        0.114        0.107       0.0069         0.23          0.3         3.96       0.0619\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              41   70.298    0.002       0.0693      0.00677       0.0761        0.168        0.238         3.62       0.0586\n",
            "! Validation         41   70.298    0.002       0.0858      0.00601       0.0918        0.196        0.267         4.01       0.0591\n",
            "Wall time: 70.29934823100007\n",
            "! Best model       41    0.092\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42    10        0.178         0.14       0.0377        0.258        0.344         11.4        0.178\n",
            "     42    20      0.00166     1.38e-10      0.00166      7.4e-06     1.08e-05          2.4       0.0374\n",
            "     42    30        0.038       0.0358      0.00229        0.137        0.174         2.77        0.044\n",
            "     42    40        0.039        0.036      0.00301        0.139        0.174         3.17       0.0504\n",
            "     42    50        0.145        0.101       0.0435        0.234        0.292         12.1        0.191\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     42     2        0.112        0.105      0.00763        0.228        0.297         4.24       0.0662\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              42   71.817    0.002       0.0683       0.0075       0.0757        0.168        0.236         3.37       0.0562\n",
            "! Validation         42   71.817    0.002       0.0844      0.00658        0.091        0.195        0.265         4.24       0.0629\n",
            "Wall time: 71.81838914399987\n",
            "! Best model       42    0.091\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43    10       0.0354       0.0351     0.000341        0.128        0.172         1.07        0.017\n",
            "     43    20         0.07       0.0592       0.0108        0.178        0.223          6.1       0.0953\n",
            "     43    30        0.127        0.124      0.00364        0.258        0.323         3.49       0.0554\n",
            "     43    40       0.0763       0.0761     0.000186          0.2        0.253        0.801       0.0125\n",
            "     43    50       0.0937       0.0918      0.00191        0.216        0.278         2.53       0.0402\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     43     2        0.112        0.104      0.00719        0.227        0.297         4.11       0.0642\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              43   73.488    0.002        0.069      0.00559       0.0746        0.168        0.238          3.2       0.0521\n",
            "! Validation         43   73.488    0.002       0.0843      0.00626       0.0906        0.194        0.265         4.14       0.0614\n",
            "Wall time: 73.48928897199994\n",
            "! Best model       43    0.091\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44    10        0.102       0.0955      0.00668        0.217        0.284         4.73        0.075\n",
            "     44    20       0.0968       0.0874      0.00935        0.217        0.271         5.59       0.0888\n",
            "     44    30        0.105       0.0942       0.0103        0.222        0.282         5.88       0.0934\n",
            "     44    40       0.0281       0.0231      0.00499        0.108         0.14         4.08       0.0648\n",
            "     44    50        0.192        0.145       0.0474        0.288         0.35         12.8          0.2\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     44     2        0.111        0.103      0.00818        0.226        0.294         4.35        0.068\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              44   75.030    0.002       0.0661      0.00962       0.0757        0.167        0.233         4.22       0.0689\n",
            "! Validation         44   75.030    0.002       0.0832      0.00704       0.0902        0.193        0.263         4.36       0.0647\n",
            "Wall time: 75.03083183600006\n",
            "! Best model       44    0.090\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45    10       0.0896       0.0833      0.00634        0.211        0.265         4.68       0.0731\n",
            "     45    20        0.123        0.104       0.0189        0.231        0.296         7.95        0.126\n",
            "     45    30      0.00176     1.27e-10      0.00176     6.98e-06     1.03e-05         2.47       0.0385\n",
            "     45    40       0.0628       0.0521       0.0107        0.167         0.21         6.09       0.0951\n",
            "     45    50        0.162        0.162     0.000243        0.304         0.37        0.916       0.0143\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     45     2         0.11        0.103       0.0075        0.226        0.294         4.17       0.0651\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              45   76.332    0.002       0.0694       0.0136       0.0829         0.17        0.237         5.55       0.0884\n",
            "! Validation         45   76.332    0.002       0.0832      0.00654       0.0897        0.193        0.263         4.22       0.0625\n",
            "Wall time: 76.33268553900007\n",
            "! Best model       45    0.090\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46    10       0.0881       0.0881     8.93e-06        0.214        0.272        0.176      0.00274\n",
            "     46    20       0.0224       0.0224     2.31e-05        0.104        0.137        0.278      0.00441\n",
            "     46    30       0.0862       0.0841      0.00214        0.216        0.266         2.68       0.0425\n",
            "     46    40        0.128        0.124      0.00353        0.262        0.323         3.43       0.0545\n",
            "     46    50       0.0337       0.0304      0.00328        0.115         0.16         3.31       0.0526\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     46     2         0.11        0.102      0.00794        0.225        0.293         4.32       0.0676\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              46   77.674    0.002       0.0663       0.0072       0.0735        0.166        0.233         3.44       0.0563\n",
            "! Validation         46   77.674    0.002       0.0827      0.00686       0.0896        0.192        0.263         4.35       0.0646\n",
            "Wall time: 77.67638463100002\n",
            "! Best model       46    0.090\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47    10       0.0456       0.0289       0.0166        0.126        0.156         7.57        0.118\n",
            "     47    20       0.0939       0.0876      0.00626        0.224        0.272         4.58       0.0726\n",
            "     47    30       0.0318       0.0308      0.00103        0.112        0.161         1.86       0.0295\n",
            "     47    40       0.0946       0.0936     0.000937        0.228        0.281          1.8       0.0281\n",
            "     47    50       0.0315       0.0315     2.03e-05        0.117        0.163         0.26      0.00413\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     47     2         0.11        0.102      0.00735        0.225        0.294         4.19       0.0654\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              47   78.975    0.002       0.0654      0.00968        0.075        0.165        0.232         4.01       0.0661\n",
            "! Validation         47   78.975    0.002       0.0828      0.00643       0.0892        0.192        0.263         4.24        0.063\n",
            "Wall time: 78.97664336899993\n",
            "! Best model       47    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48    10        0.108        0.092       0.0163         0.22        0.278         7.39        0.117\n",
            "     48    20       0.0945       0.0945     3.91e-05        0.231        0.282        0.362      0.00574\n",
            "     48    30       0.0191       0.0134      0.00571       0.0853        0.106         4.44       0.0694\n",
            "     48    40       0.0853       0.0572       0.0281        0.173         0.22         9.86        0.154\n",
            "     48    50       0.0462       0.0462     1.57e-05        0.155        0.197        0.233      0.00364\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     48     2        0.109        0.102      0.00762        0.224        0.293          4.2       0.0656\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              48   80.259    0.002       0.0672       0.0109       0.0781        0.166        0.235         4.51       0.0725\n",
            "! Validation         48   80.259    0.002       0.0825      0.00665       0.0892        0.192        0.262         4.28       0.0634\n",
            "Wall time: 80.26027455899998\n",
            "! Best model       48    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49    10       0.0598       0.0559      0.00389        0.171        0.217         3.66       0.0573\n",
            "     49    20        0.215        0.203       0.0121        0.249        0.413         1.61        0.101\n",
            "     49    30        0.029       0.0281     0.000858        0.124        0.154         1.72       0.0269\n",
            "     49    40       0.0353       0.0351     0.000217        0.119        0.172        0.852       0.0135\n",
            "     49    50       0.0338       0.0298      0.00393        0.117        0.159         3.63       0.0576\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     49     2        0.109        0.102      0.00746        0.224        0.293         4.12       0.0644\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              49   81.535    0.002       0.0663      0.00726       0.0735        0.164        0.233         3.93       0.0633\n",
            "! Validation         49   81.535    0.002       0.0823      0.00657       0.0889        0.191        0.262         4.24       0.0627\n",
            "Wall time: 81.535761976\n",
            "! Best model       49    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50    10       0.0429       0.0293       0.0136        0.109        0.157         6.75        0.107\n",
            "     50    20        0.185        0.173       0.0118        0.289        0.382         6.39       0.0999\n",
            "     50    30       0.0971       0.0902      0.00684        0.217        0.276         4.86       0.0759\n",
            "     50    40       0.0901       0.0895     0.000565        0.215        0.275         1.37       0.0218\n",
            "     50    50       0.0777       0.0704      0.00725        0.195        0.244            5       0.0781\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     50     2        0.108          0.1      0.00795        0.222        0.291          4.3       0.0672\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              50   82.825    0.002       0.0695      0.00907       0.0786         0.17        0.239         4.06       0.0659\n",
            "! Validation         50   82.825    0.002       0.0817      0.00695       0.0887        0.191        0.261         4.39       0.0651\n",
            "Wall time: 82.82574077999993\n",
            "! Best model       50    0.089\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51    10       0.0305       0.0299     0.000573        0.118        0.159         1.38        0.022\n",
            "     51    20      0.00242      1.5e-10      0.00242     7.82e-06     1.12e-05         2.89       0.0452\n",
            "     51    30        0.197        0.169       0.0275        0.218        0.377         9.74        0.152\n",
            "     51    40        0.136         0.12       0.0165        0.265        0.318         7.54        0.118\n",
            "     51    50        0.126        0.101       0.0258        0.231        0.291         9.43        0.147\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     51     2        0.108        0.101      0.00714        0.223        0.291         4.08       0.0638\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              51   84.149    0.002       0.0655      0.00829       0.0738        0.166        0.232         4.05        0.066\n",
            "! Validation         51   84.149    0.002       0.0819      0.00631       0.0883        0.191        0.261          4.2       0.0621\n",
            "Wall time: 84.14979256699985\n",
            "! Best model       51    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52    10       0.0825       0.0824     8.67e-05        0.207        0.264        0.539      0.00855\n",
            "     52    20       0.0279       0.0274     0.000515        0.112        0.152         1.31       0.0208\n",
            "     52    30        0.083       0.0795      0.00352        0.209        0.259         3.43       0.0544\n",
            "     52    40        0.118        0.103       0.0156        0.241        0.294         7.34        0.115\n",
            "     52    50       0.0985        0.097      0.00142        0.229        0.286         2.18       0.0345\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     52     2        0.108        0.101      0.00676        0.223        0.292         3.98       0.0621\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              52   85.853    0.002       0.0674       0.0175       0.0849        0.166        0.235         5.99       0.0968\n",
            "! Validation         52   85.853    0.002        0.082      0.00602        0.088        0.191        0.261         4.12       0.0608\n",
            "Wall time: 85.85605567699986\n",
            "! Best model       52    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53    10       0.0923       0.0879      0.00436        0.215        0.272         3.88       0.0606\n",
            "     53    20       0.0554       0.0527      0.00268        0.166        0.211         3.04       0.0475\n",
            "     53    30       0.0748       0.0735      0.00138        0.198        0.249         2.18       0.0341\n",
            "     53    40       0.0396       0.0334      0.00622        0.134        0.168         4.63       0.0724\n",
            "     53    50        0.106       0.0968      0.00947        0.227        0.286         5.72       0.0893\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     53     2        0.107        0.101      0.00675        0.223        0.291         4.01       0.0626\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              53   87.573    0.002       0.0661      0.00589        0.072        0.166        0.233         3.41       0.0553\n",
            "! Validation         53   87.573    0.002       0.0817      0.00601       0.0877        0.191        0.261         4.13       0.0612\n",
            "Wall time: 87.57379500699994\n",
            "! Best model       53    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54    10       0.0679       0.0678     0.000114        0.191        0.239        0.628      0.00982\n",
            "     54    20       0.0939        0.091       0.0029        0.221        0.277         3.16       0.0494\n",
            "     54    30       0.0791       0.0767      0.00238        0.199        0.254         2.87       0.0448\n",
            "     54    40       0.0265       0.0114       0.0151       0.0778       0.0982         7.21        0.113\n",
            "     54    50       0.0698       0.0674       0.0024        0.189        0.238         2.88        0.045\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     54     2        0.107       0.0997      0.00728        0.222         0.29          4.1       0.0641\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              54   88.887    0.002       0.0674      0.00815       0.0756        0.166        0.234          3.9       0.0639\n",
            "! Validation         54   88.887    0.002       0.0812      0.00645       0.0877         0.19         0.26         4.24       0.0627\n",
            "Wall time: 88.88793866399988\n",
            "! Best model       54    0.088\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55    10       0.0564       0.0487      0.00775         0.16        0.202         5.17       0.0808\n",
            "     55    20       0.0374       0.0374     5.97e-05         0.14        0.177        0.447       0.0071\n",
            "     55    30       0.0402       0.0358      0.00442        0.141        0.174         3.91        0.061\n",
            "     55    40        0.237        0.227       0.0108        0.252        0.437          6.1       0.0953\n",
            "     55    50      0.00454     1.41e-10      0.00454     7.34e-06     1.09e-05         3.96       0.0618\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     55     2        0.107       0.0998      0.00686        0.222         0.29         3.98       0.0622\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              55   90.177    0.002       0.0658      0.00793       0.0737        0.165        0.233         3.75       0.0622\n",
            "! Validation         55   90.177    0.002       0.0814      0.00612       0.0875         0.19         0.26         4.14       0.0611\n",
            "Wall time: 90.17894650199992\n",
            "! Best model       55    0.087\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56    10        0.177        0.147       0.0304        0.279        0.352         10.2         0.16\n",
            "     56    20       0.0373       0.0326       0.0047        0.121        0.166         3.96       0.0629\n",
            "     56    30        0.102          0.1      0.00164        0.228        0.291         2.38       0.0371\n",
            "     56    40       0.0841       0.0835     0.000603        0.211        0.265         1.44       0.0225\n",
            "     56    50       0.0791       0.0698      0.00929        0.193        0.242         5.66       0.0885\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     56     2        0.106       0.0992      0.00683        0.221        0.289         4.04       0.0632\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              56   91.458    0.002       0.0649      0.00758       0.0725        0.164         0.23         3.87       0.0623\n",
            "! Validation         56   91.458    0.002       0.0809      0.00611        0.087        0.189         0.26         4.18        0.062\n",
            "Wall time: 91.460713081\n",
            "! Best model       56    0.087\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57    10       0.0383       0.0365      0.00176         0.12        0.175         2.43       0.0385\n",
            "     57    20        0.046        0.043      0.00294        0.124         0.19         3.14       0.0498\n",
            "     57    30        0.114        0.101        0.013        0.239        0.292          6.7        0.105\n",
            "     57    40         0.07       0.0685      0.00146        0.185         0.24         2.24       0.0351\n",
            "     57    50        0.192        0.186      0.00559        0.324        0.396         4.39       0.0686\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     57     2        0.107        0.102      0.00548        0.224        0.293         3.61       0.0564\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              57   92.756    0.002       0.0714      0.00529       0.0767        0.171        0.243         3.38       0.0549\n",
            "! Validation         57   92.756    0.002        0.083      0.00504        0.088        0.191        0.263         3.81        0.056\n",
            "Wall time: 92.75671532299998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58    10        0.112       0.0905       0.0214        0.213        0.276         8.46        0.134\n",
            "     58    20       0.0844       0.0423       0.0421        0.148        0.189         11.9        0.188\n",
            "     58    30        0.205        0.203      0.00132        0.247        0.414        0.534       0.0334\n",
            "     58    40       0.0164     9.88e-11       0.0164      6.1e-06     9.12e-06         7.52        0.118\n",
            "     58    50       0.0271       0.0216      0.00548        0.104        0.135         4.28        0.068\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     58     2        0.106       0.0996      0.00689        0.221         0.29         4.19       0.0655\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              58   94.037    0.002       0.0702       0.0181       0.0883        0.173         0.24         6.56        0.104\n",
            "! Validation         58   94.037    0.002       0.0811      0.00612       0.0873         0.19         0.26         4.24       0.0633\n",
            "Wall time: 94.03808873799994\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59    10         0.13        0.124      0.00572        0.255        0.323         4.37       0.0694\n",
            "     59    20        0.137        0.121       0.0164        0.256        0.319         7.41        0.118\n",
            "     59    30       0.0603        0.056      0.00429        0.173        0.217         3.85       0.0601\n",
            "     59    40       0.0756       0.0739      0.00166        0.204         0.25         2.35       0.0374\n",
            "     59    50         0.12       0.0968       0.0234         0.22        0.286         8.85         0.14\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     59     2        0.107       0.0999      0.00719        0.222         0.29         4.26       0.0665\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              59   95.384    0.002       0.0677      0.00978       0.0775        0.168        0.236         4.88       0.0782\n",
            "! Validation         59   95.384    0.002       0.0811      0.00631       0.0874         0.19         0.26          4.3       0.0642\n",
            "Wall time: 95.38544911300005\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60    10        0.128        0.121      0.00739        0.252        0.319         4.97       0.0789\n",
            "     60    20       0.0392       0.0356      0.00366         0.13        0.173          3.5       0.0556\n",
            "     60    30       0.0436       0.0393      0.00429        0.137        0.182         3.85       0.0601\n",
            "     60    40        0.146        0.141        0.005        0.282        0.344         4.15       0.0649\n",
            "     60    50       0.0398       0.0397      4.9e-05        0.149        0.183        0.411      0.00643\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     60     2        0.108       0.0999      0.00762        0.222         0.29         4.39       0.0686\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              60   96.692    0.002       0.0683      0.00614       0.0744        0.167        0.237         3.28       0.0543\n",
            "! Validation         60   96.692    0.002        0.081      0.00664       0.0876         0.19         0.26          4.4       0.0659\n",
            "Wall time: 96.69283250800004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61    10        0.124        0.122      0.00119        0.258        0.321         1.99       0.0316\n",
            "     61    20        0.121       0.0868       0.0341        0.216         0.27         10.9         0.17\n",
            "     61    30       0.0713        0.069      0.00221        0.194        0.241         2.72       0.0431\n",
            "     61    40       0.0404       0.0364      0.00405        0.139        0.175         3.74       0.0584\n",
            "     61    50       0.0281        0.028     0.000119        0.113        0.153         0.63         0.01\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     61     2        0.107       0.0999      0.00718        0.222         0.29         4.21       0.0658\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              61   98.162    0.002       0.0643      0.00856       0.0729        0.163        0.229          4.1       0.0671\n",
            "! Validation         61   98.162    0.002       0.0809      0.00632       0.0872         0.19         0.26         4.28       0.0637\n",
            "Wall time: 98.16321777300004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62    10       0.0637       0.0636     0.000127        0.182        0.231        0.661       0.0103\n",
            "     62    20       0.0289     1.32e-10       0.0289     7.33e-06     1.05e-05         9.99        0.156\n",
            "     62    30       0.0413       0.0383      0.00292        0.142         0.18         3.13       0.0496\n",
            "     62    40      0.00331     1.47e-10      0.00331     7.67e-06     1.11e-05         3.38       0.0528\n",
            "     62    50       0.0877       0.0874     0.000247        0.214        0.271        0.923       0.0144\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     62     2        0.106       0.0997      0.00626        0.222         0.29         3.99       0.0624\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              62   99.920    0.002       0.0665      0.00984       0.0763        0.167        0.233         4.43       0.0715\n",
            "! Validation         62   99.920    0.002       0.0806       0.0056       0.0862        0.189        0.259         4.09       0.0608\n",
            "Wall time: 99.92121728699999\n",
            "! Best model       62    0.086\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63    10       0.0784       0.0774     0.000994        0.197        0.255         1.82       0.0289\n",
            "     63    20       0.0128      0.00989      0.00295       0.0718       0.0913         3.19       0.0499\n",
            "     63    30       0.0375       0.0362      0.00139        0.122        0.175         2.15       0.0342\n",
            "     63    40       0.0269       0.0264     0.000513        0.112        0.149         1.31       0.0208\n",
            "     63    50       0.0389       0.0385     0.000334        0.147         0.18         1.07       0.0168\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     63     2        0.106       0.0981      0.00763         0.22        0.287         4.29        0.067\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              63  101.444    0.002       0.0649       0.0122       0.0771        0.164         0.23         4.77       0.0781\n",
            "! Validation         63  101.444    0.002       0.0796      0.00667       0.0862        0.188        0.257         4.36       0.0648\n",
            "Wall time: 101.4446137519999\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64    10        0.151         0.14       0.0109        0.272        0.344         6.03       0.0957\n",
            "     64    20       0.0285        0.027      0.00143        0.113        0.151         2.19       0.0347\n",
            "     64    30       0.0708       0.0698      0.00104        0.192        0.243         1.87       0.0296\n",
            "     64    40       0.0229       0.0229     3.55e-06        0.108        0.139        0.109      0.00173\n",
            "     64    50         0.11       0.0919       0.0177         0.22        0.278         7.69        0.122\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     64     2        0.106       0.0991      0.00671        0.221        0.289         3.97       0.0621\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              64  102.753    0.002       0.0677      0.00731        0.075        0.167        0.236         3.52       0.0581\n",
            "! Validation         64  102.753    0.002       0.0803      0.00594       0.0862        0.189        0.259         4.09       0.0604\n",
            "Wall time: 102.75356544199985\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65    10       0.0299       0.0295     0.000354        0.121        0.158         1.11       0.0173\n",
            "     65    20       0.0873       0.0845      0.00275         0.21        0.267         3.04       0.0482\n",
            "     65    30        0.114        0.105      0.00937        0.244        0.297         5.69       0.0888\n",
            "     65    40       0.0302       0.0302     1.64e-05        0.128        0.159        0.238      0.00372\n",
            "     65    50        0.066       0.0651     0.000817        0.188        0.234         1.68       0.0262\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     65     2        0.105       0.0988      0.00647         0.22        0.288         3.92       0.0613\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              65  104.036    0.002       0.0625      0.00689       0.0694        0.162        0.226         3.51       0.0578\n",
            "! Validation         65  104.036    0.002       0.0802      0.00575       0.0859        0.189        0.258         4.04       0.0596\n",
            "Wall time: 104.03736071799995\n",
            "! Best model       65    0.086\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66    10        0.014       0.0121      0.00186       0.0806        0.101         2.53       0.0396\n",
            "     66    20       0.0884       0.0609       0.0275        0.181        0.226         9.75        0.152\n",
            "     66    30       0.0348       0.0344     0.000372        0.138         0.17         1.13       0.0177\n",
            "     66    40       0.0799       0.0776      0.00225          0.2        0.256         2.74       0.0435\n",
            "     66    50       0.0593       0.0591     0.000203        0.173        0.223        0.836       0.0131\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     66     2        0.105       0.0976      0.00696        0.219        0.287         4.04       0.0631\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              66  105.345    0.002       0.0616      0.00881       0.0704        0.161        0.225         4.24       0.0683\n",
            "! Validation         66  105.345    0.002       0.0795      0.00615       0.0856        0.188        0.257         4.15       0.0614\n",
            "Wall time: 105.34687313199993\n",
            "! Best model       66    0.086\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67    10        0.197        0.167       0.0302        0.221        0.375         2.55         0.16\n",
            "     67    20       0.0904       0.0857      0.00475        0.215        0.269         3.98       0.0632\n",
            "     67    30       0.0362       0.0357      0.00044        0.131        0.174         1.21       0.0192\n",
            "     67    40       0.0327       0.0326     0.000138        0.129        0.166        0.679       0.0108\n",
            "     67    50        0.201        0.195       0.0059        0.331        0.406         4.51       0.0705\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     67     2        0.104       0.0981      0.00615         0.22        0.288          3.8       0.0593\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              67  106.656    0.002       0.0654      0.00616       0.0716        0.165        0.232         3.36       0.0552\n",
            "! Validation         67  106.656    0.002         0.08      0.00554       0.0856        0.188        0.258         3.96       0.0583\n",
            "Wall time: 106.65727105099995\n",
            "! Best model       67    0.086\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68    10        0.113       0.0792       0.0343         0.21        0.258         10.9         0.17\n",
            "     68    20       0.0307       0.0307      2.9e-05        0.125        0.161        0.312      0.00494\n",
            "     68    30       0.0111      0.00949      0.00165       0.0696       0.0894         2.39       0.0373\n",
            "     68    40        0.157        0.132       0.0253        0.193        0.333         9.35        0.146\n",
            "     68    50       0.0913       0.0909     0.000383        0.224        0.277         1.15        0.018\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     68     2        0.104       0.0973      0.00625        0.219        0.286         3.89       0.0609\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              68  107.942    0.002        0.063       0.0083       0.0713        0.162        0.226         3.79       0.0611\n",
            "! Validation         68  107.942    0.002       0.0794       0.0056        0.085        0.188        0.257         4.03       0.0595\n",
            "Wall time: 107.942468082\n",
            "! Best model       68    0.085\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69    10       0.0874       0.0838      0.00362        0.215        0.266         3.48       0.0552\n",
            "     69    20       0.0103     1.29e-10       0.0103      7.2e-06     1.04e-05         5.96       0.0931\n",
            "     69    30       0.0385       0.0342       0.0043        0.135         0.17         3.85       0.0602\n",
            "     69    40       0.0975       0.0907      0.00682        0.216        0.276         4.85       0.0758\n",
            "     69    50        0.204        0.189       0.0142        0.239        0.399         1.75        0.109\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     69     2        0.103       0.0973      0.00593        0.218        0.286         3.78       0.0591\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              69  109.239    0.002       0.0654      0.00552        0.071        0.164        0.231         3.29       0.0533\n",
            "! Validation         69  109.239    0.002       0.0794      0.00536       0.0848        0.187        0.257         3.93        0.058\n",
            "Wall time: 109.24044583499995\n",
            "! Best model       69    0.085\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70    10       0.0462       0.0442      0.00196        0.153        0.193          2.6       0.0406\n",
            "     70    20       0.0524       0.0511      0.00137        0.164        0.207         2.18        0.034\n",
            "     70    30       0.0937       0.0888      0.00492        0.209        0.273         4.06       0.0644\n",
            "     70    40        0.112       0.0824       0.0298        0.213        0.264         10.1        0.158\n",
            "     70    50       0.0407       0.0344      0.00632        0.129         0.17         4.67        0.073\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     70     2        0.103       0.0961      0.00654        0.217        0.285         3.97       0.0621\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              70  110.536    0.002       0.0644      0.00748       0.0719        0.163        0.229         3.69       0.0596\n",
            "! Validation         70  110.536    0.002       0.0787      0.00582       0.0845        0.187        0.256         4.09       0.0605\n",
            "Wall time: 110.5372583169999\n",
            "! Best model       70    0.084\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71    10       0.0989       0.0929      0.00596         0.22         0.28         4.46       0.0708\n",
            "     71    20       0.0666       0.0658     0.000716        0.186        0.236         1.55       0.0246\n",
            "     71    30       0.0317       0.0315     0.000179        0.121        0.163        0.775       0.0123\n",
            "     71    40         0.06       0.0589      0.00115        0.176        0.223         1.99       0.0311\n",
            "     71    50        0.125        0.114       0.0107        0.252         0.31         6.07       0.0948\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     71     2        0.102        0.096      0.00646        0.217        0.284         3.94       0.0615\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              71  112.187    0.002       0.0634      0.00933       0.0728        0.163        0.228         4.16       0.0676\n",
            "! Validation         71  112.187    0.002       0.0786      0.00578       0.0844        0.187        0.256         4.07       0.0601\n",
            "Wall time: 112.18900117699991\n",
            "! Best model       71    0.084\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72    10        0.108        0.103      0.00556        0.234        0.294         4.31       0.0684\n",
            "     72    20       0.0883       0.0838      0.00451        0.214        0.266         3.95       0.0617\n",
            "     72    30       0.0864       0.0839      0.00248        0.212        0.266         2.88       0.0457\n",
            "     72    40       0.0377       0.0377     2.36e-06        0.121        0.178       0.0889      0.00141\n",
            "     72    50        0.174        0.173      0.00109         0.22        0.382         1.94       0.0303\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     72     2        0.102       0.0961      0.00595        0.217        0.285          3.8       0.0594\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              72  113.903    0.002       0.0616      0.00638        0.068         0.16        0.224         3.48       0.0575\n",
            "! Validation         72  113.903    0.002       0.0786       0.0054        0.084        0.186        0.256         3.97       0.0585\n",
            "Wall time: 113.90407904499989\n",
            "! Best model       72    0.084\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73    10        0.017       0.0121      0.00493       0.0809        0.101         4.13       0.0645\n",
            "     73    20       0.0118       0.0107      0.00107       0.0747       0.0949         1.92         0.03\n",
            "     73    30       0.0465       0.0403       0.0062         0.14        0.184         4.62       0.0723\n",
            "     73    40       0.0747        0.074     0.000689        0.199         0.25         1.52       0.0241\n",
            "     73    50       0.0376       0.0346      0.00303        0.135        0.171         3.18       0.0505\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     73     2        0.102       0.0958      0.00619        0.216        0.284         3.84         0.06\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              73  115.263    0.002       0.0667      0.00615       0.0729        0.165        0.234         3.57       0.0578\n",
            "! Validation         73  115.263    0.002       0.0785      0.00557       0.0841        0.186        0.256            4        0.059\n",
            "Wall time: 115.26400874799992\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74    10       0.0971     1.56e-10       0.0971     7.91e-06     1.15e-05         18.3        0.286\n",
            "     74    20       0.0976       0.0903      0.00735        0.218        0.276         4.96       0.0787\n",
            "     74    30       0.0158     1.42e-10       0.0158     7.68e-06     1.09e-05          7.4        0.116\n",
            "     74    40       0.0494       0.0479      0.00146        0.159        0.201         2.25       0.0351\n",
            "     74    50       0.0533       0.0483      0.00504         0.16        0.202         4.17       0.0651\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     74     2        0.102       0.0957       0.0061        0.217        0.284         3.83       0.0598\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              74  116.556    0.002       0.0612      0.00728       0.0685         0.16        0.223         3.53       0.0574\n",
            "! Validation         74  116.556    0.002       0.0784      0.00549       0.0839        0.186        0.256         3.98       0.0587\n",
            "Wall time: 116.55694446200005\n",
            "! Best model       74    0.084\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75    10        0.151        0.133       0.0183        0.274        0.334         7.94        0.124\n",
            "     75    20       0.0324       0.0323     2.11e-05        0.134        0.165         0.27      0.00422\n",
            "     75    30       0.0721       0.0705      0.00162        0.192        0.244         2.36       0.0369\n",
            "     75    40       0.0387       0.0282       0.0105        0.125        0.154         6.01       0.0939\n",
            "     75    50       0.0924       0.0747       0.0176        0.199        0.251         7.68        0.122\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     75     2        0.102       0.0946       0.0071        0.215        0.282         4.04       0.0632\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              75  117.858    0.002       0.0624      0.00908       0.0714        0.161        0.226         3.96       0.0641\n",
            "! Validation         75  117.858    0.002       0.0776      0.00629       0.0839        0.185        0.254         4.19       0.0618\n",
            "Wall time: 117.85892722599988\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76    10        0.137        0.133      0.00436        0.267        0.334         3.82       0.0606\n",
            "     76    20       0.0669       0.0641      0.00278        0.186        0.232          3.1       0.0484\n",
            "     76    30         0.22         0.18       0.0395        0.228         0.39         2.92        0.182\n",
            "     76    40       0.0785       0.0743      0.00415        0.201         0.25         3.73       0.0592\n",
            "     76    50        0.041       0.0356      0.00538        0.137        0.173         4.24       0.0673\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     76     2        0.102       0.0957      0.00597        0.216        0.284         3.79       0.0593\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              76  119.200    0.002       0.0686      0.00863       0.0772        0.168        0.237          4.4       0.0718\n",
            "! Validation         76  119.200    0.002       0.0784      0.00538       0.0837        0.186        0.256         3.94       0.0581\n",
            "Wall time: 119.20065857500003\n",
            "! Best model       76    0.084\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77    10     0.000485     1.41e-10     0.000485     7.55e-06     1.09e-05         1.29       0.0202\n",
            "     77    20       0.0366       0.0329      0.00376        0.126        0.166         3.55       0.0563\n",
            "     77    30        0.165        0.119       0.0465        0.257        0.316         12.5        0.198\n",
            "     77    40       0.0349        0.032      0.00287         0.12        0.164          3.1       0.0492\n",
            "     77    50        0.036       0.0344      0.00164        0.133         0.17         2.34       0.0372\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     77     2        0.102       0.0955      0.00662        0.216        0.284         3.95       0.0617\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              77  120.508    0.002       0.0627       0.0131       0.0759        0.162        0.227         5.15       0.0835\n",
            "! Validation         77  120.508    0.002        0.078      0.00588       0.0839        0.186        0.255         4.09       0.0603\n",
            "Wall time: 120.50839233299985\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78    10        0.118        0.112      0.00618         0.25        0.307         4.62       0.0722\n",
            "     78    20       0.0402       0.0361      0.00403        0.138        0.175         3.67       0.0583\n",
            "     78    30        0.135        0.121       0.0134        0.261         0.32          6.7        0.106\n",
            "     78    40       0.0557       0.0474      0.00831        0.157          0.2         5.36       0.0837\n",
            "     78    50       0.0144     1.84e-10       0.0144     8.53e-06     1.25e-05         7.05         0.11\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     78     2        0.102       0.0959      0.00645        0.217        0.284         3.94       0.0616\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              78  121.824    0.002       0.0634       0.0059       0.0693        0.163        0.228         3.66       0.0588\n",
            "! Validation         78  121.824    0.002       0.0781      0.00575       0.0838        0.186        0.255         4.07       0.0601\n",
            "Wall time: 121.82490766499996\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79    10        0.102       0.0995      0.00209         0.23         0.29         2.69        0.042\n",
            "     79    20       0.0272       0.0272      8.4e-05        0.112        0.151         0.53      0.00841\n",
            "     79    30       0.0325       0.0323     0.000259        0.128        0.165        0.932       0.0148\n",
            "     79    40       0.0269       0.0267     0.000198        0.121         0.15        0.828       0.0129\n",
            "     79    50       0.0861        0.085      0.00111         0.21        0.268         1.92       0.0305\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     79     2        0.102       0.0962       0.0062        0.217        0.285         3.79       0.0592\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              79  123.116    0.002       0.0631      0.00579       0.0689        0.161        0.227         3.56       0.0578\n",
            "! Validation         79  123.116    0.002       0.0782      0.00558       0.0838        0.186        0.255         3.96       0.0581\n",
            "Wall time: 123.11715516599997\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80    10        0.048       0.0463      0.00163        0.159        0.198         2.37        0.037\n",
            "     80    20       0.0965       0.0863       0.0103         0.22         0.27         5.86       0.0931\n",
            "     80    30       0.0768       0.0765      0.00032        0.202        0.254         1.04       0.0164\n",
            "     80    40       0.0823       0.0803      0.00204        0.207         0.26         2.61       0.0415\n",
            "     80    50       0.0997       0.0928      0.00693        0.219         0.28         4.81       0.0764\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     80     2        0.102       0.0954      0.00624        0.216        0.283         3.86       0.0603\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              80  124.564    0.002       0.0623      0.00751       0.0698        0.161        0.226         3.64       0.0592\n",
            "! Validation         80  124.564    0.002       0.0778      0.00559       0.0833        0.185        0.255            4        0.059\n",
            "Wall time: 124.56545340000002\n",
            "! Best model       80    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81    10       0.0784       0.0746      0.00384        0.201        0.251         3.58       0.0569\n",
            "     81    20       0.0826       0.0811      0.00145        0.207        0.261         2.24        0.035\n",
            "     81    30       0.0124       0.0115     0.000853       0.0777       0.0984         1.72       0.0268\n",
            "     81    40       0.0543        0.048      0.00629         0.16        0.201         4.66       0.0728\n",
            "     81    50       0.0385       0.0361      0.00242        0.121        0.174         2.85       0.0452\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     81     2        0.101       0.0954      0.00594        0.216        0.284         3.74       0.0584\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              81  126.278    0.002       0.0633      0.00752       0.0708        0.162        0.228         3.86       0.0633\n",
            "! Validation         81  126.278    0.002       0.0778      0.00536       0.0832        0.185        0.255          3.9       0.0574\n",
            "Wall time: 126.27930181199986\n",
            "! Best model       81    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82    10       0.0343     1.84e-10       0.0343     8.52e-06     1.25e-05         10.9         0.17\n",
            "     82    20       0.0245       0.0225      0.00197        0.105        0.138         2.57       0.0408\n",
            "     82    30       0.0351       0.0314      0.00369        0.113        0.163         3.51       0.0557\n",
            "     82    40       0.0447       0.0365      0.00821        0.133        0.175         5.24       0.0832\n",
            "     82    50       0.0498       0.0479      0.00196         0.16        0.201          2.6       0.0406\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     82     2        0.101       0.0945      0.00619        0.215        0.282         3.86       0.0604\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              82  127.874    0.002       0.0614      0.00862         0.07         0.16        0.224         4.03       0.0652\n",
            "! Validation         82  127.874    0.002       0.0773      0.00555       0.0829        0.185        0.254            4        0.059\n",
            "Wall time: 127.87513801799992\n",
            "! Best model       82    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83    10       0.0898       0.0692       0.0206        0.189        0.241         8.44        0.132\n",
            "     83    20        0.143        0.123       0.0205        0.262        0.322         8.27        0.131\n",
            "     83    30       0.0358       0.0346      0.00121        0.134        0.171         2.01        0.032\n",
            "     83    40       0.0857       0.0771      0.00859        0.204        0.255         5.36       0.0851\n",
            "     83    50        0.087       0.0866     0.000438        0.215         0.27         1.23       0.0192\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     83     2          0.1       0.0943      0.00612        0.215        0.282         3.81       0.0596\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              83  129.161    0.002       0.0633      0.00808       0.0714        0.162        0.228         3.82        0.063\n",
            "! Validation         83  129.161    0.002       0.0772       0.0055       0.0826        0.185        0.254         3.97       0.0584\n",
            "Wall time: 129.161637078\n",
            "! Best model       83    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84    10       0.0824       0.0821     0.000227        0.204        0.263        0.871       0.0138\n",
            "     84    20       0.0398       0.0322      0.00754        0.127        0.165         5.02       0.0797\n",
            "     84    30       0.0444       0.0399      0.00447        0.145        0.183         3.93       0.0614\n",
            "     84    40       0.0686       0.0686     4.04e-05        0.195         0.24        0.367      0.00583\n",
            "     84    50        0.122       0.0968       0.0253        0.227        0.286         9.34        0.146\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     84     2          0.1       0.0937      0.00647        0.214        0.281          3.9       0.0609\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              84  130.461    0.002       0.0623      0.00931       0.0717        0.161        0.226         4.36       0.0709\n",
            "! Validation         84  130.461    0.002       0.0768      0.00578       0.0826        0.184        0.253         4.05       0.0598\n",
            "Wall time: 130.46241548199987\n",
            "! Best model       84    0.083\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85    10       0.0286     1.23e-10       0.0286     7.11e-06     1.02e-05         9.93        0.155\n",
            "     85    20       0.0653       0.0653     5.58e-06        0.187        0.235        0.139      0.00217\n",
            "     85    30       0.0573     1.88e-10       0.0573     8.61e-06     1.26e-05         14.1         0.22\n",
            "     85    40       0.0168     1.02e-10       0.0168     6.42e-06     9.27e-06         7.62        0.119\n",
            "     85    50       0.0282       0.0282     4.44e-05         0.11        0.154        0.385      0.00612\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     85     2          0.1       0.0939      0.00653        0.214        0.281         3.86       0.0604\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              85  131.813    0.002       0.0635      0.00625       0.0698        0.164        0.228         2.95       0.0491\n",
            "! Validation         85  131.813    0.002       0.0769      0.00584       0.0827        0.184        0.253         4.03       0.0592\n",
            "Wall time: 131.81351102400004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86    10       0.0434       0.0341       0.0093        0.133         0.17         5.58       0.0885\n",
            "     86    20     4.95e-06     1.49e-10     4.95e-06     7.62e-06     1.12e-05        0.131      0.00204\n",
            "     86    30        0.024       0.0235     0.000521        0.105        0.141         1.32        0.021\n",
            "     86    40        0.079        0.072      0.00702        0.193        0.246         4.92       0.0769\n",
            "     86    50      0.00332     1.29e-10      0.00332     7.27e-06     1.04e-05         3.38       0.0529\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     86     2          0.1       0.0936      0.00642        0.214        0.281         3.87       0.0605\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              86  133.113    0.002       0.0617      0.00751       0.0692        0.159        0.225         3.98       0.0641\n",
            "! Validation         86  133.113    0.002       0.0766      0.00575       0.0824        0.184        0.253         4.04       0.0595\n",
            "Wall time: 133.11438340099994\n",
            "! Best model       86    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87    10       0.0959       0.0914      0.00449        0.221        0.277         3.88       0.0615\n",
            "     87    20        0.117        0.113      0.00409         0.25        0.308          3.7       0.0587\n",
            "     87    30       0.0394       0.0308      0.00859        0.116        0.161         5.36       0.0851\n",
            "     87    40       0.0882       0.0875      0.00071         0.22        0.272         1.57       0.0245\n",
            "     87    50       0.0895       0.0893     0.000136        0.211        0.274        0.674       0.0107\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     87     2       0.0994       0.0934      0.00604        0.214        0.281         3.82       0.0597\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              87  134.432    0.002       0.0629      0.00697       0.0698        0.162        0.228         4.02       0.0652\n",
            "! Validation         87  134.432    0.002       0.0765      0.00545       0.0819        0.184        0.253         3.98       0.0587\n",
            "Wall time: 134.434229039\n",
            "! Best model       87    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88    10       0.0445       0.0443     0.000212        0.147        0.193        0.855       0.0134\n",
            "     88    20       0.0429       0.0416      0.00136        0.149        0.187         2.17       0.0339\n",
            "     88    30       0.0944       0.0941     0.000238        0.229        0.282        0.892       0.0142\n",
            "     88    40       0.0704       0.0671      0.00333        0.193        0.238         3.34        0.053\n",
            "     88    50        0.032       0.0277      0.00431        0.112        0.153          3.8       0.0603\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     88     2       0.0999       0.0937      0.00622        0.214        0.281          3.8       0.0594\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              88  135.752    0.002       0.0649      0.00517       0.0701        0.165        0.231          3.2       0.0525\n",
            "! Validation         88  135.752    0.002       0.0767       0.0056       0.0823        0.184        0.253         3.98       0.0585\n",
            "Wall time: 135.75314526700004\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89    10       0.0447       0.0366      0.00812         0.14        0.176         5.29       0.0827\n",
            "     89    20       0.0951       0.0927      0.00241        0.222        0.279         2.88        0.045\n",
            "     89    30         0.09       0.0829      0.00708        0.211        0.264         4.87       0.0773\n",
            "     89    40       0.0278       0.0255      0.00236         0.12        0.147         2.86       0.0446\n",
            "     89    50        0.175        0.149       0.0258        0.274        0.355         9.43        0.147\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     89     2       0.0994       0.0938      0.00562        0.214        0.281         3.67       0.0574\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              89  137.032    0.002       0.0619      0.00916       0.0711        0.161        0.226         4.85       0.0787\n",
            "! Validation         89  137.032    0.002       0.0767      0.00512       0.0818        0.184        0.253         3.85       0.0566\n",
            "Wall time: 137.03282852699977\n",
            "! Best model       89    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90    10       0.0485       0.0466      0.00193        0.156        0.198         2.58       0.0404\n",
            "     90    20         0.12         0.12     2.31e-05        0.254        0.318        0.278      0.00441\n",
            "     90    30       0.0445       0.0441     0.000352        0.154        0.193          1.1       0.0172\n",
            "     90    40        0.135        0.124       0.0118        0.251        0.323         6.39       0.0999\n",
            "     90    50        0.131       0.0826       0.0483        0.208        0.264         12.7        0.202\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     90     2       0.0994       0.0929      0.00647        0.213         0.28         3.89       0.0607\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              90  138.628    0.002       0.0584      0.00849       0.0668        0.157         0.22         3.52        0.059\n",
            "! Validation         90  138.628    0.002        0.076      0.00578       0.0818        0.183        0.252         4.04       0.0594\n",
            "Wall time: 138.62960218499984\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91    10       0.0427       0.0359      0.00676        0.132        0.174         4.75       0.0755\n",
            "     91    20        0.086       0.0827      0.00329        0.208        0.264         3.32       0.0527\n",
            "     91    30       0.0878       0.0871     0.000731        0.211        0.271         1.56       0.0248\n",
            "     91    40        0.118        0.116      0.00174        0.253        0.313         2.41       0.0383\n",
            "     91    50      0.00708     1.22e-10      0.00708     6.99e-06     1.01e-05         4.94       0.0772\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     91     2       0.0992       0.0934      0.00584        0.214         0.28         3.74       0.0585\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              91  140.306    0.002       0.0636      0.00747       0.0711        0.163        0.229         3.91       0.0633\n",
            "! Validation         91  140.306    0.002       0.0764      0.00528       0.0817        0.184        0.252         3.88       0.0571\n",
            "Wall time: 140.30732964399976\n",
            "! Best model       91    0.082\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92    10       0.0458       0.0406      0.00517        0.149        0.185         4.22        0.066\n",
            "     92    20       0.0894       0.0873      0.00206        0.213        0.271         2.62       0.0416\n",
            "     92    30        0.122        0.116      0.00605        0.255        0.312         4.57       0.0714\n",
            "     92    40       0.0302       0.0268      0.00336        0.108         0.15         3.35       0.0532\n",
            "     92    50       0.0389       0.0341      0.00482        0.137        0.169         4.08       0.0637\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     92     2       0.0994       0.0929      0.00647        0.213         0.28          3.9       0.0609\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              92  141.727    0.002        0.062      0.00536       0.0673        0.161        0.226         3.19       0.0516\n",
            "! Validation         92  141.727    0.002        0.076      0.00573       0.0818        0.183        0.252         4.01        0.059\n",
            "Wall time: 141.7285469909998\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93    10       0.0763       0.0763     1.19e-05          0.2        0.254        0.203      0.00317\n",
            "     93    20       0.0863        0.086     0.000324        0.218        0.269         1.06       0.0165\n",
            "     93    30        0.146        0.143      0.00202        0.273        0.348         2.64       0.0413\n",
            "     93    40       0.0362       0.0344      0.00183        0.129         0.17         2.52       0.0393\n",
            "     93    50       0.0333       0.0311       0.0022        0.131        0.162         2.75        0.043\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     93     2        0.099       0.0928      0.00618        0.213         0.28         3.85       0.0602\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              93  143.071    0.002        0.061      0.00552       0.0665        0.159        0.223          3.3       0.0538\n",
            "! Validation         93  143.071    0.002       0.0759       0.0055       0.0814        0.183        0.252         3.97       0.0584\n",
            "Wall time: 143.07154018499978\n",
            "! Best model       93    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94    10       0.0262       0.0254     0.000859         0.11        0.146         1.69       0.0269\n",
            "     94    20       0.0669       0.0667     0.000238        0.189        0.237        0.892       0.0142\n",
            "     94    30      0.00501     1.26e-10      0.00501     7.21e-06     1.03e-05         4.16       0.0649\n",
            "     94    40       0.0637       0.0623      0.00137        0.184        0.229         2.18        0.034\n",
            "     94    50        0.135        0.118       0.0171        0.256        0.315         7.68         0.12\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     94     2       0.0993       0.0929       0.0064        0.213         0.28         3.84         0.06\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              94  144.402    0.002        0.061      0.00457       0.0656        0.159        0.224         2.91       0.0479\n",
            "! Validation         94  144.402    0.002       0.0759      0.00569       0.0816        0.183        0.252         3.97       0.0584\n",
            "Wall time: 144.40345155900013\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95    10        0.206        0.185       0.0217         0.23        0.394         2.16        0.135\n",
            "     95    20      0.00141     1.25e-10      0.00141      6.9e-06     1.02e-05          2.2       0.0344\n",
            "     95    30       0.0384       0.0357      0.00267         0.13        0.173         2.99       0.0474\n",
            "     95    40       0.0731       0.0726     0.000553        0.194        0.247         1.38       0.0216\n",
            "     95    50       0.0354       0.0352     0.000127        0.136        0.172        0.652       0.0104\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     95     2       0.0991       0.0926      0.00648        0.213        0.279         3.86       0.0603\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              95  145.717    0.002       0.0614       0.0067       0.0681         0.16        0.224         3.56        0.058\n",
            "! Validation         95  145.717    0.002       0.0758      0.00574       0.0815        0.183        0.251         3.99       0.0587\n",
            "Wall time: 145.71863529099983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96    10       0.0602       0.0601     4.36e-05        0.181        0.225        0.388      0.00606\n",
            "     96    20       0.0367       0.0364     0.000272        0.137        0.175        0.954       0.0151\n",
            "     96    30       0.0341       0.0331      0.00103        0.135        0.167         1.89       0.0295\n",
            "     96    40        0.183        0.163       0.0195        0.218        0.371         2.05        0.128\n",
            "     96    50       0.0297       0.0293     0.000427        0.126        0.157         1.21        0.019\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     96     2       0.0983       0.0919      0.00646        0.212        0.278         3.94       0.0616\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              96  147.019    0.002       0.0614      0.00585       0.0672        0.161        0.225          3.3       0.0537\n",
            "! Validation         96  147.019    0.002       0.0753      0.00573        0.081        0.182        0.251         4.04       0.0597\n",
            "Wall time: 147.0202805250001\n",
            "! Best model       96    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97    10        0.202        0.186       0.0153        0.234        0.396         1.82        0.114\n",
            "     97    20       0.0764       0.0705      0.00592        0.197        0.244         4.45       0.0706\n",
            "     97    30       0.0159      0.00929      0.00657       0.0693       0.0885         4.76       0.0744\n",
            "     97    40       0.0275       0.0242       0.0033        0.109        0.143         3.32       0.0527\n",
            "     97    50       0.0345       0.0342     0.000229        0.134         0.17        0.874       0.0139\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     97     2       0.0982       0.0919      0.00625        0.212        0.278         3.84       0.0601\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              97  148.316    0.002       0.0623      0.00649       0.0688         0.16        0.226         3.88       0.0627\n",
            "! Validation         97  148.316    0.002       0.0752      0.00558       0.0808        0.182         0.25         3.98       0.0585\n",
            "Wall time: 148.31652541799986\n",
            "! Best model       97    0.081\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98    10        0.127        0.127     5.02e-05        0.264        0.327        0.416       0.0065\n",
            "     98    20       0.0316       0.0296        0.002        0.109        0.158         2.58        0.041\n",
            "     98    30        0.128        0.114       0.0135        0.251         0.31         6.73        0.107\n",
            "     98    40        0.029        0.028      0.00108         0.12        0.153         1.93       0.0302\n",
            "     98    50       0.0196       0.0105      0.00911       0.0746        0.094         5.61       0.0876\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     98     2       0.0985       0.0919      0.00658        0.212        0.278         3.98       0.0623\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              98  149.629    0.002       0.0653      0.00758       0.0729        0.165        0.232         3.74       0.0603\n",
            "! Validation         98  149.629    0.002       0.0752      0.00579        0.081        0.182         0.25         4.07       0.0601\n",
            "Wall time: 149.63012660100003\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99    10       0.0608       0.0607     0.000119        0.181        0.226         0.64         0.01\n",
            "     99    20       0.0236       0.0204      0.00316          0.1        0.131         3.25       0.0516\n",
            "     99    30       0.0824       0.0823     0.000186        0.209        0.263        0.801       0.0125\n",
            "     99    40         0.04        0.035      0.00501        0.119        0.172         4.09        0.065\n",
            "     99    50       0.0354       0.0354      2.6e-05        0.137        0.173        0.295      0.00468\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "     99     2       0.0984       0.0908      0.00756        0.211        0.277         4.27       0.0667\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train              99  151.025    0.002       0.0628      0.00843       0.0712        0.162        0.227         4.08        0.066\n",
            "! Validation         99  151.025    0.002       0.0743      0.00656       0.0808        0.181        0.249         4.32       0.0641\n",
            "Wall time: 151.02645670899983\n",
            "\n",
            "training\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100    10       0.0129      1.4e-10       0.0129     7.56e-06     1.09e-05         6.68        0.104\n",
            "    100    20        0.123        0.104       0.0188        0.249        0.297         8.05        0.126\n",
            "    100    30       0.0339       0.0326      0.00126        0.134        0.166         2.05       0.0326\n",
            "    100    40       0.0451       0.0424      0.00276        0.152        0.189         3.08       0.0482\n",
            "    100    50       0.0816       0.0812     0.000457        0.204        0.262         1.24       0.0196\n",
            "\n",
            "validation\n",
            "# Epoch batch         loss       loss_f       loss_e        f_mae       f_rmse        e_mae      e/N_mae\n",
            "    100     2       0.0987       0.0914      0.00736        0.212        0.277          4.2       0.0657\n",
            "\n",
            "\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train             100  152.742    0.002       0.0612      0.00631       0.0675         0.16        0.224         3.62       0.0584\n",
            "! Validation        100  152.742    0.002       0.0746      0.00639        0.081        0.182        0.249         4.25        0.063\n",
            "Wall time: 152.744180228\n",
            "! Stop training: max epochs\n",
            "Wall time: 152.75952793199986\n",
            "Cumulative wall time: 152.75952793199986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae █▄▃▂▁▂▂▂▂▁▂▁▂▁▂▂▁▂▁▁▂▁▁▂▂▁▁▁▁▁▂▁▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae █▄▄▂▁▂▂▂▂▁▂▁▂▁▂▂▁▂▁▁▂▁▁▂▂▁▁▁▁▁▂▁▂▂▂▂▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae █▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse █▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss █▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e █▃▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f █▄▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae █▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse █▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f █▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall ▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 LR 0.002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    cumulative_wall 152.74225\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              epoch 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_e/N_mae 0.05844\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_e_mae 3.62174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     training_f_mae 0.16005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_f_rmse 0.22411\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_loss 0.06751\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_e 0.00631\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    training_loss_f 0.0612\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_e/N_mae 0.06302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_e_mae 4.24893\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   validation_f_mae 0.18189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_f_rmse 0.2493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_loss 0.08095\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_e 0.00639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  validation_loss_f 0.07456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               wall 152.74225\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/wandb/offline-run-20240104_184521-yb0moznz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/offline-run-20240104_184521-yb0moznz/logs\u001b[0m\n",
            "CPU times: user 5.46 s, sys: 762 ms, total: 6.22 s\n",
            "Wall time: 15min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls results/silicon-tutorial/si #/best_model.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVdaRnXwrKrg",
        "outputId": "d90b2077-3349-44f4-dbf0-29b53a050f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best_model.pth\tlast_model.pth\tmetrics_batch_train.csv  metrics_epoch.csv\t     trainer.pth\n",
            "config.yaml\tlog\t\tmetrics_batch_val.csv\t metrics_initialization.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q -U numpy"
      ],
      "metadata": {
        "id": "TRdVJF_Y8SBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nequip.utils import Config\n",
        "from nequip.model import model_from_config\n",
        "from nequip.data import AtomicData, ASEDataset\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "config = Config.from_file(\"results/silicon-tutorial/si/config.yaml\")\n",
        "\n",
        "#config[\"train_on_keys\"]=[\"forces\", \"total_energy\"]\n",
        "#config[\"model_builders\"] = [\"EnergyModel\", \"PerSpeciesRescale\", \"ForceOutput\", \"RescaleEnergyEtc\"]\n",
        "model = model_from_config(config, initialize=False)\n",
        "d = torch.load('results/silicon-tutorial/si/best_model.pth',map_location=device)\n",
        "model.load_state_dict(d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg9svjLM5jI2",
        "outputId": "8156affb-6ccd-428d-c32a-33d5e0ed5822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0fb_mry5jLl",
        "outputId": "4440bdfc-056e-45c0-bdfe-a05e5e030226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RescaleOutput(\n",
              "  (model): GradientOutput(\n",
              "    (func): SequentialGraphNetwork(\n",
              "      (one_hot): OneHotAtomEncoding()\n",
              "      (radial_basis): RadialBasisEdgeEncoding(\n",
              "        (basis): NormalizedBasis(\n",
              "          (basis): BesselBasis()\n",
              "        )\n",
              "        (cutoff): PolynomialCutoff()\n",
              "      )\n",
              "      (spharm): SphericalHarmonicEdgeAttrs(\n",
              "        (sh): SphericalHarmonics()\n",
              "      )\n",
              "      (allegro): Allegro_Module(\n",
              "        (latents): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (env_embed_mlps): ModuleList(\n",
              "          (0): ScalarMLPFunction(\n",
              "            (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "          )\n",
              "        )\n",
              "        (tps): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (linears): ModuleList(\n",
              "          (0): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "        (env_linears): ModuleList(\n",
              "          (0): Identity()\n",
              "        )\n",
              "        (_env_weighter): MakeWeightedChannels()\n",
              "        (final_latent): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng): ScalarMLP(\n",
              "        (_module): ScalarMLPFunction(\n",
              "          (_forward): RecursiveScriptModule(original_name=GraphModule)\n",
              "        )\n",
              "      )\n",
              "      (edge_eng_sum): EdgewiseEnergySum()\n",
              "      (per_species_rescale): PerSpeciesScaleShift()\n",
              "      (total_energy_sum): AtomwiseReduce()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test example material\n",
        "from jarvis.core.atoms import Atoms\n",
        "from jarvis.db.figshare import get_jid_data\n",
        "atoms = Atoms.from_dict(get_jid_data(jid='JVASP-1002',dataset='dft_3d')['atoms'])\n",
        "ase_atoms = atoms.ase_converter()\n",
        "a = ASEDataset.from_atoms_list([ase_atoms,ase_atoms],extra_fixed_fields={\"r_max\": 5.0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxCCG4xx5jOK",
        "outputId": "a92ab30c-6a72-41b1-d294-d1b10fa5d47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining 3D dataset 76k ...\n",
            "Reference:https://www.nature.com/articles/s41524-020-00440-1\n",
            "Other versions:https://doi.org/10.6084/m9.figshare.6815699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40.8M/40.8M [00:03<00:00, 10.7MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the zipfile...\n",
            "Loading completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing dataset...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nequip.data import AtomicData, Collater, dataset_from_config, register_fields, AtomicDataDict\n",
        "from nequip.data.transforms import TypeMapper\n",
        "# c = Collater.for_dataset(a, exclude_keys=[])\n",
        "a = AtomicData.from_ase(ase_atoms,5)\n",
        "data = AtomicData.to_AtomicDataDict(a)\n",
        "# tm = TypeMapper(chemical_symbol_to_type={\"Si\": 0})\n",
        "tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "data = tm(data)\n",
        "out = model(data)\n",
        "print(out)\n"
      ],
      "metadata": {
        "id": "0B5JNf8r7Ijz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e78381-0ecf-4fa6-a498-c81e6dad1eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'edge_index': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
            "         0, 1, 1, 1, 1, 0, 1, 1]]), 'pos': tensor([[3.9248, 2.7753, 6.7980],\n",
            "        [0.5607, 0.3965, 0.9711]]), 'cell': tensor([[ 3.3642e+00, -2.5027e-09,  1.9423e+00],\n",
            "        [ 1.1214e+00,  3.1718e+00,  1.9423e+00],\n",
            "        [-2.5910e-09, -1.8321e-09,  3.8846e+00]]), 'edge_cell_shift': tensor([[ 2.,  1.,  0.],\n",
            "        [ 0.,  2.,  0.],\n",
            "        [ 1.,  1.,  0.],\n",
            "        [ 0.,  1.,  0.],\n",
            "        [ 0.,  1.,  0.],\n",
            "        [-1.,  1.,  0.],\n",
            "        [ 2.,  0.,  0.],\n",
            "        [ 1.,  2.,  0.],\n",
            "        [ 0., -1.,  1.],\n",
            "        [ 1.,  0.,  0.],\n",
            "        [ 1.,  0.,  0.],\n",
            "        [-1.,  0.,  0.],\n",
            "        [ 1., -1.,  0.],\n",
            "        [ 0., -1.,  0.],\n",
            "        [ 1.,  0.,  2.],\n",
            "        [ 0.,  0.,  2.],\n",
            "        [ 0.,  1.,  2.],\n",
            "        [ 0.,  1.,  1.],\n",
            "        [ 2.,  0.,  1.],\n",
            "        [ 1.,  0.,  1.],\n",
            "        [ 0.,  0.,  1.],\n",
            "        [ 0.,  0.,  1.],\n",
            "        [-1.,  0.,  1.],\n",
            "        [ 1.,  1.,  1.],\n",
            "        [ 0.,  2.,  1.],\n",
            "        [ 0.,  1., -1.],\n",
            "        [ 0.,  0., -1.],\n",
            "        [ 1.,  0., -1.],\n",
            "        [ 0.,  0., -2.],\n",
            "        [ 0., -1., -2.],\n",
            "        [-1.,  0., -2.],\n",
            "        [-2., -1.,  0.],\n",
            "        [-1., -1.,  0.],\n",
            "        [ 0., -1.,  0.],\n",
            "        [ 0., -1.,  0.],\n",
            "        [ 1., -1.,  0.],\n",
            "        [ 0., -1., -1.],\n",
            "        [ 1.,  0.,  0.],\n",
            "        [-1.,  0.,  0.],\n",
            "        [-1., -1., -1.],\n",
            "        [-2.,  0.,  0.],\n",
            "        [-1.,  0.,  0.],\n",
            "        [-2.,  0., -1.],\n",
            "        [ 1.,  0., -1.],\n",
            "        [ 0.,  1., -1.],\n",
            "        [ 0., -2.,  0.],\n",
            "        [-1., -2.,  0.],\n",
            "        [-1.,  0., -1.],\n",
            "        [ 0.,  0., -1.],\n",
            "        [ 0.,  0., -1.],\n",
            "        [ 0., -1.,  1.],\n",
            "        [ 0.,  0.,  1.],\n",
            "        [-1.,  0.,  1.],\n",
            "        [ 0., -2., -1.],\n",
            "        [-1.,  1.,  0.],\n",
            "        [ 0.,  1.,  0.]]), 'pbc': tensor([True, True, True]), 'atom_types': tensor([[0],\n",
            "        [0]]), 'node_attrs': tensor([[1.],\n",
            "        [1.]]), 'node_features': tensor([[1.],\n",
            "        [1.]]), 'edge_vectors': tensor([[ 4.4855e+00,  7.9294e-01,  4.7684e-07],\n",
            "        [-1.1214e+00,  3.9647e+00, -1.9423e+00],\n",
            "        [ 1.1214e+00,  7.9294e-01, -1.9423e+00],\n",
            "        [-2.2428e+00,  7.9294e-01, -3.8846e+00],\n",
            "        [ 1.1214e+00,  3.1718e+00,  1.9423e+00],\n",
            "        [-2.2428e+00,  3.1718e+00,  0.0000e+00],\n",
            "        [ 3.3642e+00, -2.3788e+00, -1.9423e+00],\n",
            "        [ 2.2428e+00,  3.9647e+00,  4.7684e-07],\n",
            "        [-1.1214e+00, -3.1718e+00,  1.9423e+00],\n",
            "        [ 2.3842e-06, -2.3788e+00, -3.8846e+00],\n",
            "        [ 3.3642e+00, -2.5027e-09,  1.9423e+00],\n",
            "        [-3.3642e+00,  2.5027e-09, -1.9423e+00],\n",
            "        [ 2.2428e+00, -3.1718e+00,  0.0000e+00],\n",
            "        [-1.1214e+00, -3.1718e+00, -1.9423e+00],\n",
            "        [ 2.3842e-06, -2.3788e+00,  3.8846e+00],\n",
            "        [-3.3641e+00, -2.3788e+00,  1.9423e+00],\n",
            "        [-2.2428e+00,  7.9294e-01,  3.8846e+00],\n",
            "        [-2.2428e+00,  7.9294e-01,  1.9073e-06],\n",
            "        [ 3.3642e+00, -2.3788e+00,  1.9423e+00],\n",
            "        [ 2.3842e-06, -2.3788e+00,  1.9073e-06],\n",
            "        [-3.3641e+00, -2.3788e+00, -1.9423e+00],\n",
            "        [-2.5910e-09, -1.8321e-09,  3.8846e+00],\n",
            "        [-3.3642e+00,  6.7060e-10,  1.9423e+00],\n",
            "        [ 1.1214e+00,  7.9294e-01,  1.9423e+00],\n",
            "        [-1.1214e+00,  3.9647e+00,  1.9423e+00],\n",
            "        [ 1.1214e+00,  3.1718e+00, -1.9423e+00],\n",
            "        [ 2.5910e-09,  1.8321e-09, -3.8846e+00],\n",
            "        [ 3.3642e+00, -6.7060e-10, -1.9423e+00],\n",
            "        [ 3.3641e+00,  2.3788e+00, -1.9423e+00],\n",
            "        [ 2.2428e+00, -7.9294e-01, -3.8846e+00],\n",
            "        [-2.3842e-06,  2.3788e+00, -3.8846e+00],\n",
            "        [-4.4855e+00, -7.9294e-01, -4.7684e-07],\n",
            "        [-1.1214e+00, -7.9294e-01,  1.9423e+00],\n",
            "        [ 2.2428e+00, -7.9294e-01,  3.8846e+00],\n",
            "        [-1.1214e+00, -3.1718e+00, -1.9423e+00],\n",
            "        [ 2.2428e+00, -3.1718e+00,  0.0000e+00],\n",
            "        [ 2.2428e+00, -7.9294e-01, -1.9073e-06],\n",
            "        [ 3.3642e+00, -2.5027e-09,  1.9423e+00],\n",
            "        [-2.3842e-06,  2.3788e+00,  3.8846e+00],\n",
            "        [-1.1214e+00, -7.9294e-01, -1.9423e+00],\n",
            "        [-3.3642e+00,  2.3788e+00,  1.9423e+00],\n",
            "        [-3.3642e+00,  2.5027e-09, -1.9423e+00],\n",
            "        [-3.3642e+00,  2.3788e+00, -1.9423e+00],\n",
            "        [ 3.3642e+00, -6.7060e-10, -1.9423e+00],\n",
            "        [ 1.1214e+00,  3.1718e+00, -1.9423e+00],\n",
            "        [ 1.1214e+00, -3.9647e+00,  1.9423e+00],\n",
            "        [-2.2428e+00, -3.9647e+00, -4.7684e-07],\n",
            "        [-2.3842e-06,  2.3788e+00, -1.9073e-06],\n",
            "        [ 3.3641e+00,  2.3788e+00,  1.9423e+00],\n",
            "        [ 2.5910e-09,  1.8321e-09, -3.8846e+00],\n",
            "        [-1.1214e+00, -3.1718e+00,  1.9423e+00],\n",
            "        [-2.5910e-09, -1.8321e-09,  3.8846e+00],\n",
            "        [-3.3642e+00,  6.7060e-10,  1.9423e+00],\n",
            "        [ 1.1214e+00, -3.9647e+00, -1.9423e+00],\n",
            "        [-2.2428e+00,  3.1718e+00,  0.0000e+00],\n",
            "        [ 1.1214e+00,  3.1718e+00,  1.9423e+00]], grad_fn=<AddBackward0>), 'edge_lengths': tensor([4.5551, 4.5551, 2.3788, 4.5551, 3.8846, 3.8846, 4.5551, 4.5551, 3.8846,\n",
            "        4.5551, 3.8846, 3.8846, 3.8846, 3.8846, 4.5551, 4.5551, 4.5551, 2.3788,\n",
            "        4.5551, 2.3788, 4.5551, 3.8846, 3.8846, 2.3788, 4.5551, 3.8846, 3.8846,\n",
            "        3.8846, 4.5551, 4.5551, 4.5551, 4.5551, 2.3788, 4.5551, 3.8846, 3.8846,\n",
            "        2.3788, 3.8846, 4.5551, 2.3788, 4.5551, 3.8846, 4.5551, 3.8846, 3.8846,\n",
            "        4.5551, 4.5551, 2.3788, 4.5551, 3.8846, 3.8846, 3.8846, 3.8846, 4.5551,\n",
            "        3.8846, 3.8846], grad_fn=<LinalgVectorNormBackward0>), 'edge_embedding': tensor([[-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [ 0.2168, -0.3988, -0.9802, -0.4456,  0.0754, -0.0714, -0.5471, -0.4132],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.0446, -0.0195, -0.0082, -0.0167, -0.0037, -0.0139, -0.0028, -0.0107],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345],\n",
            "        [-0.2707, -0.2508, -0.0362, -0.1217, -0.1027, -0.0153, -0.1320, -0.0345]],\n",
            "       grad_fn=<MulBackward0>), 'edge_attrs': tensor([[ 1.0000e+00,  1.7056e+00,  3.0151e-01,  1.8132e-07],\n",
            "        [ 1.0000e+00, -4.2640e-01,  1.5076e+00, -7.3855e-01],\n",
            "        [ 1.0000e+00,  8.1650e-01,  5.7735e-01, -1.4142e+00],\n",
            "        [ 1.0000e+00, -8.5280e-01,  3.0151e-01, -1.4771e+00],\n",
            "        [ 1.0000e+00,  5.0000e-01,  1.4142e+00,  8.6603e-01],\n",
            "        [ 1.0000e+00, -1.0000e+00,  1.4142e+00,  0.0000e+00],\n",
            "        [ 1.0000e+00,  1.2792e+00, -9.0453e-01, -7.3855e-01],\n",
            "        [ 1.0000e+00,  8.5280e-01,  1.5076e+00,  1.8132e-07],\n",
            "        [ 1.0000e+00, -5.0000e-01, -1.4142e+00,  8.6603e-01],\n",
            "        [ 1.0000e+00,  9.0658e-07, -9.0453e-01, -1.4771e+00],\n",
            "        [ 1.0000e+00,  1.5000e+00, -1.1159e-09,  8.6603e-01],\n",
            "        [ 1.0000e+00, -1.5000e+00,  1.1159e-09, -8.6603e-01],\n",
            "        [ 1.0000e+00,  1.0000e+00, -1.4142e+00,  0.0000e+00],\n",
            "        [ 1.0000e+00, -5.0000e-01, -1.4142e+00, -8.6603e-01],\n",
            "        [ 1.0000e+00,  9.0658e-07, -9.0453e-01,  1.4771e+00],\n",
            "        [ 1.0000e+00, -1.2792e+00, -9.0453e-01,  7.3855e-01],\n",
            "        [ 1.0000e+00, -8.5280e-01,  3.0151e-01,  1.4771e+00],\n",
            "        [ 1.0000e+00, -1.6330e+00,  5.7735e-01,  1.3888e-06],\n",
            "        [ 1.0000e+00,  1.2792e+00, -9.0453e-01,  7.3855e-01],\n",
            "        [ 1.0000e+00,  1.7360e-06, -1.7321e+00,  1.3888e-06],\n",
            "        [ 1.0000e+00, -1.2792e+00, -9.0453e-01, -7.3855e-01],\n",
            "        [ 1.0000e+00, -1.1553e-09, -8.1690e-10,  1.7321e+00],\n",
            "        [ 1.0000e+00, -1.5000e+00,  2.9901e-10,  8.6603e-01],\n",
            "        [ 1.0000e+00,  8.1650e-01,  5.7735e-01,  1.4142e+00],\n",
            "        [ 1.0000e+00, -4.2640e-01,  1.5076e+00,  7.3855e-01],\n",
            "        [ 1.0000e+00,  5.0000e-01,  1.4142e+00, -8.6603e-01],\n",
            "        [ 1.0000e+00,  1.1553e-09,  8.1690e-10, -1.7321e+00],\n",
            "        [ 1.0000e+00,  1.5000e+00, -2.9901e-10, -8.6603e-01],\n",
            "        [ 1.0000e+00,  1.2792e+00,  9.0453e-01, -7.3855e-01],\n",
            "        [ 1.0000e+00,  8.5280e-01, -3.0151e-01, -1.4771e+00],\n",
            "        [ 1.0000e+00, -9.0658e-07,  9.0453e-01, -1.4771e+00],\n",
            "        [ 1.0000e+00, -1.7056e+00, -3.0151e-01, -1.8132e-07],\n",
            "        [ 1.0000e+00, -8.1650e-01, -5.7735e-01,  1.4142e+00],\n",
            "        [ 1.0000e+00,  8.5280e-01, -3.0151e-01,  1.4771e+00],\n",
            "        [ 1.0000e+00, -5.0000e-01, -1.4142e+00, -8.6603e-01],\n",
            "        [ 1.0000e+00,  1.0000e+00, -1.4142e+00,  0.0000e+00],\n",
            "        [ 1.0000e+00,  1.6330e+00, -5.7735e-01, -1.3888e-06],\n",
            "        [ 1.0000e+00,  1.5000e+00, -1.1159e-09,  8.6603e-01],\n",
            "        [ 1.0000e+00, -9.0658e-07,  9.0453e-01,  1.4771e+00],\n",
            "        [ 1.0000e+00, -8.1650e-01, -5.7735e-01, -1.4142e+00],\n",
            "        [ 1.0000e+00, -1.2792e+00,  9.0453e-01,  7.3855e-01],\n",
            "        [ 1.0000e+00, -1.5000e+00,  1.1159e-09, -8.6603e-01],\n",
            "        [ 1.0000e+00, -1.2792e+00,  9.0453e-01, -7.3855e-01],\n",
            "        [ 1.0000e+00,  1.5000e+00, -2.9901e-10, -8.6603e-01],\n",
            "        [ 1.0000e+00,  5.0000e-01,  1.4142e+00, -8.6603e-01],\n",
            "        [ 1.0000e+00,  4.2640e-01, -1.5076e+00,  7.3855e-01],\n",
            "        [ 1.0000e+00, -8.5280e-01, -1.5076e+00, -1.8132e-07],\n",
            "        [ 1.0000e+00, -1.7360e-06,  1.7321e+00, -1.3888e-06],\n",
            "        [ 1.0000e+00,  1.2792e+00,  9.0453e-01,  7.3855e-01],\n",
            "        [ 1.0000e+00,  1.1553e-09,  8.1690e-10, -1.7321e+00],\n",
            "        [ 1.0000e+00, -5.0000e-01, -1.4142e+00,  8.6603e-01],\n",
            "        [ 1.0000e+00, -1.1553e-09, -8.1690e-10,  1.7321e+00],\n",
            "        [ 1.0000e+00, -1.5000e+00,  2.9901e-10,  8.6603e-01],\n",
            "        [ 1.0000e+00,  4.2640e-01, -1.5076e+00, -7.3855e-01],\n",
            "        [ 1.0000e+00, -1.0000e+00,  1.4142e+00,  0.0000e+00],\n",
            "        [ 1.0000e+00,  5.0000e-01,  1.4142e+00,  8.6603e-01]],\n",
            "       grad_fn=<StackBackward0>), 'edge_features': tensor([[ 0.0054,  0.0150, -0.0122,  ...,  0.0012,  0.0019,  0.0055],\n",
            "        [ 0.0054,  0.0150, -0.0122,  ...,  0.0012,  0.0019,  0.0055],\n",
            "        [-0.3500,  0.5763, -0.1174,  ...,  0.2479, -0.3243,  0.0976],\n",
            "        ...,\n",
            "        [ 0.0054,  0.0150, -0.0122,  ...,  0.0012,  0.0019,  0.0055],\n",
            "        [ 0.0750,  0.0990, -0.0991,  ...,  0.0225,  0.0435,  0.0187],\n",
            "        [ 0.0750,  0.0990, -0.0991,  ...,  0.0225,  0.0435,  0.0187]],\n",
            "       grad_fn=<IndexAddBackward0>), 'edge_energy': tensor([[-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.8009],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [-0.0085],\n",
            "        [-0.8009],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338],\n",
            "        [-0.0085],\n",
            "        [ 0.1338],\n",
            "        [ 0.1338]], grad_fn=<MmBackward0>), 'atomic_energy': tensor([[-5.8824],\n",
            "        [-5.8824]], grad_fn=<AddBackward0>), 'batch': tensor([0, 0]), 'total_energy': tensor([[-11.7649]], grad_fn=<ScatterAddBackward0>), 'forces': tensor([[ 2.2136e-05,  1.2571e-05,  9.7556e-06],\n",
            "        [-2.2123e-05, -1.2524e-05, -9.7565e-06]], grad_fn=<NegBackward0>)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "if not os.path.exists('jarvis_leaderboard'):\n",
        "  !git clone https://github.com/usnistgov/jarvis_leaderboard.git\n",
        "# os.chdir('jarvis_leaderboard')\n",
        "# !pip install -e .\n",
        "os.chdir('/content/jarvis_leaderboard/jarvis_leaderboard/contributions/')\n",
        "os.makedirs('allegro_si')\n",
        "os.chdir('allegro_si')"
      ],
      "metadata": {
        "id": "vQaB-lCuPvwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://figshare.com/ndownloader/files/40357663 -O mlearn.json.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXXN8_JlPbdr",
        "outputId": "a06ac51b-6710-4e00-debf-a95e1394234b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-04 19:04:57--  https://figshare.com/ndownloader/files/40357663\n",
            "Resolving figshare.com (figshare.com)... 52.16.114.44, 79.125.18.57, 2a05:d018:1f4:d003:9e37:18d0:4037:e50b, ...\n",
            "Connecting to figshare.com (figshare.com)|52.16.114.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240104/eu-west-1/s3/aws4_request&X-Amz-Date=20240104T190458Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=2f0a5e11290aa93f3c0e82f50ff430efd499e42e18e1fea5815acdccbffcef01 [following]\n",
            "--2024-01-04 19:04:58--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/40357663/mlearn.json.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20240104/eu-west-1/s3/aws4_request&X-Amz-Date=20240104T190458Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=2f0a5e11290aa93f3c0e82f50ff430efd499e42e18e1fea5815acdccbffcef01\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.35.72, 52.218.105.82, 52.92.37.0, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.35.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2542319 (2.4M) [application/zip]\n",
            "Saving to: ‘mlearn.json.zip’\n",
            "\n",
            "mlearn.json.zip     100%[===================>]   2.42M  1.99MB/s    in 1.2s    \n",
            "\n",
            "2024-01-04 19:05:00 (1.99 MB/s) - ‘mlearn.json.zip’ saved [2542319/2542319]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json,zipfile\n",
        "#This dataset should have same test split (with ids) as the mlearn dataset built above\n",
        "df = pd.DataFrame(\n",
        "    json.loads(\n",
        "        zipfile.ZipFile(\"mlearn.json.zip\").read(\n",
        "            \"mlearn.json\"\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Jw72urwrQG_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJa4Uh62QfUC",
        "outputId": "b5a06686-159f-42d0-dbb1-8ed552494634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/jarvis_leaderboard/jarvis_leaderboard/contributions/allegro_si\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_allegro_forces(model=[],atoms=[],cutoff=5):\n",
        "    ase_atoms = atoms.ase_converter()\n",
        "    a = AtomicData.from_ase(ase_atoms,cutoff)\n",
        "    data = AtomicData.to_AtomicDataDict(a)\n",
        "    tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "    data = tm(data)\n",
        "    out = model(data)\n",
        "    pen=out['total_energy'].squeeze().cpu().detach().numpy().tolist()\n",
        "    num_atoms=atoms.num_atoms\n",
        "    pf=out['forces'].squeeze().cpu().detach().numpy()\n",
        "    return pen,pf,_"
      ],
      "metadata": {
        "id": "5i1-CsYHQvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element"
      ],
      "metadata": {
        "id": "VRQ8whzM3Zwm",
        "outputId": "9bcc4473-5929-48e6-db60-f2e22cc8961e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Si'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "for i in glob.glob(\"../../benchmarks/AI/MLFF/*energy*.zip\"):\n",
        "    if \"mlearn\" in i and element in i:\n",
        "        fname_e = (\n",
        "            \"AI-MLFF-energy-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-mae.csv\"\n",
        "        )\n",
        "        fname_f = (\n",
        "            \"AI-MLFF-forces-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-multimae.csv\"\n",
        "        )\n",
        "        fname_s = (\n",
        "            \"AI-MLFF-stresses-\"\n",
        "            + i.split(\"/\")[-1].split(\"_energy.json.zip\")[0]\n",
        "            + \"-test-multimae.csv\"\n",
        "        )\n",
        "        f_e = open(fname_e, \"w\")\n",
        "        f_f = open(fname_f, \"w\")\n",
        "        #f_s = open(fname_s, \"w\")\n",
        "\n",
        "        f_e.write(\"id,target,prediction\\n\")\n",
        "        f_f.write(\"id,target,prediction\\n\")\n",
        "        #f_s.write(\"id,prediction\\n\")\n",
        "        #\n",
        "        print(i)\n",
        "        dat = json.loads(\n",
        "            zipfile.ZipFile(i).read(i.split(\"/\")[-1].split(\".zip\")[0])\n",
        "        )\n",
        "        print(dat[\"test\"])\n",
        "        for key, val in dat[\"test\"].items():\n",
        "            entry = df[df[\"jid\"] == key]\n",
        "            atoms = Atoms.from_dict(entry.atoms.values[0])\n",
        "            # print(key,val,df[df['jid']==key],atoms)\n",
        "            # energy,forces=get_alignn_forces(atoms)\n",
        "            energy, forces, stress = get_allegro_forces(model=model,atoms=atoms)\n",
        "            print(key, val, energy, atoms.num_atoms)\n",
        "            line = key +\",\"+ str(entry.energy.values[0])+\",\" + str(energy) + \"\\n\"\n",
        "            f_e.write(line)\n",
        "            line = (\n",
        "                key\n",
        "                + \",\"\n",
        "                + str(\";\".join(map(str, np.array(entry.forces.values[0]).flatten())))\n",
        "                + \",\"\n",
        "                + str(\";\".join(map(str, np.array(forces).flatten())))\n",
        "                + \"\\n\"\n",
        "            )\n",
        "            f_f.write(line)\n",
        "            # line = (\n",
        "            #     key\n",
        "            #     + \",\"\n",
        "            #     + str(\";\".join(map(str, np.array(stress).flatten())))\n",
        "            #     + \"\\n\"\n",
        "            # )\n",
        "            # f_s.write(line)\n",
        "        f_e.close()\n",
        "        f_f.close()\n",
        "        # f_s.close()\n",
        "        zname = fname_e + \".zip\"\n",
        "        with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            myzip.write(fname_e)\n",
        "\n",
        "        zname = fname_f + \".zip\"\n",
        "        with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "            myzip.write(fname_f)\n",
        "\n",
        "        # zname = fname_s + \".zip\"\n",
        "        # with zipfile.ZipFile(zname, \"w\") as myzip:\n",
        "        #     myzip.write(fname_s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZJz8BOPGYB",
        "outputId": "15e00e7f-5bdb-4ecb-e920-281d21aec2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../../benchmarks/AI/MLFF/mlearn_Si_energy.json.zip\n",
            "{'Si-215': -297.62773938, 'Si-216': -295.77170067, 'Si-217': -291.28958206, 'Si-218': -296.24088456, 'Si-219': -294.41361742, 'Si-220': -334.75283939, 'Si-221': -334.69215136, 'Si-222': -184.71808052, 'Si-223': -121.41180043, 'Si-224': -338.93899696, 'Si-225': -338.83557056, 'Si-226': -335.68901422, 'Si-227': -333.7064957, 'Si-228': -344.85564046, 'Si-229': -344.81108268, 'Si-230': -298.83222646, 'Si-231': -298.96501782, 'Si-232': -295.20943762, 'Si-233': -291.86293882, 'Si-234': -344.74080048, 'Si-235': -344.74080047, 'Si-236': -344.74080046, 'Si-237': -341.22165747, 'Si-238': -341.22165734, 'Si-239': -341.22165747}\n",
            "Si-215 -297.62773938 -327.4481201171875 63\n",
            "Si-216 -295.77170067 -323.73291015625 63\n",
            "Si-217 -291.28958206 -325.13543701171875 63\n",
            "Si-218 -296.24088456 -323.0979919433594 63\n",
            "Si-219 -294.41361742 -323.4185485839844 63\n",
            "Si-220 -334.75283939 -366.3343200683594 63\n",
            "Si-221 -334.69215136 -366.5677490234375 63\n",
            "Si-222 -184.71808052 -208.11734008789062 36\n",
            "Si-223 -121.41180043 -137.68653869628906 24\n",
            "Si-224 -338.93899696 -367.03826904296875 64\n",
            "Si-225 -338.83557056 -366.92376708984375 64\n",
            "Si-226 -335.68901422 -364.04010009765625 64\n",
            "Si-227 -333.7064957 -361.84796142578125 64\n",
            "Si-228 -344.85564046 -372.9916687011719 64\n",
            "Si-229 -344.81108268 -372.876953125 64\n",
            "Si-230 -298.83222646 -332.7695617675781 64\n",
            "Si-231 -298.96501782 -333.75616455078125 64\n",
            "Si-232 -295.20943762 -329.10028076171875 64\n",
            "Si-233 -291.86293882 -326.3515319824219 64\n",
            "Si-234 -344.74080048 -367.50390625 64\n",
            "Si-235 -344.74080047 -367.50390625 64\n",
            "Si-236 -344.74080046 -367.50390625 64\n",
            "Si-237 -341.22165747 -367.19073486328125 64\n",
            "Si-238 -341.22165734 -367.1907958984375 64\n",
            "Si-239 -341.22165747 -367.1907958984375 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -altr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK1rdjyGRqGD",
        "outputId": "c97dfc7b-74d5-4bbc-a282-74bb9c201c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 2728\n",
            "-rw-r--r--   1 root root 2542319 Apr 29  2023 mlearn.json.zip\n",
            "drwxr-xr-x 384 root root   28672 Jan  4 19:04 ..\n",
            "drwxr-xr-x   2 root root    4096 Jan  4 19:05 .\n",
            "-rw-r--r--   1 root root   98947 Jan  4 19:05 AI-MLFF-forces-mlearn_Si-test-multimae.csv\n",
            "-rw-r--r--   1 root root    1009 Jan  4 19:05 AI-MLFF-energy-mlearn_Si-test-mae.csv\n",
            "-rw-r--r--   1 root root   99129 Jan  4 19:05 AI-MLFF-forces-mlearn_Si-test-multimae.csv.zip\n",
            "-rw-r--r--   1 root root    1181 Jan  4 19:05 AI-MLFF-energy-mlearn_Si-test-mae.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_df = pd.read_csv('AI-MLFF-energy-mlearn_Si-test-mae.csv.zip')"
      ],
      "metadata": {
        "id": "FU4jyfg-Rtjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "rGyr7zZGRwKB",
        "outputId": "b897f83c-57c3-460b-9876-a00b35ed14a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id      target  prediction\n",
              "0   Si-215 -297.627739 -311.281281\n",
              "1   Si-216 -295.771701 -308.110199\n",
              "2   Si-217 -291.289582 -309.122131\n",
              "3   Si-218 -296.240885 -307.671967\n",
              "4   Si-219 -294.413617 -307.857819\n",
              "5   Si-220 -334.752839 -344.896484\n",
              "6   Si-221 -334.692151 -344.974274\n",
              "7   Si-222 -184.718081 -195.669464\n",
              "8   Si-223 -121.411800 -129.180267\n",
              "9   Si-224 -338.938997 -346.408173\n",
              "10  Si-225 -338.835571 -346.246033\n",
              "11  Si-226 -335.689014 -343.577698\n",
              "12  Si-227 -333.706496 -341.727631\n",
              "13  Si-228 -344.855640 -351.940643\n",
              "14  Si-229 -344.811083 -351.816498\n",
              "15  Si-230 -298.832226 -316.325928\n",
              "16  Si-231 -298.965018 -316.924011\n",
              "17  Si-232 -295.209438 -313.108765\n",
              "18  Si-233 -291.862939 -309.919128\n",
              "19  Si-234 -344.740800 -349.788391\n",
              "20  Si-235 -344.740800 -349.788391\n",
              "21  Si-236 -344.740800 -349.788361\n",
              "22  Si-237 -341.221657 -346.382965\n",
              "23  Si-238 -341.221657 -346.383026\n",
              "24  Si-239 -341.221657 -346.382996"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de85a1b1-4506-4a24-aeb5-cc71b571aaa6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Si-215</td>\n",
              "      <td>-297.627739</td>\n",
              "      <td>-311.281281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Si-216</td>\n",
              "      <td>-295.771701</td>\n",
              "      <td>-308.110199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Si-217</td>\n",
              "      <td>-291.289582</td>\n",
              "      <td>-309.122131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Si-218</td>\n",
              "      <td>-296.240885</td>\n",
              "      <td>-307.671967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Si-219</td>\n",
              "      <td>-294.413617</td>\n",
              "      <td>-307.857819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Si-220</td>\n",
              "      <td>-334.752839</td>\n",
              "      <td>-344.896484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Si-221</td>\n",
              "      <td>-334.692151</td>\n",
              "      <td>-344.974274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Si-222</td>\n",
              "      <td>-184.718081</td>\n",
              "      <td>-195.669464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Si-223</td>\n",
              "      <td>-121.411800</td>\n",
              "      <td>-129.180267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Si-224</td>\n",
              "      <td>-338.938997</td>\n",
              "      <td>-346.408173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Si-225</td>\n",
              "      <td>-338.835571</td>\n",
              "      <td>-346.246033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Si-226</td>\n",
              "      <td>-335.689014</td>\n",
              "      <td>-343.577698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Si-227</td>\n",
              "      <td>-333.706496</td>\n",
              "      <td>-341.727631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Si-228</td>\n",
              "      <td>-344.855640</td>\n",
              "      <td>-351.940643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Si-229</td>\n",
              "      <td>-344.811083</td>\n",
              "      <td>-351.816498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Si-230</td>\n",
              "      <td>-298.832226</td>\n",
              "      <td>-316.325928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Si-231</td>\n",
              "      <td>-298.965018</td>\n",
              "      <td>-316.924011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Si-232</td>\n",
              "      <td>-295.209438</td>\n",
              "      <td>-313.108765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Si-233</td>\n",
              "      <td>-291.862939</td>\n",
              "      <td>-309.919128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Si-234</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-349.788391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Si-235</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-349.788391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Si-236</td>\n",
              "      <td>-344.740800</td>\n",
              "      <td>-349.788361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Si-237</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-346.382965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Si-238</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-346.383026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Si-239</td>\n",
              "      <td>-341.221657</td>\n",
              "      <td>-346.382996</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de85a1b1-4506-4a24-aeb5-cc71b571aaa6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de85a1b1-4506-4a24-aeb5-cc71b571aaa6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de85a1b1-4506-4a24-aeb5-cc71b571aaa6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-85da32dd-c817-49e0-80b5-d44f6034f2fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-85da32dd-c817-49e0-80b5-d44f6034f2fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-85da32dd-c817-49e0-80b5-d44f6034f2fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7a5c8399-1b34-4ab4-959e-95ccca54eebc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('en_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7a5c8399-1b34-4ab4-959e-95ccca54eebc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('en_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(en_df['target'],en_df['prediction'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJDW6nbNSMyC",
        "outputId": "d082af47-1877-4bf8-8389-1fce619c83c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.112282754542193"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(en_df['target'],en_df['prediction'],'.')\n",
        "plt.xlabel('DFT energy(eV)')\n",
        "plt.ylabel('FF energy(eV)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "X1zEvZXTtBVi",
        "outputId": "717e537a-f153-4932-9e4d-d5764fc7c6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'FF energy(eV)')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_df = pd.read_csv('AI-MLFF-forces-mlearn_Si-test-multimae.csv.zip')"
      ],
      "metadata": {
        "id": "qUDMBAxNtNJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "7delVOXdtgEI",
        "outputId": "4c1fda0a-2fd5-4ed7-fdcc-85bd6513aed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                             target  \\\n",
              "0   Si-215  -0.05394626;0.05113266;0.18121306;0.26663997;-...   \n",
              "1   Si-216  3.30282396;-1.05803218;0.22431344;0.30581845;1...   \n",
              "2   Si-217  0.40295226;0.92358416;0.05267631;1.92042467;-1...   \n",
              "3   Si-218  -0.97261346;-0.16676844;-1.20511235;-0.2330653...   \n",
              "4   Si-219  -0.91219868;0.41237799;1.15301928;0.15818158;-...   \n",
              "5   Si-220  -0.05608486;0.22205158;0.76935161;-0.01888349;...   \n",
              "6   Si-221  -0.99485517;0.48253845;-0.19532324;0.46312852;...   \n",
              "7   Si-222  0.00684328;0.01878958;0.01564654;-0.0083981;0....   \n",
              "8   Si-223  -0.00309968;-0.00311653;-0.0031407;-0.00574422...   \n",
              "9   Si-224  0.10803849;1.5201289;-1.43824908;-1.60811649;-...   \n",
              "10  Si-225  0.91448851;-1.09437286;-0.00344895;1.61222432;...   \n",
              "11  Si-226  0.02527078;-0.62806301;-0.2229749;-0.75328873;...   \n",
              "12  Si-227  0.00995144;-1.16831526;1.45085777;1.01651386;0...   \n",
              "13  Si-228  -0.20717037;-0.37315105;-0.62300221;0.78652078...   \n",
              "14  Si-229  -0.88379159;-0.87350648;1.62210639;-0.28082132...   \n",
              "15  Si-230  1.9104226;2.27558322;1.53211371;-0.19180055;1....   \n",
              "16  Si-231  0.47738134;1.44069956;0.33205219;0.54120931;-0...   \n",
              "17  Si-232  -0.61196745;0.20739327;2.37155302;1.29064753;-...   \n",
              "18  Si-233  1.58924009;-1.32676264;1.70069606;0.14674622;1...   \n",
              "19  Si-234  -0.0;-0.0;0.0;-0.0;-0.0;0.0;0.0;-0.0;-0.0;0.0;...   \n",
              "20  Si-235  -0.0;-0.0;-0.0;-0.0;0.0;0.0;-0.0;0.0;0.0;-0.0;...   \n",
              "21  Si-236  0.0;-0.0;0.0;-0.0;-0.0;-0.0;0.0;0.0;0.0;-0.0;0...   \n",
              "22  Si-237  0.0;-0.0;-1.14476094;-0.0;-0.0;-1.14476094;0.0...   \n",
              "23  Si-238  -0.0;-1.14476111;-0.0;-0.0;-1.14476111;-0.0;-0...   \n",
              "24  Si-239  -1.14476008;0.0;-0.0;-1.14476008;0.0;0.0;-1.14...   \n",
              "\n",
              "                                           prediction  \n",
              "0   -0.1637012;-0.24842407;0.70421517;-0.22590739;...  \n",
              "1   3.801369;-0.82320356;0.8429266;0.40708798;1.30...  \n",
              "2   0.73041385;0.823104;-0.14624879;2.6112099;-2.3...  \n",
              "3   -1.0439833;0.04591956;-1.47682;-0.36395317;-0....  \n",
              "4   -1.1425277;0.33830065;1.5611945;0.0024222583;-...  \n",
              "5   0.010513019;0.13157949;0.57110226;-0.082589716...  \n",
              "6   -0.9077917;0.54527456;-0.27858397;0.550107;0.2...  \n",
              "7   -0.42252806;0.35819685;-0.022435397;-0.1823158...  \n",
              "8   0.43682277;-0.1987735;-0.27614632;-0.009905735...  \n",
              "9   0.22568935;1.2939186;-1.5026921;-1.571835;-0.2...  \n",
              "10  0.78282404;-1.1384423;-0.31071678;1.4780734;0....  \n",
              "11  0.025555838;-0.55150247;-0.22299787;-0.1775725...  \n",
              "12  0.21631436;-1.2614381;1.5368348;1.1257374;0.50...  \n",
              "13  -0.15941307;-0.4116645;-0.6235566;0.7541975;-0...  \n",
              "14  -1.0584755;-0.8535907;1.6141362;-0.44434792;-0...  \n",
              "15  2.6076174;2.2040653;2.0917013;-0.62219524;1.57...  \n",
              "16  0.2660787;1.599385;0.18822727;0.033764318;-1.2...  \n",
              "17  -0.6394315;0.13746455;2.760345;1.8180473;-1.07...  \n",
              "18  2.0060532;-1.6184714;1.7326064;0.49501508;1.97...  \n",
              "19  -2.9134564e-05;2.9575778e-05;3.0174653e-05;-2....  \n",
              "20  2.9413262e-05;-2.9010233e-05;3.0142022e-05;2.9...  \n",
              "21  2.8939918e-05;2.9521187e-05;-2.7696602e-05;2.9...  \n",
              "22  -8.62591e-06;-1.1072494e-05;-1.7885191;-6.8331...  \n",
              "23  -8.17515e-06;-1.7885202;-1.08340755e-05;2.0419...  \n",
              "24  -1.7885201;-9.4468705e-06;-1.0063988e-05;-1.78...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16f7fe17-48e1-4047-a027-123dddc661ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Si-215</td>\n",
              "      <td>-0.05394626;0.05113266;0.18121306;0.26663997;-...</td>\n",
              "      <td>-0.1637012;-0.24842407;0.70421517;-0.22590739;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Si-216</td>\n",
              "      <td>3.30282396;-1.05803218;0.22431344;0.30581845;1...</td>\n",
              "      <td>3.801369;-0.82320356;0.8429266;0.40708798;1.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Si-217</td>\n",
              "      <td>0.40295226;0.92358416;0.05267631;1.92042467;-1...</td>\n",
              "      <td>0.73041385;0.823104;-0.14624879;2.6112099;-2.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Si-218</td>\n",
              "      <td>-0.97261346;-0.16676844;-1.20511235;-0.2330653...</td>\n",
              "      <td>-1.0439833;0.04591956;-1.47682;-0.36395317;-0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Si-219</td>\n",
              "      <td>-0.91219868;0.41237799;1.15301928;0.15818158;-...</td>\n",
              "      <td>-1.1425277;0.33830065;1.5611945;0.0024222583;-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Si-220</td>\n",
              "      <td>-0.05608486;0.22205158;0.76935161;-0.01888349;...</td>\n",
              "      <td>0.010513019;0.13157949;0.57110226;-0.082589716...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Si-221</td>\n",
              "      <td>-0.99485517;0.48253845;-0.19532324;0.46312852;...</td>\n",
              "      <td>-0.9077917;0.54527456;-0.27858397;0.550107;0.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Si-222</td>\n",
              "      <td>0.00684328;0.01878958;0.01564654;-0.0083981;0....</td>\n",
              "      <td>-0.42252806;0.35819685;-0.022435397;-0.1823158...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Si-223</td>\n",
              "      <td>-0.00309968;-0.00311653;-0.0031407;-0.00574422...</td>\n",
              "      <td>0.43682277;-0.1987735;-0.27614632;-0.009905735...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Si-224</td>\n",
              "      <td>0.10803849;1.5201289;-1.43824908;-1.60811649;-...</td>\n",
              "      <td>0.22568935;1.2939186;-1.5026921;-1.571835;-0.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Si-225</td>\n",
              "      <td>0.91448851;-1.09437286;-0.00344895;1.61222432;...</td>\n",
              "      <td>0.78282404;-1.1384423;-0.31071678;1.4780734;0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Si-226</td>\n",
              "      <td>0.02527078;-0.62806301;-0.2229749;-0.75328873;...</td>\n",
              "      <td>0.025555838;-0.55150247;-0.22299787;-0.1775725...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Si-227</td>\n",
              "      <td>0.00995144;-1.16831526;1.45085777;1.01651386;0...</td>\n",
              "      <td>0.21631436;-1.2614381;1.5368348;1.1257374;0.50...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Si-228</td>\n",
              "      <td>-0.20717037;-0.37315105;-0.62300221;0.78652078...</td>\n",
              "      <td>-0.15941307;-0.4116645;-0.6235566;0.7541975;-0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Si-229</td>\n",
              "      <td>-0.88379159;-0.87350648;1.62210639;-0.28082132...</td>\n",
              "      <td>-1.0584755;-0.8535907;1.6141362;-0.44434792;-0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Si-230</td>\n",
              "      <td>1.9104226;2.27558322;1.53211371;-0.19180055;1....</td>\n",
              "      <td>2.6076174;2.2040653;2.0917013;-0.62219524;1.57...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Si-231</td>\n",
              "      <td>0.47738134;1.44069956;0.33205219;0.54120931;-0...</td>\n",
              "      <td>0.2660787;1.599385;0.18822727;0.033764318;-1.2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Si-232</td>\n",
              "      <td>-0.61196745;0.20739327;2.37155302;1.29064753;-...</td>\n",
              "      <td>-0.6394315;0.13746455;2.760345;1.8180473;-1.07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Si-233</td>\n",
              "      <td>1.58924009;-1.32676264;1.70069606;0.14674622;1...</td>\n",
              "      <td>2.0060532;-1.6184714;1.7326064;0.49501508;1.97...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Si-234</td>\n",
              "      <td>-0.0;-0.0;0.0;-0.0;-0.0;0.0;0.0;-0.0;-0.0;0.0;...</td>\n",
              "      <td>-2.9134564e-05;2.9575778e-05;3.0174653e-05;-2....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Si-235</td>\n",
              "      <td>-0.0;-0.0;-0.0;-0.0;0.0;0.0;-0.0;0.0;0.0;-0.0;...</td>\n",
              "      <td>2.9413262e-05;-2.9010233e-05;3.0142022e-05;2.9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Si-236</td>\n",
              "      <td>0.0;-0.0;0.0;-0.0;-0.0;-0.0;0.0;0.0;0.0;-0.0;0...</td>\n",
              "      <td>2.8939918e-05;2.9521187e-05;-2.7696602e-05;2.9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Si-237</td>\n",
              "      <td>0.0;-0.0;-1.14476094;-0.0;-0.0;-1.14476094;0.0...</td>\n",
              "      <td>-8.62591e-06;-1.1072494e-05;-1.7885191;-6.8331...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Si-238</td>\n",
              "      <td>-0.0;-1.14476111;-0.0;-0.0;-1.14476111;-0.0;-0...</td>\n",
              "      <td>-8.17515e-06;-1.7885202;-1.08340755e-05;2.0419...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Si-239</td>\n",
              "      <td>-1.14476008;0.0;-0.0;-1.14476008;0.0;0.0;-1.14...</td>\n",
              "      <td>-1.7885201;-9.4468705e-06;-1.0063988e-05;-1.78...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16f7fe17-48e1-4047-a027-123dddc661ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-16f7fe17-48e1-4047-a027-123dddc661ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-16f7fe17-48e1-4047-a027-123dddc661ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-607ecf72-468e-484f-96f8-dd36ee82cea7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-607ecf72-468e-484f-96f8-dd36ee82cea7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-607ecf72-468e-484f-96f8-dd36ee82cea7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_116f74c2-e7b5-4bce-8665-96199c5e4cff\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('f_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_116f74c2-e7b5-4bce-8665-96199c5e4cff button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('f_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = np.concatenate([np.array(i.split(';'),dtype='float') for i in f_df['target'].values])\n",
        "pred= np.concatenate([np.array(i.split(';'),dtype='float') for i in f_df['prediction'].values])\n",
        "plt.plot(target,pred,'.')\n",
        "plt.xlabel('DFT forces(eV/A)')\n",
        "plt.ylabel('FF forces(eV/A)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "wXMeidhyucZU",
        "outputId": "57e5bfc8-9fd0-471c-f9c4-1d04b0ca1d2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'FF forces(eV/A)')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actual_en=[]\n",
        "# pred_en=[]\n",
        "# actual_forces=[]\n",
        "# pred_forces=[]\n",
        "\n",
        "# for i, j, k, l in zip(\n",
        "#     test_structures, test_energies, test_forces, train_stresses\n",
        "# ):\n",
        "#     atoms = pmg_to_atoms(i)\n",
        "#     ase_atoms = atoms.ase_converter()\n",
        "#     a = AtomicData.from_ase(ase_atoms,5)\n",
        "#     data = AtomicData.to_AtomicDataDict(a)\n",
        "#     tm = TypeMapper(chemical_symbol_to_type = config['chemical_symbol_to_type'])\n",
        "#     data = tm(data)\n",
        "#     out = model(data)\n",
        "#     pen=out['total_energy'].squeeze().cpu().detach().numpy().tolist()\n",
        "#     print(j,pen)\n",
        "#     num_atoms=atoms.num_atoms\n",
        "#     actual_en.append(j/num_atoms)\n",
        "#     pred_en.append(pen/num_atoms)\n",
        "#     actual_forces.append(np.array(k).flatten())\n",
        "#     pf=out['forces'].squeeze().cpu().detach().numpy()\n",
        "#     pred_forces.append(pf.flatten())\n",
        "#     #break"
      ],
      "metadata": {
        "id": "HyOyUFYQLfbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %matplotlib inline\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(actual_en,pred_en,'.')\n",
        "# plt.xlabel('DFT energy(eV/atom)')\n",
        "# plt.ylabel('FF energy(eV/atom)')"
      ],
      "metadata": {
        "id": "zedmdoAcNmHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import mean_absolute_error\n",
        "# mean_absolute_error(actual_en,pred_en)#energy error MAE per atom"
      ],
      "metadata": {
        "id": "ZgJemMF5N3Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actual_forces = np.concatenate(actual_forces)\n",
        "# pred_forces = np.concatenate(pred_forces)"
      ],
      "metadata": {
        "id": "ms8fXomtOVsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(actual_forces,pred_forces,'.')"
      ],
      "metadata": {
        "id": "R8DdStiMOlM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean_absolute_error(actual_forces,pred_forces)"
      ],
      "metadata": {
        "id": "NICx2BStO1Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out.keys()"
      ],
      "metadata": {
        "id": "yvbS6NaC7U0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out['total_energy'].squeeze().cpu().detach().numpy().tolist()"
      ],
      "metadata": {
        "id": "F4QLSATpJmIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out['forces'].squeeze().cpu().detach().numpy() #.tolist()"
      ],
      "metadata": {
        "id": "kFIOTBuOJqWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "# params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "# params"
      ],
      "metadata": {
        "id": "T7lR1ljg9Fum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "mZ_0eoNe9j8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqfWeB-TIJXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}